<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Functional Data Analysis SEIO Working Group</title>
<link>https://realworlddatascience.net/latest-content.html</link>
<atom:link href="https://realworlddatascience.net/latest-content.xml" rel="self" type="application/rss+xml"/>
<description></description>
<image>
<url>https://realworlddatascience.net/images/rwds-logo-150px.png</url>
<title>Functional Data Analysis SEIO Working Group</title>
<link>https://realworlddatascience.net/latest-content.html</link>
<height>83</height>
<width>144</width>
</image>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Mon, 11 Dec 2023 23:00:00 GMT</lastBuildDate>
<item>
  <title>Creating Christmas cards with R</title>
  <dc:creator>Nicola Rennie</dc:creator>
  <link>https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/xmas-cards.html</link>
  <description><![CDATA[ 




<p>When you think about data visualisation in R <span class="citation" data-cites="R-base">(R Core Team 2022)</span>, you’d be forgiven for not jumping straight to thinking about creating Christmas cards. However, the package and functions we often use to create bar charts and line graphs can be repurposed to create festive images. This tutorial provides a step-by-step guide to creating a Christmas card featuring a snowman – entirely in R. Though this seems like just a fun exercise, the functions and techniques you learn in this tutorial can also transfer into more traditional data visualisations created using {ggplot2} <span class="citation" data-cites="ggplot2">(Wickham 2016)</span> in R.</p>
<p>The code in this tutorial relies on the following packages:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(ggplot2)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(ggforce)</span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;">library</span>(sf)</span></code></pre></div>
<blockquote class="blockquote">
<p>You may also have seen this tutorial presented at the <a href="https://github.com/nrennie/oxford-RUG-christmas-cards">Oxford R User Group November 2023 Meetup</a>.</p>
</blockquote>
<section id="lets-build-a-snowman" class="level2">
<h2 class="anchored" data-anchor-id="lets-build-a-snowman">Let’s build a snowman!</h2>
<p>Before we jump in to writing R code, let’s take a step back and think about what you actually need to build a snowman. If you were given some crayons and a piece of paper, what would you draw?</p>
<p>You might draw two or three circles to make up the head and body. Perhaps some smaller dots for buttons and eyes, and a (rudimentary) hat constructed from some rectangles. Some brown lines create sticks for arms and, of course, a triangle to represent a carrot for a nose. For the background elements of our Christmas card, we also need the night sky (or day if you prefer), a light dusting of snow covering the ground, and a few snowflakes falling from the sky.</p>
<p>Now lines, rectangles, circles, and triangles are all just simple geometric objects. Crucially, they’re all things that we can create with {ggplot2} in R.</p>
</section>
<section id="build-a-snowman-with-r" class="level2">
<h2 class="anchored" data-anchor-id="build-a-snowman-with-r">Build a snowman with R</h2>
<p>Let’s start with the background. The easiest way to start with a blank canvas in {ggplot2} is to create an empty plot using <code>ggplot()</code> with no arguments. We can also remove all theme elements (such as the grey background and grid lines) with <code>theme_void()</code>. To change the background colour to a dark blue for the night sky, we can edit the <code>plot.background</code> element of the theme using <code>element_rect()</code> (since the background is essentially just a big rectangle).</p>
<p>In {ggplot2} <code>fill</code> is the inner colour of shapes whilst <code>colour</code> is the outline colour. You can specify colours in different ways in R: either via the <code>rgb()</code> function, using a character string for a hex colour such as <code>"#000000"</code>, or using a named colour. If you run <code>colors()</code>, you’ll see all the valid named colours you can use. Here, we’ve picked <code>"midnightblue"</code>.</p>
<p>Let’s save this initial plot as an object <code>s1</code> that we’ll keep adding layers to. Saving plots in different stages of styling as objects can help to keep your code more modular.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">s1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">ggplot</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-2">  <span class="fu" style="color: #4758AB;">theme_void</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-3">  <span class="fu" style="color: #4758AB;">theme</span>(</span>
<span id="cb2-4">    <span class="at" style="color: #657422;">plot.background =</span> <span class="fu" style="color: #4758AB;">element_rect</span>(</span>
<span id="cb2-5">      <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"midnightblue"</span></span>
<span id="cb2-6">      )</span>
<span id="cb2-7">  )</span>
<span id="cb2-8">s1</span></code></pre></div>
<p>Next we’ll add some snow on the ground. We’ll do this by drawing a white rectangle along the bottom of the plot. There are two different functions that we could use to add a rectangle: <code>geom_rect()</code> or <code>annotate()</code>. The difference between the two is that <code>geom_rect()</code> maps columns of a <code>data.frame</code> to different elements of a plot whereas <code>annotate()</code> can take values passed in as vectors. Most of the {ggplot2} graphs you’ll see will use <code>geom_*()</code> functions. However, if you’re only adding one or two elements to a plot then <code>annotate()</code> might be quicker.</p>
<p>Since we’re only adding one rectangle for the snow, it’s easier to use <code>annotate()</code> with the <code>"rect"</code> geometry. This requires four arguments: the minimum and maximum x and y coordinates of the rectangle – essentially specifying where the corners are. We can also change the colour of the rectangle and its outline using the <code>fill</code> and <code>colour</code> arguments. Here, I’ve used a very light grey instead of white.</p>
<p>If we don’t set the axis limits using <code>xlim()</code> and <code>ylim()</code>, the plot area will resize to fit the area of the snow rectangle. The night sky background will disappear. You can choose any axis limits you wish here – but the unit square will make it easier to find the right coordinates when deciding where to position other elements. Finally, we add <code>coord_fixed()</code> to fix the 1:1 aspect ratio and make sure our grid is actually square with <code>expand = FALSE</code> to remove the additional padding at the sides of the plot.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">s2 <span class="ot" style="color: #003B4F;">&lt;-</span> s1 <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-2">  <span class="fu" style="color: #4758AB;">annotate</span>(</span>
<span id="cb3-3">    <span class="at" style="color: #657422;">geom =</span> <span class="st" style="color: #20794D;">"rect"</span>,</span>
<span id="cb3-4">    <span class="at" style="color: #657422;">xmin =</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="at" style="color: #657422;">xmax =</span> <span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb3-5">    <span class="at" style="color: #657422;">ymin =</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="at" style="color: #657422;">ymax =</span> <span class="fl" style="color: #AD0000;">0.2</span>,</span>
<span id="cb3-6">    <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"grey98"</span>,</span>
<span id="cb3-7">    <span class="at" style="color: #657422;">colour =</span> <span class="st" style="color: #20794D;">"grey98"</span></span>
<span id="cb3-8">  ) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-9">  <span class="fu" style="color: #4758AB;">xlim</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-10">  <span class="fu" style="color: #4758AB;">ylim</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-11">  <span class="fu" style="color: #4758AB;">coord_fixed</span>(<span class="at" style="color: #657422;">expand =</span> <span class="cn" style="color: #8f5902;">FALSE</span>)</span>
<span id="cb3-12">s2</span></code></pre></div>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s1.png" class="img-fluid" alt="Dark blue square."></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s2.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom."></p>
</div>
</div>
</div>
<p>To finish off the background, we’ll add some falling snowflakes. We first need to decide where on the plot the snowflakes will appear. We’ll be plotting lots of snowflakes, so manually typing out the coordinates of where they’ll be would be very inefficient. Instead, we can use functions to generate the locations randomly. For this we’ll use the uniform distribution. The uniform distribution has two parameters – the lower and upper bounds where any values between the bounds are equally likely. You can generate samples from a uniform distribution in R using the <code>runif()</code> function.</p>
<p>When generating random numbers in R (or any other programming language), it’s important to set a seed. This means that if you give your code to someone else, they’ll get the same random numbers as you. Some people choose to use the date as the random seed and since we’re making Christmas cards, we’ll use Christmas day as the random seed – in <code>yyyymmdd</code> format, of course!</p>
<p>We create a variable <code>n</code> specifying how many snowflakes we’ll create. Creating a variable rather than hard coding the variables makes it easier to vary how many snowflakes we want. Since our plot grid goes between 0 and 1 in both the x and y directions, we generate random numbers between 0 and 1 for both the x and y coordinates and store the values in a <code>data.frame</code> called <code>snowflakes</code>.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;">set.seed</span>(<span class="dv" style="color: #AD0000;">20231225</span>)</span>
<span id="cb4-2">n <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">100</span></span>
<span id="cb4-3">snowflakes <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(</span>
<span id="cb4-4">  <span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">runif</span>(n, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb4-5">  <span class="at" style="color: #657422;">y =</span> <span class="fu" style="color: #4758AB;">runif</span>(n, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-6">)</span></code></pre></div>
<p>Now we can plot the <code>snowflakes</code> data using <code>geom_point()</code> – the same function you’d use for a scatter plot. Since we’re using a <code>geom_*()</code> function, we need to tell {ggplot2} which columns go on the <code>x</code> and <code>y</code> axes inside the <code>aes()</code> function. To plot the snowflakes, we’re going to make using of R’s different point characters. The default when plotting with <code>geom_point()</code> is a small black dot, but we can choose to use a small star (close enough to a snowflake!) by setting <code>pch = 8</code> and changing the <code>colour</code> to <code>"white"</code>.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">s3 <span class="ot" style="color: #003B4F;">&lt;-</span> s2 <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-2">  <span class="fu" style="color: #4758AB;">geom_point</span>(</span>
<span id="cb5-3">    <span class="at" style="color: #657422;">data =</span> snowflakes,</span>
<span id="cb5-4">    <span class="at" style="color: #657422;">mapping =</span> <span class="fu" style="color: #4758AB;">aes</span>(</span>
<span id="cb5-5">      <span class="at" style="color: #657422;">x =</span> x,</span>
<span id="cb5-6">      <span class="at" style="color: #657422;">y =</span> y</span>
<span id="cb5-7">    ),</span>
<span id="cb5-8">    <span class="at" style="color: #657422;">colour =</span> <span class="st" style="color: #20794D;">"white"</span>,</span>
<span id="cb5-9">    <span class="at" style="color: #657422;">pch =</span> <span class="dv" style="color: #AD0000;">8</span></span>
<span id="cb5-10">  )</span>
<span id="cb5-11">s3</span></code></pre></div>
<p>Now comes the part where we start rolling up some snowballs! Or, in the case of an R snowman, we draw some circles. Unfortunately, there isn’t a built-in <code>geom_*()</code> function in {ggplot2} for plotting circles. We could use <code>geom_point()</code> here and increase the size of the points but this approach can look a little bit <em>fuzzy</em> when the points are very large. Instead, we’ll turn to a {ggplot2} extension package for some additional <code>geom_*</code> functions - {ggforce} <span class="citation" data-cites="ggforce">(Pedersen 2022)</span>.</p>
<p>The <code>geom_circle()</code> function requires at least three elements mapped to the aesthetics inside <code>aes()</code>: the coordinates of the centre of the circle given by <code>x0</code> and <code>y0</code>, and the radii of each of the circles, <code>r</code>. Instead of creating a separate data frame and passing it into <code>geom_circle()</code>, we can alternatively create the data frame inside the function. The <code>fill</code> and <code>colour</code> arguments work as they do in {ggplot2} and we can set both to <code>"white"</code>.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">s4 <span class="ot" style="color: #003B4F;">&lt;-</span> s3 <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-2">  <span class="fu" style="color: #4758AB;">geom_circle</span>(</span>
<span id="cb6-3">    <span class="at" style="color: #657422;">data =</span> <span class="fu" style="color: #4758AB;">data.frame</span>(</span>
<span id="cb6-4">      <span class="at" style="color: #657422;">x0 =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.6</span>),</span>
<span id="cb6-5">      <span class="at" style="color: #657422;">y0 =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.3</span>, <span class="fl" style="color: #AD0000;">0.5</span>),</span>
<span id="cb6-6">      <span class="at" style="color: #657422;">r =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.15</span>, <span class="fl" style="color: #AD0000;">0.1</span>)</span>
<span id="cb6-7">    ),</span>
<span id="cb6-8">    <span class="at" style="color: #657422;">mapping =</span> <span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x0 =</span> x0, <span class="at" style="color: #657422;">y0 =</span> y0, <span class="at" style="color: #657422;">r =</span> r),</span>
<span id="cb6-9">    <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"white"</span>,</span>
<span id="cb6-10">    <span class="at" style="color: #657422;">colour =</span> <span class="st" style="color: #20794D;">"white"</span></span>
<span id="cb6-11">  )</span>
<span id="cb6-12">s4</span></code></pre></div>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s3.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom and small random white stars."></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s4.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom and small random white stars, and two white circles slightly off centre."></p>
</div>
</div>
</div>
<p>We can use <code>geom_point()</code> again to add some more points to represent the buttons and the eyes. Here, we’ll manually specify the coordinates of the points. For the buttons we add them in a vertical line in the middle of the snowman’s body circle, and for the eyes we add them in a horizontal line in the middle of the head circle.</p>
<p>Since no two rocks are exactly the same size, we can add some random variation to the size of the points using <code>runif()</code> again. We generate five different sizes between 2 and 4.5. For reference, the default point size is 1.5. Adding <code>scale_size_identity()</code> means that the sizes of the points are actually equally to the sizes we generated from <code>runif()</code> and removes the legend that is automatically added when we add <code>size</code> inside <code>aes()</code>.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">s5 <span class="ot" style="color: #003B4F;">&lt;-</span> s4 <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb7-2">  <span class="fu" style="color: #4758AB;">geom_point</span>(</span>
<span id="cb7-3">    <span class="at" style="color: #657422;">data =</span> <span class="fu" style="color: #4758AB;">data.frame</span>(</span>
<span id="cb7-4">      <span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.57</span>, <span class="fl" style="color: #AD0000;">0.62</span>),</span>
<span id="cb7-5">      <span class="at" style="color: #657422;">y =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.25</span>, <span class="fl" style="color: #AD0000;">0.3</span>, <span class="fl" style="color: #AD0000;">0.35</span>, <span class="fl" style="color: #AD0000;">0.52</span>, <span class="fl" style="color: #AD0000;">0.52</span>),</span>
<span id="cb7-6">      <span class="at" style="color: #657422;">size =</span> <span class="fu" style="color: #4758AB;">runif</span>(<span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="fl" style="color: #AD0000;">4.5</span>)</span>
<span id="cb7-7">    ),</span>
<span id="cb7-8">    <span class="at" style="color: #657422;">mapping =</span> <span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">size =</span> size)</span>
<span id="cb7-9">  ) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb7-10">  <span class="fu" style="color: #4758AB;">scale_size_identity</span>()</span>
<span id="cb7-11">s5</span></code></pre></div>
<p>To add sticks for arms, we can make use of <code>geom_segment()</code> to draw some lines. We could also use <code>geom_path()</code> but that is designed to connect points across multiple cases, whereas <code>geom_segment()</code> draws a single line per row of data – and we don’t want to join the snowman’s arms together!</p>
<p>To use <code>geom_segment()</code> we need to create a data frame containing the x and y coordinates for the start and end of each line, and then pass this into the aesthetic mapping with <code>aes()</code>. We can control the colour and width of the lines using the <code>colour</code> and <code>linewidth</code> arguments. Setting the <code>lineend</code> argument to <code>"round"</code> means that the ends of the lines will be rounded rather than the default straight edge.</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">s6 <span class="ot" style="color: #003B4F;">&lt;-</span> s5 <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb8-2">  <span class="fu" style="color: #4758AB;">geom_segment</span>(</span>
<span id="cb8-3">    <span class="at" style="color: #657422;">data =</span> <span class="fu" style="color: #4758AB;">data.frame</span>(</span>
<span id="cb8-4">      <span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.46</span>, <span class="fl" style="color: #AD0000;">0.7</span>),</span>
<span id="cb8-5">      <span class="at" style="color: #657422;">xend =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.33</span>, <span class="fl" style="color: #AD0000;">0.85</span>),</span>
<span id="cb8-6">      <span class="at" style="color: #657422;">y =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.3</span>, <span class="fl" style="color: #AD0000;">0.3</span>),</span>
<span id="cb8-7">      <span class="at" style="color: #657422;">yend =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.4</span>, <span class="fl" style="color: #AD0000;">0.4</span>)</span>
<span id="cb8-8">    ),</span>
<span id="cb8-9">    <span class="at" style="color: #657422;">mapping =</span> <span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend),</span>
<span id="cb8-10">    <span class="at" style="color: #657422;">colour =</span> <span class="st" style="color: #20794D;">"chocolate4"</span>,</span>
<span id="cb8-11">    <span class="at" style="color: #657422;">lineend =</span> <span class="st" style="color: #20794D;">"round"</span>,</span>
<span id="cb8-12">    <span class="at" style="color: #657422;">linewidth =</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb8-13">  )</span>
<span id="cb8-14">s6</span></code></pre></div>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s5.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom and small random white stars, and two white circles slightly off centre. Five black dots denote two eyes and three buttons on a snowman."></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s6.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom and small random white stars, and two white circles slightly off centre. Five black dots denote two eyes and three buttons on a snowman. Two brown lines look like arms."></p>
</div>
</div>
</div>
<p>We’ll now add a (very simple) hat to our snowman, fashioned out of two rectangles. We can add the rectangles as we did before using the <code>annotate()</code> function and specifying the locations of the corners of the rectangles. We start with a shorter wider rectangle for the brim of the hat, and then a taller, narrower rectangle for the crown of the hat. Since we’ll colour them both <code>"brown"</code>, it doesn’t matter if they overlap a little bit.</p>
<p>This <em>might</em> be one of the situations we should have used <code>geom_rect()</code> instead of <code>annotate()</code> but it might take a lot of trial and error to position the hat exactly where we want it, and this seemed a little easier with <code>annotate()</code>.</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">s7 <span class="ot" style="color: #003B4F;">&lt;-</span> s6 <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb9-2">  <span class="fu" style="color: #4758AB;">annotate</span>(</span>
<span id="cb9-3">    <span class="at" style="color: #657422;">geom =</span> <span class="st" style="color: #20794D;">"rect"</span>,</span>
<span id="cb9-4">    <span class="at" style="color: #657422;">xmin =</span> <span class="fl" style="color: #AD0000;">0.46</span>, <span class="at" style="color: #657422;">xmax =</span> <span class="fl" style="color: #AD0000;">0.74</span>,</span>
<span id="cb9-5">    <span class="at" style="color: #657422;">ymin =</span> <span class="fl" style="color: #AD0000;">0.55</span>, <span class="at" style="color: #657422;">ymax =</span> <span class="fl" style="color: #AD0000;">0.60</span>,</span>
<span id="cb9-6">    <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"brown"</span></span>
<span id="cb9-7">  ) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb9-8">  <span class="fu" style="color: #4758AB;">annotate</span>(</span>
<span id="cb9-9">    <span class="at" style="color: #657422;">geom =</span> <span class="st" style="color: #20794D;">"rect"</span>,</span>
<span id="cb9-10">    <span class="at" style="color: #657422;">xmin =</span> <span class="fl" style="color: #AD0000;">0.50</span>, <span class="at" style="color: #657422;">xmax =</span> <span class="fl" style="color: #AD0000;">0.70</span>,</span>
<span id="cb9-11">    <span class="at" style="color: #657422;">ymin =</span> <span class="fl" style="color: #AD0000;">0.56</span>, <span class="at" style="color: #657422;">ymax =</span> <span class="fl" style="color: #AD0000;">0.73</span>,</span>
<span id="cb9-12">    <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"brown"</span></span>
<span id="cb9-13">  )</span>
<span id="cb9-14">s7</span></code></pre></div>
<p>Now we can move on to the final component of building a snowman – the carrot for his nose! We’re going to use a triangle for the nose. Unfortunately, there are no built-in triangle geoms in {ggplot2} so we’ll have to make our own. There are different ways to do this, but here we’re going to make use of the {sf} package <span class="citation" data-cites="sf">(Pebesma 2018)</span>. The {sf} package (short for <em>simple features</em>) is designed for working with spatial data. Although we’re not working with maps, we can still use {sf} to make shapes – including polygons.</p>
<p>We start by constructing a matrix with two columns – one for x coordinates and one for y. The x coordinates start in the middle of the head and go slightly to the right for the triangle point. The y coordinates take a little bit more trial and error to get right. Note that although triangles only have three corners, we have four rows of points. The last row must be the same as the first to make the polygon <em>closed</em>. The matrix is then converted into a spatial object using the <code>st_polygon()</code> function, and we can check how it looks using <code>plot()</code>.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">nose_pts <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">matrix</span>(</span>
<span id="cb10-2">  <span class="fu" style="color: #4758AB;">c</span>(</span>
<span id="cb10-3">    <span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb10-4">    <span class="fl" style="color: #AD0000;">0.65</span>, <span class="fl" style="color: #AD0000;">0.48</span>,</span>
<span id="cb10-5">    <span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.46</span>,</span>
<span id="cb10-6">    <span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb10-7">  ),</span>
<span id="cb10-8">  <span class="at" style="color: #657422;">ncol =</span> <span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb10-9">  <span class="at" style="color: #657422;">byrow =</span> <span class="cn" style="color: #8f5902;">TRUE</span></span>
<span id="cb10-10">)</span>
<span id="cb10-11">nose <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">st_polygon</span>(<span class="fu" style="color: #4758AB;">list</span>(nose_pts))</span>
<span id="cb10-12"><span class="fu" style="color: #4758AB;">plot</span>(nose)</span></code></pre></div>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s7.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom and small random white stars, and two white circles slightly off centre. Five black dots denote two eyes and three buttons on a snowman. Two brown lines look like arms. Two red rectangles form a hat."></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/nose.png" class="img-fluid" alt="Outline of a triangle pointing to the right against a white background."></p>
</div>
</div>
</div>
<p>We can plot <code>sf</code> objects with {ggplot2} using <code>geom_sf()</code>. <code>geom_sf()</code> is a slightly special <code>geom</code> since we don’t need to specify an aesthetic mapping for the <code>x</code> and <code>y</code> axes – they are determined automatically from the <code>sf</code> object along with which type of geometry to draw. If your <code>sf</code> object has points, points will be drawn. If it has country shapes, polygons will be drawn. Like other <code>geom_*()</code> functions, we can change the <code>colour</code> and <code>fill</code> arguments to a different colour – in this case <code>"orange"</code> to represent a carrot!</p>
<p>You should see a <code>Coordinate system already present. Adding new coordinate system, which will replace the existing one.</code> message when you run the following code. The is because <code>geom_sf</code> forces it’s own coordinate system on the plot overriding our previous code specifying <code>coord_fixed()</code>. If you run it without the <code>coord_sf(expand = FALSE)</code>, the extra space around the plot will reappear. We can remove it again with <code>expand = FALSE</code>.</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1">s8 <span class="ot" style="color: #003B4F;">&lt;-</span> s7 <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb11-2">  <span class="fu" style="color: #4758AB;">geom_sf</span>(</span>
<span id="cb11-3">    <span class="at" style="color: #657422;">data =</span> nose,</span>
<span id="cb11-4">    <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"orange"</span>,</span>
<span id="cb11-5">    <span class="at" style="color: #657422;">colour =</span> <span class="st" style="color: #20794D;">"orange"</span></span>
<span id="cb11-6">  ) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb11-7">  <span class="fu" style="color: #4758AB;">coord_sf</span>(<span class="at" style="color: #657422;">expand =</span> <span class="cn" style="color: #8f5902;">FALSE</span>)</span>
<span id="cb11-8">s8</span></code></pre></div>
<blockquote class="blockquote">
<p>You <em>could</em> skip the <code>sf</code> part of this completely and pass the coordinates directly into <code>geom_polygon()</code> instead. However, I’ve often found it quicker and easier to tinker with polygon shapes using <code>sf</code>.</p>
</blockquote>
<p>A key part of any Christmas card is the message wishing recipients a Merry Christmas! We can add text to our plot using the <code>annotate()</code> function and the <code>"text"</code> geometry (you could instead use <code>geom_text()</code> if you prefer). When adding text, we require at least three arguments: the <code>x</code> and <code>y</code> coordinates of where the text should be added, and the <code>label</code> denoting what text should appear. We can supply additional arguments to <code>annotate()</code> to style the text, such as: <code>colour</code> (which changes the colour of the text); <code>family</code> (to define which font to use); <code>fontface</code> (which determines if the font is bold or italic, for example); and <code>size</code> (which changes the size of the text). The <code>"mono"</code> option for <code>family</code> tells {ggplot2} to use the default system monospace font.</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">s9 <span class="ot" style="color: #003B4F;">&lt;-</span> s8 <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-2">  <span class="fu" style="color: #4758AB;">annotate</span>(</span>
<span id="cb12-3">    <span class="at" style="color: #657422;">geom =</span> <span class="st" style="color: #20794D;">"text"</span>,</span>
<span id="cb12-4">    <span class="at" style="color: #657422;">x =</span> <span class="fl" style="color: #AD0000;">0.5</span>, <span class="at" style="color: #657422;">y =</span> <span class="fl" style="color: #AD0000;">0.07</span>,</span>
<span id="cb12-5">    <span class="at" style="color: #657422;">label =</span> <span class="st" style="color: #20794D;">"Merry Christmas"</span>,</span>
<span id="cb12-6">    <span class="at" style="color: #657422;">colour =</span> <span class="st" style="color: #20794D;">"red3"</span>,</span>
<span id="cb12-7">    <span class="at" style="color: #657422;">family =</span> <span class="st" style="color: #20794D;">"mono"</span>,</span>
<span id="cb12-8">    <span class="at" style="color: #657422;">fontface =</span> <span class="st" style="color: #20794D;">"bold"</span>, <span class="at" style="color: #657422;">size =</span> <span class="dv" style="color: #AD0000;">7</span></span>
<span id="cb12-9">  )</span>
<span id="cb12-10">s9</span></code></pre></div>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s8.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom and small random white stars, and two white circles slightly off centre. Five black dots denote two eyes and three buttons on a snowman. Two brown lines look like arms. Two red rectangles form a hat. Orange triangle as a nose."></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/s9.png" class="img-fluid" alt="Dark blue square with off-white rectangle at the bottom and small random white stars, and two white circles slightly off centre. Five black dots denote two eyes and three buttons on a snowman. Two brown lines look like arms. Two red rectangles form a hat. Orange triangle as a nose, with text reading Merry Christmas at the bottom."></p>
</div>
</div>
</div>
</section>
<section id="sending-christmas-cards-in-r" class="level2">
<h2 class="anchored" data-anchor-id="sending-christmas-cards-in-r">Sending Christmas cards in R</h2>
<p>Now that we’ve finished creating our Christmas card, we need to think about how to send it. You could save it as an image file using <code>ggsave()</code>, print it out, and send it in the post. Or you could also use R to send it!</p>
<p>There are many different R packages for sending emails from R. If you create a database of email addresses and names, you could personalise the message on the Christmas card and then send it automatically as an email from R. If you want to automate the process of sending physical cards from R, you might be interested in the <a href="https://github.com/jnolis">{ggirl} package</a> from Jacqueline Nolis <span class="citation" data-cites="ggirl">(Nolis 2023)</span>. {ggirl} allows you to send postcards with a <code>ggplot</code> object printed on the front. {ggirl} is also an incredible example of <a href="https://jnolis.com/blog/introducing_ggirl/">an eCommerce platform built with R</a>! Note that {ggirl} can currently only send physical items to addresses in the United States.</p>
</section>
<section id="other-christmas-r-packages" class="level2">
<h2 class="anchored" data-anchor-id="other-christmas-r-packages">Other Christmas R packages</h2>
<p>If you’re curious about making Christmas cards with R but you don’t have the time to make them from scratch, you’ll likely find the <code>christmas</code> R package <span class="citation" data-cites="christmas">(Barrera-Gomez 2022)</span> helpful. This package from Jose Barrera-Gomez can generate lots of different Christmas cards, many of them animated and available in different languages (English, Catalan and Spanish).</p>
<p>Emil Hvitfeldt has also created a <a href="https://quarto.org/">Quarto</a> <a href="https://github.com/EmilHvitfeldt/quarto-snow">extension that gives the effect of falling snowflakes</a> on HTML outputs – including revealjs slides which is perfect for festive presentations!</p>
<p>Have you made your own Christmas cards with R? We’d love to see your designs!</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container" style="margin-top: 2.25rem;">
<p>Inspired by Nicola’s tutorial, Real World Data Science has indeed made its own Christmas card design. <a href="../../../../../../viewpoints/editors-blog/posts/2023/12/12/rwds-xmas-card.qmd">Check out our attempt over at the Editors’ Blog</a>!</p>
</div>
</div>
</div>
<div class="article-btn">
<p><a href="../../../../../../ideas/tutorials/index.html">Explore more Tutorials</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<strong>Nicola Rennie</strong> is a lecturer in health data science in the Centre for Health Informatics, Computing, and Statistics (CHICAS) within Lancaster Medical School at Lancaster University. She’s an R enthusiast, data visualisation aficionado, and generative artist, among other things. Her personal website is hosted at <a href="https://nrennie.github.io/">nrennie.rbind.io</a>, and she is a co-author of the <a href="https://royal-statistical-society.github.io/datavisguide/">Royal Statistical Society’s Best Practices for Data Visualisation</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Nicola Rennie
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Rennie, Nicola. 2023. “Creating Christmas cards with R.” Real World Data Science, December 12, 2023. <a href="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/xmas-cards.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-christmas" class="csl-entry">
Barrera-Gomez, Jose. 2022. <em>Christmas: Generation of Different Animated Christmas Cards</em>. <a href="https://CRAN.R-project.org/package=christmas">https://CRAN.R-project.org/package=christmas</a>.
</div>
<div id="ref-ggirl" class="csl-entry">
Nolis, Jacqueline. 2023. <em>Ggirl: Ggplot2 Art in Real Life</em>. <a href="https://github.com/jnolis/ggirl">https://github.com/jnolis/ggirl</a>.
</div>
<div id="ref-sf" class="csl-entry">
Pebesma, Edzer. 2018. <span>“<span class="nocase">Simple Features for R: Standardized Support for Spatial Vector Data</span>.”</span> <em><span>The R Journal</span></em> 10 (1): 439–46. <a href="https://doi.org/10.32614/RJ-2018-009">https://doi.org/10.32614/RJ-2018-009</a>.
</div>
<div id="ref-ggforce" class="csl-entry">
Pedersen, Thomas Lin. 2022. <em>Ggforce: Accelerating ’Ggplot2’</em>. <a href="https://CRAN.R-project.org/package=ggforce">https://CRAN.R-project.org/package=ggforce</a>.
</div>
<div id="ref-R-base" class="csl-entry">
R Core Team. 2022. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-ggplot2" class="csl-entry">
Wickham, Hadley. 2016. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="https://ggplot2.tidyverse.org">https://ggplot2.tidyverse.org</a>.
</div>
</div></section></div> ]]></description>
  <category>R</category>
  <category>Data visualisation</category>
  <guid>https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/xmas-cards.html</guid>
  <pubDate>Mon, 11 Dec 2023 23:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/ideas/tutorials/posts/2023/12/12/images/xmas-card.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Evaluating artificial intelligence: How data science and statistics can make sense of AI models</title>
  <dc:creator>Brian Tarran</dc:creator>
  <link>https://realworlddatascience.net/viewpoints/posts/2023/12/06/ai-fringe.html</link>
  <description><![CDATA[ 




<p>A little over a month ago, governments, technology firms, multilateral organisations, and academic and civil society groups came together at Bletchley Park – home of Britain’s World War II code breakers – to discuss the safety and risks of artificial intelligence.</p>
<p>One output from that event was <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">a declaration</a>, signed by countries in attendance, of their resolve to “work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all.”</p>
<p>We also heard from UK prime minister Rishi Sunak of <a href="https://www.gov.uk/government/news/prime-minister-launches-new-ai-safety-institute">plans for an AI Safety Institute</a>, to be based in the UK, which will “carefully test new types of frontier AI before and after they are released to address the potentially harmful capabilities of AI models, including exploring all the risks, from social harms like bias and misinformation, to the most unlikely but extreme risk, such as humanity losing control of AI completely.”</p>
<p>But at a panel debate at the Royal Statistical Society (RSS) the day before the Bletchley Park gathering, data scientists, statisticians, and machine learning experts questioned whether such an institute would be sufficient to meet the challenges posed by AI; whether data inputs – compared to AI model outputs – are getting the attention they deserve; and whether the summit was overly focused on <a href="https://realworlddatascience.net/viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html">AI doomerism</a> and neglecting more immediate risks and harms. There were also calls for AI developers to be more driven to solve real-world problems, rather than just pursuing AI for AI’s sake.</p>
<p>The RSS event was chaired by Andrew Garrett, the Society’s president, and formed part of the national <a href="https://aifringe.org/">AI Fringe programme of activities</a>. The panel featured:</p>
<ul>
<li>Mihaela van der Schaar, John Humphrey Plummer professor of machine learning, artificial intelligence and medicine at the University of Cambridge and a fellow at The Alan Turing Institute.</li>
<li>Detlef Nauck, head of AI and data science research at BT, and a member of the <a href="https://realworlddatascience.net/viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html">Real World Data Science editorial board</a>.</li>
<li>Mark Levene, principal scientist in the Department of Data Science at the National Physical Laboratory.</li>
<li>Martin Goodson, chief executive of Evolution AI, and former chair of the RSS Data Science and AI Section.</li>
</ul>
<p>What follows are some edited highlights and key takeaways from the discussion.</p>
<div class="keyline">
<hr>
</div>
<section id="ai-safety-and-ai-risks" class="level2">
<h2 class="anchored" data-anchor-id="ai-safety-and-ai-risks">AI safety, and AI risks</h2>
<p><strong>Andrew Garrett:</strong> For those who were listening to the commentary last week, the PM [prime minister] made a very interesting speech. Rishi Sunak announced the creation of the world’s first AI Safety Institute in the UK, to examine, evaluate and test new types of AI. He also stated that he pushed hard to agree the first ever international statement about the risks of AI because, in his view, there wasn’t a shared understanding of the risks that we face. He used the example of the IPCC, the Intergovernmental Panel on Climate Change, to establish a truly global panel to publish a “state of AI science” report. And he also announced an investment in raw computing power, so around a billion pounds in a supercomputer, and £2.5 billion in quantum computers, making them available for researchers and businesses as well as government.</p>
<p>The RSS provided two responses this year to prominent [AI policy] reviews. The first was in June <a href="https://rss.org.uk/RSS/media/File-library/Policy/2023/RSS-AI-white-paper-response-v2-2.pdf">on the AI white paper</a>, and the second was on <a href="https://rss.org.uk/RSS/media/File-library/Policy/RSS_Evidence_Communications_and_Digital_Lords_Select_Committee_Inquiry_Large_Language_Models_September_2023.pdf">the House of Lords Select Committee inquiry into large language models</a> back in September. How do they relate to what the PM said? There’s some good news here, and maybe not quite so good news.</p>
<p>First, the RSS had requested investments in AI evaluation and a risk-based approach. And you could argue, by stating that there will be a safety institute, that that certainly ticks one of the boxes. We also recommended investment in open source, in computing power, and in data access. In terms of computing power, that was certainly in the [PM’s] speech. We spoke about strengthening leadership, and in particular including practitioners in the [AI safety] debate. A lot of academics and maybe a lot of the big tech companies have been involved in the debate, but we want to get practitioners – those close to the coalface – involved in the debate. I’m not sure we’ve seen too much of that. We recommended that strategic direction was provided, because it’s such a fast-moving area, and the fact that the Bletchley Park Summit is happening tomorrow, I think, is good for that. And we also recommended that data science capability was built amongst the regulators. I don’t think there was any mention of that.</p>
<p>That’s the context [for the RSS event today]. What I’m going to do now is ask each of the panellists to give an introductory statement around the AI summit, focusing on the safety aspects. What do they see as the biggest risk? And how would they mitigate or manage this risk?</p>
<p><strong>Detlef Nauck:</strong> I work at BT and run the AI and data science research programme. We’ve been looking at the safety, reliability, and responsibility of AI for quite a number of years already. Five years ago, we put up a responsible AI framework in the company, and this is now very much tied into our data governance and risk management frameworks.</p>
<p>Looking at the AI summit, they’re focusing on what they call “frontier models,” and they’re missing a trick here because I don’t think we need to worry about all-powerful AI; we need to worry about inadequate AI that is being used in the wrong context. For me, AI is programming with data, and that means I need to know what sort of data has been used to build the model, and I need AI vendors to be upfront about it and to tell me: What is the data that they have used to build it, how have they built it, or if they’ve tested for bias? And there are no protocols around this. So, therefore, I’m very much in favour of AI evaluation. But I don’t want to wait for an institute for AI evaluation. I want the academic research that needs to be done around this, which hasn’t been done. I want everybody who builds AI systems to take this responsibility and document properly what they’re doing.</p>
<div class="pullquote-container">
<div class="grid">
<div class="g-col-12 g-col-lg-4">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/viewpoints/posts/2023/12/06/images/llm-3d-shapes-crop.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="g-col-12 g-col-lg-8 pullquote-grid pullquote">
<p><img src="https://realworlddatascience.net/images/pullquote-purple.png" class="img-fluid" width="50"></p>
<p>I hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.</p>
</div>
</div>
</div>
<p><strong>Mihaela van der Schaar:</strong> I am an AI researcher building AI and machine learning technology. Before talking about the risks, I also would like to say that I see tremendous potential for good. Many of these machine learning AI models can transform for the better areas that I find extremely important – healthcare and education. That being said, there are substantial risks, and we need to be very careful about that. First, if not designed well, AI can be both unsafe as well as biased, and that could lead to tremendous impact, especially in medicine and education. I completely agree with all the points that the Royal Statistical Society has made not only about open source but also about data access. This AI technology cannot be built unless you have access to high quality data, and what I see a lot happening, especially in industry, is people have data sources that they’ll keep private, build second-rate or third-rate technology on them, and then turn that into commercialised products that are sold to us for a lot of money. If data is made widely available, the best as well as the safest AI can be produced, rather than monopolised.</p>
<p>Another area of risk that I’m especially worried about is human marginalisation. I hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned as an AI researcher about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.</p>
<p><strong>Martin Goodson:</strong> The AI Safety Summit is starting tomorrow. But, unfortunately, I think the government are focusing on the wrong risks. There are lots of risks to do with AI, and if you look at the scoping document for the summit, it says that what they’re interested in is misuse risk and the risk of loss of control. Misuse risk is that bad actors will gain access to information that they shouldn’t have and build chemical weapons and things like that. And the loss of control risk is that we will have this super intelligence which is going to take over and we should see, as is actually mentioned, the risk of the extinction of the human race, which I think is a bit overblown.</p>
<p>Both of these risks – the misuse risk and the loss of control risk – are potential risks. But we don’t really know how likely they are. We don’t even know whether they’re possible. But there are lots of risks that we do know are possible, like loss of jobs, and reductions in salary, particularly of white-collar jobs – that seems inevitable. There’s another risk, which is really important, which is the risk of monopolistic control by the small number of very powerful AI companies. These are the risks which are not just likely but are actually happening now – people are losing their jobs right now because of AI – and in terms of monopolistic control, OpenAI is the only company that has anything like a large language model as powerful as GPT-4. Even the mighty Google can’t really compete. This is a huge risk, I think, because we have no control over pricing: they could raise the prices if they wanted to; they could constrain access; they could only give access to certain people that they want to give access to. We don’t have any control over these systems.</p>
<p><strong>Mark Levene:</strong> I work in NPL as a principal scientist in the data science department. I’m also emeritus professor in Birkbeck, University of London. I have a long-standing expertise in machine learning and focus in NPL on trustworthy AI and uncertainty quantification. I believe that measurement is a key component in locking-in AI safety. Trustworthy AI and safe AI both have similar goals but different emphases. We strive to demonstrate the trustworthiness of an AI system so that we can have confidence in the technology making what we perceive as responsible decisions. Safe AI puts the emphasis on the prevention of harmful consequences. The risk [of AI] is significant, and it could potentially be catastrophic if we think of nuclear power plants, or weapons, and so on. I think one of the problems here is, who is actually going to take responsibility? This is a big issue, and not necessarily an issue for the scientist to decide. Also, who is accountable? For instance, the developers of large language models: are they the ones that are accountable? Or is it the people who deploy the large language models and are fine-tuning them for their use cases?</p>
<p>The other thing I want to emphasise is the socio-technical characteristics [of the AI problem]. We need to get an interdisciplinary team of people to actually try and tackle these issues.</p>
</section>
<section id="do-we-need-an-ai-safety-institute" class="level2">
<h2 class="anchored" data-anchor-id="do-we-need-an-ai-safety-institute">Do we need an AI Safety Institute?</h2>
<p><strong>Andrew Garrett:</strong> Do we need to have an AI Safety Institute, as Rishi Sunak has said? And if we don’t need one, why not?</p>
<p><strong>Detlef Nauck:</strong> I’m more in favour of encouraging academic research in the field and funding the kind of research projects that can look into how to build AI safely, [and] how to evaluate what it does. One of the key features of this technology is it has not come out of academic research; it has been built by large tech companies. And so, I think we have to do a bit of catch up in scientific research and in understanding how are we building these models, what can they do, and how do we control them?</p>
<p><strong>Mihaela van der Schaar:</strong> This technology has a life of its own now, and we are using it for all sorts of things that maybe initially was not even intended. So, shall we create an AI [safety] institute? We can, but we need to realise first that testing AI and showing that it’s safe in all sorts of ways is complicated. I would dare say that doing that well is a big research challenge by itself. I don’t think just one institute will solve it. And I feel the industry needs to bear some of the responsibility. I was very impressed by Professor [Geoffrey] Hinton, who came to Cambridge and said, “I think that some of these companies should invest as much money in making safe AI as developing AI.” I resonated quite a lot with that.</p>
<p>Also, let’s not forget, many academic researchers have two hats nowadays: they are professors, and they are working for big tech [companies] for a lot of money. So, if we take this academic, we put them in this AI tech safety institute, we have potential for corruption. I’m not saying that this will happen. But one needs to be very aware, and there needs to be a very big separation between who develops [AI technology] and who tests it. And finally, we need to realise that we may require an enormous amount of computation to be able to validate and test correctly, and very few academic or governmental organisations may have [that].</p>
<div class="pullquote-container">
<div class="grid">
<div class="g-col-12 g-col-lg-8 pullquote-grid pullquote">
<p><img src="https://realworlddatascience.net/images/pullquote-purple.png" class="img-fluid" width="50"></p>
<p>I think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?</p>
</div>
<div class="g-col-12 g-col-lg-4">
<div class="quarto-figure quarto-figure-right">
<figure class="figure">
<p><img src="https://realworlddatascience.net/viewpoints/posts/2023/12/06/images/llm-3d-shapes-crop.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<p><strong>Martin Goodson:</strong> Can I disagree with this idea of an evaluation institute? I think it’s a really, really bad idea, for two reasons. The first is an argument about fairness. If you look at drug regulation, who pays for clinical trials? It’s not the government. It’s the pharmaceutical companies. They spend billions on clinical trials. So, why do we want to do this testing for free for the big tech companies? We’re just doing product development for them. It’s insane! They should be paying to show that their products are safe.</p>
<p>The other reason is, I think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. I think it’s pathetic. We were one of the main leaders of the Human Genome Project, and we really pushed it – the Wellcome Trust and scientists in the UK pushed the Human Genome Project because we didn’t want companies to have monopolistic control over the human genome. People were idealistic, there was a moral purpose. But now, we’re so reduced that all we can do is test some APIs that have been produced by Silicon Valley companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?</p>
<p><strong>Mark Levene:</strong> Personally, I don’t see any problem in having an AI institute for safety or any other AI institutes. I think what’s important in terms of taxpayers’ money is that whatever institute or forum is invested in, it’s inclusive. One thing that the government should do is, we should have a panel of experts, and this panel should be interdisciplinary. And what this panel can do is it can advise government of the state of play in AI, and advise the regulators. And this panel doesn’t have to be static, it doesn’t have to be the same people all the time.</p>
<p><strong>Andrew Garrett:</strong> To evaluate something, whichever way you chose to do it, you need to have an inventory of those systems. So, with the current proposal, how would this AI Safety Institute have an inventory of what anyone was doing? How would it even work in practice?</p>
<p><strong>Martin Goodson:</strong> Unless we voluntarily go to them and say, “Can you test out our stuff?” then they wouldn’t. That’s the third reason why it’s a terrible idea. You’d need a licencing regime, like for drugs. You’d need to licence AI systems. But teenagers in their bedrooms are creating AI systems, so that’s impossible.</p>
</section>
<section id="lets-do-reality-centric-ai" class="level2">
<h2 class="anchored" data-anchor-id="lets-do-reality-centric-ai">Let’s do reality-centric AI!</h2>
<p><strong>Andrew Garrett:</strong> What are your thoughts about Rishi Sunak wanting the UK to be an AI powerhouse?</p>
<p><strong>Martin Goodson:</strong> It’s not going to be a powerhouse. This stuff about us being world leading in AI, it’s just a fiction. It’s a fairy tale. There are no real supercomputers in the UK. There are moves to build something, like you mentioned in your introduction, Andrew. But what are they going do with it? If they’re just going to build a supercomputer and carry on doing the same kinds of stuff that they’ve been doing for years, they’re not going to get anywhere. There needs to be a big project with an aim. You can build as many computers as you want. But if you haven’t got a plan for what to do with them, what’s the point?</p>
<p><strong>Mihaela van der Schaar:</strong> I really would agree with that. What about solving some real problem: trying to solve cancer; trying to solve our crisis in healthcare, where we don’t have enough infrastructure and doctors to take care of us? What about solving the climate change problem, or even traffic control, or preventing the next financial crisis? I wrote a little bit about that, and I call it “let’s do reality-centric AI.” Let’s have some goal that’s human empowering, take a problem that we have – energy, climate, cancer, Alzheimer’s, better education for children, and more diverse education for children – and let us solve these big challenges, and in the process we will build AI that’s hopefully more human empowering, rather than just saying, “Oh, we are going to solve everything if we have general AI.” Right now, I hear too much about AI for the sake of AI. I’m not sure, despite all the technology we build, that we have advanced in solving some real-world problems that are important for humanity – and imminently important.</p>
<p><strong>Martin Goodson:</strong> So, healthcare– I tried to make an appointment with my GP last week, and they couldn’t get me an appointment for four weeks. In the US you have this United States Medical Licencing Examination, and in order to practice medicine you need to pass all three components, you need to pass them by about 60%. They are really hard tests. GPT-4 for gets over 80% in all three of those. So, it’s perfectly plausible, I think, that an AI could do at least some of the role of the GP. But, you’re right, there is no mission to do that, there is no ambition to do that.</p>
<p><strong>Mihaela van der Schaar:</strong> Forget about replacing the doctors with ChatGPT, which I’m less sure is such a good idea. But, building AI to do the planning of healthcare, to say, “[Patient A], based on what we have found out about you, you’re not as high risk, maybe you can come in four weeks. But [patient B], you need to come tomorrow, because something is worrisome.”</p>
<p><strong>Martin Goodson:</strong> We can get into the details, but I think we are agreeing that a big mission to solve real problems would be a step forward, rather than worrying about these risks of superintelligences taking over everything, which is what the government is doing right now.</p>
</section>
<section id="managing-misinformation" class="level2">
<h2 class="anchored" data-anchor-id="managing-misinformation">Managing misinformation</h2>
<p><strong>Andrew Garrett:</strong> We have some important elections coming up in 2024 and 2025. We haven’t talked much about misinformation, and then disinformation. So, I’m interested to get your views here. How much is that a problem?</p>
<p><strong>Detlef Nauck:</strong> There’s a problem in figuring out when it happens, and that’s something we need to get our heads around. One thing that we’re looking at is, how do we make communication safe from bad actors? How do you know that you’re talking to the person you see on the camera and it’s not a deep fake? Detection mechanisms don’t really work, and they can be circumvented. So, it seems like what we need is new standards for communication systems, like watermarks and encryption built into devices. A camera should be able to say, “I’ve produced this picture, and I have watermarked it and it’s encrypted to a certain level,” and if you don’t see that, you can’t trust that what you see comes from a genuine camera, and it’s not artificially created. It’s more difficult around text and language – you can’t really watermark text.</p>
<p><strong>Mark Levene:</strong> Misinformation is not just a derivative of AI. It’s a derivative of social networks and lots of other things.</p>
<p><strong>Mihaela van der Schaar:</strong> I would agree that this is not only a problem with AI. We need to emphasise the role of education, and lifelong education. This is key to being able to comprehend, to judge for ourselves, to be trained to judge for ourselves. And maybe we need to teach different methods – from young kids to adults that are already working – to really exercise our own judgement. And that brings me to this AI for human empowerment. Can we build AI that is training us to become smarter, to become more able, more capable, more thoughtful, in addition to providing sources of information that are reliable and trustworthy?</p>
<p><strong>Andrew Garrett:</strong> So, empower people to be able to evaluate AI themselves?</p>
<p><strong>Mihaela van der Schaar:</strong> Yes, but not only AI – all information that is given to us.</p>
<p><strong>Martin Goodson:</strong> On misinformation, I think this is really an important topic, because large language models are extremely persuasive. I asked ChatGPT a puzzle question, and it calculated all of this stuff and gave me paragraphs of explanations, and the answer was [wrong]. But it was so convincing I was almost convinced that it was right. The problem is, these things have been trained on the internet and the internet is full of marketing – it’s trillions of words of extremely persuasive writing. So, these things are really persuasive, and when you put that into a political debate or an election campaign, that’s when it becomes really, really dangerous. And that is extremely worrying and needs to be regulated.</p>
<div class="pullquote-container">
<div class="grid">
<div class="g-col-12 g-col-lg-4">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/viewpoints/posts/2023/12/06/images/llm-3d-shapes-crop.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="g-col-12 g-col-lg-8 pullquote-grid pullquote">
<p><img src="https://realworlddatascience.net/images/pullquote-purple.png" class="img-fluid" width="50"></p>
<p>At the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, ‘How did this information come about? Where did it come from?’</p>
</div>
</div>
</div>
<p><strong>Mark Levene:</strong> You need ways to detect it. Even that is a big challenge. I don’t know if it’s impossible, because, if there’s regulation, for example, there should be traceability of data. So, at the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, “How did this information come about? Where did it come from?” But I agree that if you just look at an image or some text, and you don’t know where it came from, it’s easy to believe. Humans are easily fooled, because we’re just the product of what we know and what we’re used to, and if we see something that we recognise, we don’t question it.</p>
</section>
<section id="audience-qa" class="level2">
<h2 class="anchored" data-anchor-id="audience-qa">Audience Q&amp;A</h2>
<section id="how-can-we-help-organisations-to-deploy-ai-in-a-responsible-way" class="level3">
<h3 class="anchored" data-anchor-id="how-can-we-help-organisations-to-deploy-ai-in-a-responsible-way">How can we help organisations to deploy AI in a responsible way?</h3>
<p><strong>Detlef Nauck:</strong> Help for the industry to deploy AI reliably and responsibly is something that’s missing, and for that, trust in AI is one of the things that needs to be built up. And you can only build up trust in AI if you know what these things are doing and they’re properly documented and tested. So that’s the kind of infrastructure, if you like, that’s missing. It’s not all big foundation models. It’s about, how do you actually use this stuff in practice? And 90% of that will be small, purpose-built AI models. That’s an area where the government can help. How do you empower smaller companies that don’t have the background of how AI works and how it can be used, how can they be supported in knowing what they can buy and what they can use and how they can use it?</p>
<p><strong>Mark Levene:</strong> One example from healthcare which comes to mind: when you do a test, let’s say, a blood test, you don’t just get one number, you should get an interval, because there’s uncertainty. What current [AI] models do is they give you one answer, right? In fact, there’s a lot of uncertainty in the answer. One thing that can build trust is to make transparent the uncertainty that the AI outputs.</p>
</section>
<section id="how-can-data-scientists-and-statisticians-help-us-understand-how-to-use-ai-properly" class="level3">
<h3 class="anchored" data-anchor-id="how-can-data-scientists-and-statisticians-help-us-understand-how-to-use-ai-properly">How can data scientists and statisticians help us understand how to use AI properly?</h3>
<p><strong>Martin Goodson:</strong> One big thing, I think, is in culture. In machine learning – academic research and in industry – there isn’t a very scientific culture. There isn’t really an emphasis on observation and experimentation. We hire loads of people coming out of an MSc or a PhD in machine learning, and they don’t know anything, really, about doing an experiment or selection bias or how data can trip you up. All they think about is, you get a benchmark set of data and you measure the accuracy of your algorithm on that. And so there isn’t this culture of scientific experimentation and observation, which is what statistics is all about, really.</p>
<p><strong>Mihaela van der Schaar:</strong> I agree with you, this is where we are now. But we are trying to change it. As a matter of fact, at the next big AI conference, NeurIPS, we plan to do a tutorial to teach people exactly this and bring some of these problems to the forefront, because trying really to understand errors in data, biases, confounders, misrepresentation – this is the biggest problem AI has today. We shouldn’t just build yet another, let’s say, classifier. We should spend time to improve the ability of these machine learning models to deal with all sorts of data.</p>
</section>
<section id="do-we-honestly-believe-yet-another-institute-and-yet-more-regulation-is-the-answer-to-what-were-grappling-with-here" class="level3">
<h3 class="anchored" data-anchor-id="do-we-honestly-believe-yet-another-institute-and-yet-more-regulation-is-the-answer-to-what-were-grappling-with-here">Do we honestly believe yet another institute, and yet more regulation, is the answer to what we’re grappling with here?</h3>
<p><strong>Detlef Nauck:</strong> I think we all agree, another institute is not going to cut it. One of the main problems is regulators are not trained on AI, so it’s the wrong people looking into it. This is where some serious upskilling is required.</p>
</section>
<section id="are-we-wrong-to-downplay-the-existential-or-catastrophic-risks-of-ai" class="level3">
<h3 class="anchored" data-anchor-id="are-we-wrong-to-downplay-the-existential-or-catastrophic-risks-of-ai">Are we wrong to downplay the existential or catastrophic risks of AI?</h3>
<p><strong>Martin Goodson:</strong> If I was an AI, a superintelligent AI, the easiest path for me to cause the extinction of the human race would be to spread misinformation about climate change, right? So, let’s focus on misinformation, because that’s an immediate danger to our way of life. Why are we focusing on science fiction? Let’s focus on reality.</p>
</section>
<section id="ai-tech-has-advanced-but-evaluation-metrics-havent-moved-forward.-why" class="level3">
<h3 class="anchored" data-anchor-id="ai-tech-has-advanced-but-evaluation-metrics-havent-moved-forward.-why">AI tech has advanced, but evaluation metrics haven’t moved forward. Why?</h3>
<p><strong>Mihaela van der Schaar:</strong> First, the AI community that I’m part of innovates at a very fast pace, and they don’t reward metrics. I am a big fan of metrics, and I can tell you, I can publish much faster a method in these top conferences then I can publish a metric. Number two, we often have in AI very stupid benchmarks, where we test everything on one dataset, and these datasets may be very wrong. On a more positive note, this is an enormous opportunity for machine learners and statisticians to work together and advance this very important field of metrics, of test sets, of data generating processes.</p>
<p><strong>Martin Goodson:</strong> The big problem with metrics right now is contamination, because most of the academic metrics and benchmark sets that we’re talking about, they’re published on the internet, and these systems are trained on the internet. I’ve already said that I don’t think this [evaluation] institute should exist. But if it did exist, there’s one thing that they could do, which is important, and that would be to create benchmark datasets that they do not publish. But obviously, you may decide, also, that the traditional idea of having a training set and a test set just doesn’t make any sense anymore. And there are loads of issues with data contamination, and data leakage between the training sets and the test sets.</p>
</section>
</section>
<section id="closing-thoughts-what-would-you-say-to-the-ai-safety-summit" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts-what-would-you-say-to-the-ai-safety-summit">Closing thoughts: What would you say to the AI Safety Summit?</h2>
<p><strong>Andrew Garrett:</strong> If you were at the AI Safety Summit and you could make one point very succinctly, what would it be?</p>
<p><strong>Martin Goodson:</strong> You’re focusing on the wrong things.</p>
<p><strong>Mark Levene:</strong> What’s important is to have an interdisciplinary team that will advise the government, rather than to build these institutes, and that this team should be independent and a team which will change over time, and it needs to be inclusive.</p>
<p><strong>Mihaela van der Schaar:</strong> AI safety is complex, and we need to realise that people need to have the right expertise to be able to really understand the risks. And there is risk, as I mentioned before, of potential collusion, where people are both building the AI and saying it’s safe, and we need to separate these two worlds.</p>
<p><strong>Detlef Nauck:</strong> Focus on the data, not the models. That’s what’s important to build AI.</p>
<div class="article-btn">
<p><a href="../../../../../viewpoints/index.html">Discover more Viewpoints</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Royal Statistical Society
</dd>
</dl>
<p>Images by <a href="https://cream3d.com/">Wes Cockx</a> &amp; <a href="https://deepmind.google/discover/visualising-ai/">Google DeepMind</a> / <a href="https://www.betterimagesofai.org">Better Images of AI</a> / AI large language models / <a href="https://creativecommons.org/licenses/by/4.0/">Licenced by CC-BY 4.0</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Tarran, Brian. 2023. “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” Real World Data Science, December 6, 2023. <a href="https://realworlddatascience.net/viewpoints/posts/2023/12/06/ai-fringe.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>AI</category>
  <category>Large language models</category>
  <category>Accountability</category>
  <category>Regulation</category>
  <category>Metrics</category>
  <category>Events</category>
  <guid>https://realworlddatascience.net/viewpoints/posts/2023/12/06/ai-fringe.html</guid>
  <pubDate>Tue, 05 Dec 2023 23:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/viewpoints/posts/2023/12/06/images/llm-3d-shapes.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Deduplicating and linking large datasets using Splink</title>
  <dc:creator>Robin Linacre</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/11/22/splink.html</link>
  <description><![CDATA[ 




<p>In 2019, the data linking team at the Ministry of Justice was challenged to develop a new data linking methodology to produce new, higher quality linked datasets from the justice system.</p>
<p>The ultimate goal was to share new linked datasets with academic researchers, as part of the ADR UK-funded <a href="https://www.gov.uk/guidance/ministry-of-justice-data-first">Data First programme</a>. These datasets – which include data from prisons, probation, and the criminal and family courts – are now available, and researchers can <a href="https://www.gov.uk/government/publications/moj-data-first-application-form-for-secure-access-to-data">apply for secure access</a>.</p>
<p>The linking methodology is widely applicable and has been published as a free and open source software package called <a href="https://github.com/moj-analytical-services/splink">Splink</a>. The software applies statistical best practice to accurately and quickly link and deduplicate large datasets. The software has now been downloaded over 7 million times, and has been used widely in government, academia and the private sector.</p>
<section id="the-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-problem">The problem</h2>
<p>Data duplication is a ubiquitous problem affecting data quality. Organisations often have multiple records that refer to the same entity but no unique identifier that ties these entities together. Data entry errors and other issues mean that variations usually exist, so the records belonging to a single entity aren’t necessarily identical.</p>
<p>For example, in a company, customer data may have been entered multiple times in multiple different databases, with different spellings of names, different addresses, and other typos. The inability to identify which records belong to each customer presents a data quality problem at all stages of data analysis – from basic questions such as counting the number of unique customers, through to advanced statistical analysis.</p>
<p>With the growing size of datasets held by many organisations, any solution must be able to work on very large datasets of tens of millions of records or more.</p>
</section>
<section id="approach" class="level2">
<h2 class="anchored" data-anchor-id="approach">Approach</h2>
<p>In collaboration with academic experts, the team started with desk research into data linking theory and practice, and a review of existing open source software implementations.</p>
<p>One of the most common theoretical approaches described in the literature is the Fellegi-Sunter model. This statistical model has a long history of application for high profile, important record linking tasks such as in the US Census Bureau and the UK Office for National Statistics (ONS).</p>
<p>The model takes pairwise comparisons of records as an input, and outputs a match score between 0 and 1, which (loosely) can be interpreted as the probability of the two records being a match. Since the record comparison can be either two records from the same dataset, or records from different datasets, this is applicable to both deduplication and linkage problems.</p>
<p>An important benefit of the model is explainability. The model uses a number of parameters, each of which <a href="https://www.robinlinacre.com/partial_match_weights/">has an intuitive explanation</a> that can be understood by a non-technical audience. The relative simplicity of the model also means it is easier to understand and explain how biases in linkage may occur, such as varying levels of accuracy for different ethnic groups.</p>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Consider the following simple record comparison. Are these records a match?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/22/images/record_comparison.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><strong>Figure 1</strong>: Colour coded comparison of two records.</figcaption><p></p>
</figure>
</div>
<p>The parameters of the model are known as partial match weights, which capture the strength of the evidence in favour or against these records being a match.</p>
<p>They can be represented in a chart as follows, in which the highlighted bars correspond to the above example record comparison:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/22/images/partial_match_weights.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><strong>Figure 2</strong>: Chart showing partial match weights of model.</figcaption><p></p>
</figure>
</div>
<p>We can see, for example, that the first name (Robin vs Robyn) is not an exact match, but they have a Jaro-Winkler similarity of above 0.9. As a result, the model ‘activates’ the corresponding partial match weight (in orange). This lends some evidence in favour of a match, but the partial match weight is not as strong as it would have been for an exact match.</p>
<p>Similarly we can see that the non-match on gender leads to the activation (in purple) of a strong negative partial match weight.</p>
<p>The activated partial match weight can then be represented in a waterfall chart as follows, which shows how the final match score is calculated:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/22/images/waterfall.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><strong>Figure 3</strong>: Waterfall chart showing how partial match weights combine to calculate the final prediction.</figcaption><p></p>
</figure>
</div>
<p>The parameter estimates in these charts all have intuitive explanations:</p>
<ul>
<li>The partial match weight on first name is positive, but relatively weak. This makes sense, because the first names are a fuzzy match, not an exact match, so this provides only moderate evidence in favour of the record being a match.</li>
<li>The match weight for the exact match on postcode is stronger than the equivalent weight for surname. This is because the cardinality of the postcode field in the underlying data is higher than the cardinality for surname, so matches on postcode are less likely to occur by chance than matches on surname.</li>
<li>The negative match weight for the mismatch on gender is relatively strong. This reflects the fact that, in this dataset, it’s uncommon for the ‘gender’ field to match amongst truly matching records.</li>
</ul>
<p>The final result is that the model predicts these records are a match, but with only 94% probability: it’s not sure. Most examples would be less ambiguous than this one, and would have a match probability very close to either 0 or 1.</p>
<p>For further details of the theory behind the Fellegi-Sunter model, and a deep dive into the intuitive explanations of the model, I have have developed a <a href="https://www.robinlinacre.com/intro_to_probabilistic_linkage/">series of interactive tutorials</a>.</p>
</section>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>Through our desk research and open source software review, an existing software package called <a href="https://github.com/kosukeimai/fastLink">fastLink</a> was identified which implements the Fellegi-Sunter model, but unfortunately the software is not able to handle very large datasets of more than a few hundred thousand records.</p>
<p>Inspired by the popularity of fastLink, the team quickly realised that the methodology it was developing was generally applicable and could be valuable to a wide range of users if published as a software package.</p>
<p>As we spoke to colleagues across government and beyond, we found record linkage and deduplication problems are pervasive, and crop up in many different guises, meaning that any software needed to be very general and flexible.</p>
<p>The result is Splink – which is a Python package that implements the Fellegi-Sunter model, and enables parameters to be estimated using the Expectation Maximisation algorithm.</p>
<p>The package is free to use, <a href="https://github.com/moj-analytical-services/splink">and open source</a>. It is accompanied by <a href="https://moj-analytical-services.github.io/splink/index.html">detailed documentation</a>, including a <a href="https://moj-analytical-services.github.io/splink/demos/tutorials/00_Tutorial_Introduction.html">tutorial</a> and a set of <a href="https://moj-analytical-services.github.io/splink/demos/examples/examples_index.html">examples</a>.</p>
<p>Splink makes no assumptions about the type of entity being linked, so it is very flexible. We are aware of its use to match data on a variety of entity types including persons, companies, financial transactions and court cases.</p>
<p>The package closely follows the statistical approach described in fastLink. In particular it implements the same mathematical model and likelihood functions described in the <a href="http://imai.fas.harvard.edu/research/files/linkage.pdf">fastLink paper</a> (see pages 354 to 357), with a comprehensive suite of tests to ensure correctness of the implementation.</p>
<p>In addition, Splink introduces a number of innovations:</p>
<ul>
<li>Able to work at massive scale – with proven examples of its use on over 100 million records.</li>
<li>Extremely fast – capable of linking 1 million records on a laptop in around a minute.</li>
<li><a href="https://moj-analytical-services.github.io/splink/charts/index.html">Comprehensive graphical output</a> showing parameter estimates and iteration history make it easier to understand the model and diagnose statistical issues.</li>
<li><a href="https://moj-analytical-services.github.io/splink/charts/waterfall_chart.html">A waterfall chart</a> which can be generated for any record pair, which explains how the estimated match probability is derived.</li>
<li>Support for deduplication, linking, and a combination of both, including support for deduplicating and linking multiple datasets.</li>
<li>Greater customisability of record comparisons, including the ability <a href="https://moj-analytical-services.github.io/splink/topic_guides/comparisons/customising_comparisons.html">to specify custom, user defined comparison functions.</a></li>
<li>Term frequency adjustments on any number of columns.</li>
<li>It’s possible to save a model once it’s been estimated – enabling a model to be estimated, quality assured, and then reused as new data becomes available.</li>
<li>A <a href="https://moj-analytical-services.github.io/splink/">companion website</a> provides a complete description of the various configuration options, and examples of how to achieve different linking objectives.</li>
</ul>
</section>
<section id="using-splink" class="level2">
<h2 class="anchored" data-anchor-id="using-splink">Using Splink</h2>
<p><a href="https://moj-analytical-services.github.io/splink/">Full documentation</a> and <a href="https://moj-analytical-services.github.io/splink/demos/tutorials/00_Tutorial_Introduction.html">a tutorial</a> are available for Splink, but the following snippet gives a simple example of Splink in action:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> splink.datasets <span class="im" style="color: #00769E;">import</span> splink_datasets</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> splink.duckdb.blocking_rule_library <span class="im" style="color: #00769E;">import</span> block_on</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> splink.duckdb.comparison_library <span class="im" style="color: #00769E;">import</span> (</span>
<span id="cb1-4">    exact_match,</span>
<span id="cb1-5">    jaro_winkler_at_thresholds,</span>
<span id="cb1-6">    levenshtein_at_thresholds,</span>
<span id="cb1-7">)</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">from</span> splink.duckdb.linker <span class="im" style="color: #00769E;">import</span> DuckDBLinker</span>
<span id="cb1-9"></span>
<span id="cb1-10">df <span class="op" style="color: #5E5E5E;">=</span> splink_datasets.fake_1000</span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;"># Specify a data linkage model</span></span>
<span id="cb1-13">settings <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb1-14">    <span class="st" style="color: #20794D;">"link_type"</span>: <span class="st" style="color: #20794D;">"dedupe_only"</span>,</span>
<span id="cb1-15">    <span class="st" style="color: #20794D;">"blocking_rules_to_generate_predictions"</span>: [</span>
<span id="cb1-16">      block_on(<span class="st" style="color: #20794D;">"first_name"</span>),</span>
<span id="cb1-17">      block_on(<span class="st" style="color: #20794D;">"surname"</span>),</span>
<span id="cb1-18">    ],</span>
<span id="cb1-19">    <span class="st" style="color: #20794D;">"comparisons"</span>: [</span>
<span id="cb1-20">        jaro_winkler_at_thresholds(<span class="st" style="color: #20794D;">"first_name"</span>, <span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb1-21">        jaro_winkler_at_thresholds(<span class="st" style="color: #20794D;">"surname"</span>),</span>
<span id="cb1-22">        levenshtein_at_thresholds(<span class="st" style="color: #20794D;">"dob"</span>),</span>
<span id="cb1-23">        exact_match(<span class="st" style="color: #20794D;">"city"</span>, term_frequency_adjustments<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>),</span>
<span id="cb1-24">        exact_match(<span class="st" style="color: #20794D;">"email"</span>),</span>
<span id="cb1-25">    ],</span>
<span id="cb1-26">}</span>
<span id="cb1-27"></span>
<span id="cb1-28">linker <span class="op" style="color: #5E5E5E;">=</span> DuckDBLinker(df, settings)</span>
<span id="cb1-29"></span>
<span id="cb1-30"><span class="co" style="color: #5E5E5E;"># Estimate model parameters</span></span>
<span id="cb1-31"></span>
<span id="cb1-32"><span class="co" style="color: #5E5E5E;"># Direct estimation using random sampling can be used for the u probabilities</span></span>
<span id="cb1-33">linker.estimate_u_using_random_sampling(target_rows<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e6</span>)</span>
<span id="cb1-34"></span>
<span id="cb1-35"><span class="co" style="color: #5E5E5E;"># Expectation maximisation is used to train the m values</span></span>
<span id="cb1-36">br_training <span class="op" style="color: #5E5E5E;">=</span> block_on([<span class="st" style="color: #20794D;">"first_name"</span>, <span class="st" style="color: #20794D;">"surname"</span>])</span>
<span id="cb1-37">linker.estimate_parameters_using_expectation_maximisation(br_training)</span>
<span id="cb1-38"></span>
<span id="cb1-39">br_training <span class="op" style="color: #5E5E5E;">=</span> block_on(<span class="st" style="color: #20794D;">"dob"</span>)</span>
<span id="cb1-40">linker.estimate_parameters_using_expectation_maximisation(br_training)</span>
<span id="cb1-41"></span>
<span id="cb1-42"><span class="co" style="color: #5E5E5E;"># Use the model to compute pairwise match scores</span></span>
<span id="cb1-43">pairwise_predictions <span class="op" style="color: #5E5E5E;">=</span> linker.predict()</span>
<span id="cb1-44"></span>
<span id="cb1-45"><span class="co" style="color: #5E5E5E;"># Cluster the match scores into groups to produce a synthetic unique person id</span></span>
<span id="cb1-46">clusters <span class="op" style="color: #5E5E5E;">=</span> linker.cluster_pairwise_predictions_at_threshold(</span>
<span id="cb1-47">  pairwise_predictions, <span class="fl" style="color: #AD0000;">0.95</span></span>
<span id="cb1-48">)</span>
<span id="cb1-49">clusters.as_pandas_dataframe(limit<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
<p>The example shows the flexibility of Splink, and how various types of configuration can be used:</p>
<ul>
<li><strong>How should different data fields be compared?</strong> In this example, the Jaro-Winkler distance is used for names, whereas Levenshtein is used for date of birth since Jaro-Winkler is not appropriate for numeric data.</li>
<li><strong>What blocking rules should be used?</strong> Blocking rules are the primary determinants of how fast Splink will run, but there is a trade off between speed and accuracy. In this case, the input data is small, so the blocking rules are loose.</li>
<li><strong>How should the model parameters be estimated?</strong> In this case, the user has no labels for supervised training, and so uses the unsupervised Expectation Maximisation approach.</li>
<li><strong>Is clustering needed?</strong> In this case, each person may potentially have many duplicates, so clustering is used. This creates an estimated (synthetic) unique identifier for each entity (person) in the input dataset.</li>
</ul>
</section>
<section id="outcomes" class="level2">
<h2 class="anchored" data-anchor-id="outcomes">Outcomes</h2>
<p>Splink has been used to link some of the largest datasets held by the Ministry of Justice as part of the <a href="https://www.gov.uk/guidance/ministry-of-justice-data-first">Data First programme</a>, and researchers are now <a href="https://www.gov.uk/government/publications/moj-data-first-application-form-for-secure-access-to-data">able to apply for secure access to these datasets</a>. Research using this data <a href="https://www.ons.gov.uk/aboutus/whatwedo/statistics/requestingstatistics/onsresearchexcellenceaward">won the ONS Linked Administrative Data Award at the 2022 Research Excellence Awards</a>.</p>
<p>More widely, the demand for Splink has been higher than we expected – with over 7 million downloads. It has been used in other government departments including the Office for National Statistics and internationally, the private sector, and published academic research from top international universities.</p>
<p>Splink has also had external contributions from over 30 people, including staff at the Australian Bureau of Statistics, DataBricks, other government departments, academics, and various private sector consultancies.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container" style="margin-top: 2.25rem;">
<p><strong>Editor’s note</strong>: For more on data linkage, <a href="https://realworlddatascience.net/viewpoints/interviews/posts/2023/10/16/data-sharing-in-gov.html">check out our interview with Helen Miller-Bakewell of the UK Office for Statistics Regulation</a>, discussing the OSR report, <a href="https://osr.statisticsauthority.gov.uk/publication/data-sharing-and-linkage-for-the-public-good/">Data Sharing and Linkage for the Public Good</a>.</p>
</div>
</div>
</div>
<div class="article-btn">
<p><a href="../../../../../case-studies/index.html">Find more case studies</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<strong>Robin Linacre</strong> is an economist, data scientist and data engineer based at the UK Ministry of Justice. He is the lead author of Splink.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Robin Linacre
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/case-studies/posts/2023/11/22/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/case-studies/posts/2023/11/22/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@possessedphotography?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Possessed Photography</a> on <a href="https://unsplash.com/photos/yellow-metal-chain-NwpSBZMhc-M?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Linacre, Robin. 2023. “Deduplicating and linking large datasets using Splink.” Real World Data Science, November 22, 2023. <a href="https://realworlddatascience.net/case-studies/posts/2023/11/22/splink.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>Crime and justice</category>
  <category>Data quality</category>
  <category>Data linkage</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/11/22/splink.html</guid>
  <pubDate>Tue, 21 Nov 2023 23:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/11/22/images/possessed-photography-NwpSBZMhc-M-unsplash.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Learning from failure: ‘Red flags’ in body-worn camera data</title>
  <dc:creator>Noah Wright</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/11/16/learning-from-failure.html</link>
  <description><![CDATA[ 




<p>Incarcerated youth are an exceptionally vulnerable population, and body-worn cameras are an important tool of accountability both for those incarcerated and the staff who supervise them. In 2018 the Texas Juvenile Justice Department (TJJD) deployed body-worn cameras for the first time, and this is a case study of how the agency developed a methodology for measuring the success of the camera rollout. This is also a case study of analysis failure, as it became clear that real-world implementation problems were corrupting the data and rendering the methodology unusable. However, the process of working through the causes of this failure helped the agency identify previously unrecognized problems and ultimately proved to be of great benefit. The purpose of this case study is to demonstrate how negative findings can still be incredibly useful in real-world settings.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Why body-worn cameras?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body" style="margin:0">
<p>Body-worn cameras became a standard tool of policing in the US in the mid-2010s. By recording officer interactions with the public, law enforcement agencies could achieve a greater degree of accountability. Not only could credible claims of police abuse against civilians be easily verified, the argument went, but false accusations would decline as well, saving law enforcement agencies time and resources that would otherwise be wasted on spurious allegations. Initial studies seemed to support this argument.</p>
<p>TJJD faced similar issues to law enforcement agencies, and body-worn cameras seemed like they could be a useful tool. Secure youth residential facilities in Texas all had overhead cameras, but these were very old (they still ran on tape) and captured no audio. This presented a number of problems when it came to deciphering contested incidents, not to mention that these cameras had clearly not prevented any of the agency’s prior scandals from taking place. TJJD received special funding from the legislature to roll out body-worn cameras system-wide, and all juvenile correctional officers were required to wear one.</p>
</div>
</div>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>From the outset of the rollout of body-worn cameras, TJJD faced a major issue with implementation: in 2019, body worn cameras were an established tool for law enforcement, but there was very little literature or best practice to draw from for their use in a correctional environment. Unlike police officers, juvenile correctional officers (JCOs) deal directly with their charges for virtually their entire shift. In an eight-hour shift, a police officer might record a few calls and traffic stops. A juvenile correctional officer, on the other hand, would record for almost eight consecutive hours. And, because TJJD recorded round-the-clock for hundreds of employees at a time, this added up very quickly to <em>a lot</em> of footage.</p>
<p>For example, a typical dorm in a correctional center might have four JCOs assigned to it. Across a single week, these four JCOs would be expected to record at least 160 hours of footage.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/jcos-recording-totals.png" class="img-fluid figure-img" alt="A table illustrating working hours over the course of a week for four juvenile correctional officers"></p>
</figure>
</div>
<div class="figure-caption" style="text-align: center;">
<p><strong>Figure 1:</strong> Four JCOs x 40 hours per week = 160 hours of footage.</p>
</div>
<p>This was replicated across every dorm. Three dorms, for example, would produce nearly 500 hours of footage, as seen below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/jcos-recording-totals-across-dorms.png" class="img-fluid figure-img" alt="A table illustrating working hours over the course of a week for four juvenile correctional officers in each of three dorms in one juvenile correctional facility"></p>
</figure>
</div>
<div class="figure-caption" style="text-align: center;">
<p><strong>Figure 2:</strong> Three dorms x four JCOs x 40 hours per week = 480 hours of footage.</p>
</div>
<p>Finally, we had more than one facility. Four facilities with three dorms each would produce nearly 2,000 hours of footage every week.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/jcos-recording-totals-across-facilities.png" class="img-fluid figure-img" alt="A table illustrating working hours over the course of a week for four juvenile correctional officers in each of three dorms in four separate juvenile correctional facilities"></p>
</figure>
</div>
<div class="figure-caption" style="text-align: center;">
<p><strong>Figure 3:</strong> Four facilities x three dorms x four JCOs x 40 hours per week = 1,960 hours of footage.</p>
</div>
<p>In actuality, we had a total of five facilities each with over a dozen dorms producing an anticipated <strong>17,000 hours</strong> of footage every week – an impossible amount to monitor manually.</p>
<p>As a result, footage review had to be done in a limited, reactive manner. If our monitoring team received an incident report, they could easily zero in on the cameras of the officers involved and review the incident accordingly. But our executive team had hoped to be able to use the footage proactively, looking for “red flags” in order to <em>prevent</em> potential abuses instead of only responding to allegations.</p>
<p>Because the agency had no way of automating the monitoring of footage, any proactive analysis had to be metadata-based. But what to look for in the metadata? Once again, the lack of best-practice literature left us in the lurch. So, we brainstormed ideas for “red flags” and came up with the following that could be screened for using camera metadata:</p>
<ol type="1">
<li><p><strong>Minimal quantity of footage</strong> – our camera policy required correctional officers to have their cameras on at all times in the presence of youth. No footage meant they weren’t using their cameras.</p></li>
<li><p><strong>Frequently turning the camera on and off</strong> – a correctional officer working a dorm should have their cameras always on when around youth and not be turning them on and off repeatedly.</p></li>
<li><p><strong>Large gaps between clips</strong> – it defeats the purpose of having cameras if they’re not turned on.</p></li>
</ol>
<p>In addition, we came up with a fourth red flag, which could be screened for by comparing camera metadata with shift-tracking metadata:</p>
<ol start="4" type="1">
<li><strong>Mismatch between clips recorded and shifts worked</strong> – the agency had very recently rolled out a new shift tracking software. We should expect to see the hours logged by the body cameras roughly match the shift hours worked.</li>
</ol>
</section>
<section id="analysis-part-1-quality-control-and-footage-analysis" class="level2">
<h2 class="anchored" data-anchor-id="analysis-part-1-quality-control-and-footage-analysis">Analysis, part 1: Quality control and footage analysis</h2>
<p>For this analysis, I gathered the most recent three weeks of body-worn camera data – which, at the time, covered April 1–21, 2019. I also pulled data from Shifthound (our shift management software) covering the same time period. Finally, I gathered HR data from CAPPS, the system that most of the State of Texas used at the time for personnel management and finance.<sup>1</sup> I then performed some quality control work, summarized in the dropdown box below.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Initial quality control steps
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><code>SkimR</code> is a helpful <code>R</code> package for exploratory analysis that gives summary statistics for every variable in a data frame, including missing values. After using the <code>skim</code> function on clip data, shift data, and HR data, I noticed that the clip data had some missing values for employee ID. This was an error which pointed to data entry mistakes – body-worn cameras do not record footage on their own, after all, so employee IDs should be assigned to each clip.</p>
<p>From here I compared the employee ID field in the clip data to the employee ID field in the HR data. Somewhat surprisingly, IDs existed in the clip data that did not correspond to any entries in the HR data, indicating yet more data entry mistakes – the HR data is the ground truth for all employee IDs. I checked the shift data for the same error – employee IDs that did not exist in the HR data – and found the same problem.</p>
<p>As well as employee IDs that did not exist in the HR data, I also looked for employee IDs in the footage and shift data which related to staff who were not actually employed between April 1–21, 2019. I found some examples of this, which indicated yet more errors: staff cannot use a body-worn camera or log a shift if they have yet to begin working or if they have been terminated (system permissions are revoked upon leaving employment).</p>
<p>I made a list of every erroneous ID to pass off to HR and monitoring staff before excluding them from the subsequent analysis. In total, 10.6% of clips representing 11.3% of total footage had to be excluded due to these initial data quality issues, foreshadowing the subsequent data quality issues the analysis would uncover.</p>
<p>The full analysis script <a href="https://t.ly/BUNRZ">can be found on GitHub</a>.</p>
</div>
</div>
</div>
<p>In order to operationalize the “red flags” from our brainstorming session, I needed to see what exactly the cameras captured in their metadata. The variables most relevant to our purposes were:</p>
<ul>
<li>Clip start</li>
<li>Clip end</li>
<li>Camera used</li>
<li>Who was assigned to the camera at the time</li>
<li>The role of the person assigned to the camera</li>
</ul>
<p>Using these fields, I first created the following <strong>aggregations per employee ID</strong>:</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/film.png" class="img-fluid figure-img" alt="graphical icon representing a strip of film"></p>
<p></p><figcaption class="figure-caption"><strong>Number of clips</strong> = Number of clips recorded.</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/calendar.png" class="img-fluid figure-img" alt="graphical icon representing a calendar"></p>
<p></p><figcaption class="figure-caption"><strong>Days with footage</strong> = Number of discrete dates that appear in these clips.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/clock.png" class="img-fluid figure-img" alt="graphical icon representing a stopwatch"></p>
<p></p><figcaption class="figure-caption"><strong>Footage hours</strong> = Total duration of all shot footage.</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/caution.png" class="img-fluid figure-img" alt="graphical icon of an exclamation mark inside a circle"></p>
<p></p><figcaption class="figure-caption"><strong>Significant gaps</strong> = Number of clips where the previous clip’s end date was either greater than 15 minutes or less than eight hours before current clip’s start date.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>I used these aggregations to devise the following <strong>staff metrics</strong>:</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/clip-per-day.png" class="img-fluid figure-img" alt="graphical icons representing a strip of film and a calendar"></p>
<p></p><figcaption class="figure-caption"><strong>Clips per day</strong> = Number of clips / Days with footage.</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/footage-per-day.png" class="img-fluid figure-img" alt="graphical icons representing a stopwatch and a calendar"></p>
<p></p><figcaption class="figure-caption"><strong>Footage per day</strong> = Footage hours / Days with footage.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/avg-clip-length.png" class="img-fluid figure-img" alt="graphical icons representing a stopwatch and a strip of film"></p>
<p></p><figcaption class="figure-caption"><strong>Average clip length</strong> = Footage hours / Number of clips.</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/gaps-per-day.png" class="img-fluid figure-img" alt="graphical icons of an exclamation mark inside a circle and a calendar"></p>
<p></p><figcaption class="figure-caption"><strong>Gaps per day</strong> = Gaps / Days with footage.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>Once I established these metrics for each employee I looked at their respective distributions. Standard staff shift lengths at the time were eight hours. If staff were using their cameras appropriately, we would expect to see distributions centered around clip lengths of about an hour, eight or fewer clips per day, and 8-12 footage hours per day. We would also expect to see 0 large gaps.</p>
<details>
<summary>
Show the code
</summary>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><span class="in" style="color: #5E5E5E;">```{r}</span></span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-3"></span>
<span id="cb1-4">Footage_Metrics_by_Employee <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">read_csv</span>(<span class="st" style="color: #20794D;">"Output/Footage Metrics by Employee.csv"</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6">Footage_Metrics_by_Employee <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-7">  <span class="fu" style="color: #4758AB;">select</span>(<span class="sc" style="color: #5E5E5E;">-</span>Clips, <span class="sc" style="color: #5E5E5E;">-</span>Days_With_Footage, <span class="sc" style="color: #5E5E5E;">-</span>Footage_Hours, <span class="sc" style="color: #5E5E5E;">-</span>Gaps) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="sc" style="color: #5E5E5E;">-</span>Employee_ID, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">"Metric"</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"Value"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-9">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> Value)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-10">  <span class="fu" style="color: #4758AB;">geom_histogram</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-11">  <span class="fu" style="color: #4758AB;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;">~</span>Metric, <span class="at" style="color: #657422;">scales =</span> <span class="st" style="color: #20794D;">"free"</span>)</span>
<span id="cb1-12"><span class="in" style="color: #5E5E5E;">```</span></span></code></pre></div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/fig-1.png" class="img-fluid figure-img" alt="Four histograms of key metrics - average clip length, average number of clips per day, average footage hours per day, and average gaps per day."></p>
</figure>
</div>
<p>By eyeballing the distributions I could tell most staff were recording fewer than 10 clips per day, shooting about 0.5–2 hours for each clip, for a total of 2–10 hours of daily footage, with the majority of employees having less than one significant gap per day. Superficially, this appeared to provide evidence of widespread attempts at complying with the body-worn camera policy and no systemic rejection or resistance. If this were indeed the case, then we could turn our attention to individual outliers.</p>
<p>First, though, we thought we would attempt to validate this initial impression by testing another assumption. If each employee works on average 40 hours per week – a substantial underestimate given how common overtime was – we should expect, over a three-week period, to see about 120 hours of footage per employee in the dataset. This is <em>not</em> what we found.</p>
<p>Average footage per employee was 70.2 hours over the three-week period, meaning that the average employee was recording less than 60% of shift hours worked. With so many hours going unrecorded for unknown reasons, we needed to investigate further.</p>
<p>Surely the shift data would clarify this…</p>
</section>
<section id="analysis-part-2-footage-and-shift-comparison" class="level2">
<h2 class="anchored" data-anchor-id="analysis-part-2-footage-and-shift-comparison">Analysis, part 2: Footage and shift comparison</h2>
<p>With the data on shifts worked from our timekeeping system, I could theoretically compare actual shifts worked to the amount of footage recorded. If there were patterns in where the gaps in footage fell, that comparison might help to explain why.</p>
<p>In order to join the shift data to the camera data, I needed a common unit of analysis beyond “Employee ID.” Using only this value would produce a nonsensical table that joined up every clip of footage to every shift worked.</p>
<p>For example, let’s take employee #9001005 at Facility Epsilon between April 1–3. This employee has the following clips recorded during that time period:</p>
<div class="table-responsive">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Employee_ID</th>
<th style="text-align: left;">Clip_ID</th>
<th style="text-align: left;">Clip_Start</th>
<th style="text-align: left;">Clip_End</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">156421</td>
<td style="text-align: left;">2019-04-01 05:54:34</td>
<td style="text-align: left;">2019-04-01 08:34:34</td>
</tr>
<tr class="even">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">155093</td>
<td style="text-align: left;">2019-04-01 08:40:59</td>
<td style="text-align: left;">2019-04-01 08:54:51</td>
</tr>
<tr class="odd">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">151419</td>
<td style="text-align: left;">2019-04-01 09:03:16</td>
<td style="text-align: left;">2019-04-01 11:00:30</td>
</tr>
<tr class="even">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">153133</td>
<td style="text-align: left;">2019-04-01 11:10:09</td>
<td style="text-align: left;">2019-04-01 12:39:51</td>
</tr>
<tr class="odd">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">151088</td>
<td style="text-align: left;">2019-04-01 12:57:51</td>
<td style="text-align: left;">2019-04-01 14:06:44</td>
</tr>
<tr class="even">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">150947</td>
<td style="text-align: left;">2019-04-02 05:56:34</td>
<td style="text-align: left;">2019-04-02 09:48:50</td>
</tr>
<tr class="odd">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">151699</td>
<td style="text-align: left;">2019-04-02 09:54:23</td>
<td style="text-align: left;">2019-04-02 12:17:15</td>
</tr>
</tbody>
</table>
</div>
<p>We can join this to a similar table of shifts logged. This particular employee had the following shifts scheduled from April 1–3:</p>
<div class="table-responsive">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Employee_ID</th>
<th style="text-align: left;">Shift_ID</th>
<th style="text-align: left;">Shift_Start</th>
<th style="text-align: left;">Shift_End</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">E050603</td>
<td style="text-align: left;">2019-04-01 06:00:00</td>
<td style="text-align: left;">2019-04-01 14:00:00</td>
</tr>
<tr class="even">
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">E051303</td>
<td style="text-align: left;">2019-04-02 06:00:00</td>
<td style="text-align: left;">2019-04-02 14:00:00</td>
</tr>
</tbody>
</table>
</div>
<p>The table shows two eight-hour morning shifts from 6:00 am to 2:00 pm. We can join the two tables together by ID on a messy many-to-many join, but that tells us nothing about how much they overlap (or fail to overlap) without extensive additional work. For example, we have a unique identifier for employee clip (Clip_ID) and employee shift (Shift_ID), but what we need is a unique identifier that can be used to join the two. Fortunately, for this particular data we can <em>create</em> a unique identifier since both clips and shifts are fundamentally measures of <em>time</em>. While Employee_ID is not in itself unique (i.e., one employee can have multiple clips attached to that ID), Employee_ID combined with time of day is unique. A person can only be in one place at a time, after all!</p>
<p>To reshape the data for joining, I created a function that takes any data frame with a start and end column and unfolds it into discrete units of time. Using the code below to create the “Interval_Convert” function, the shift data above for employee 9001005 converts into one entry per hour of the day per shift. As a result, two eight-hour shifts get turned into 16 employee hours (a sample of which is shown below).</p>
<details>
<summary>
Show the code
</summary>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><span class="in" style="color: #5E5E5E;">```{r}</span></span>
<span id="cb2-2"><span class="fu" style="color: #4758AB;">library</span>(sqldf)</span>
<span id="cb2-3"><span class="fu" style="color: #4758AB;">library</span>(lubridate)</span>
<span id="cb2-4"></span>
<span id="cb2-5">Interval_Convert <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(DF, Start_Col, End_Col, Int_Unit, <span class="at" style="color: #657422;">Int_Length =</span> <span class="dv" style="color: #AD0000;">1</span>) {</span>
<span id="cb2-6"><span class="fu" style="color: #4758AB;">browser</span>()</span>
<span id="cb2-7">  Start_Col2 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">enquo</span>(Start_Col)</span>
<span id="cb2-8">  End_Col2 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">enquo</span>(End_Col)</span>
<span id="cb2-9">  </span>
<span id="cb2-10">  Start_End <span class="ot" style="color: #003B4F;">&lt;-</span> DF <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-11">    <span class="fu" style="color: #4758AB;">ungroup</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-12">    <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">Min_Start =</span> <span class="fu" style="color: #4758AB;">min</span>(<span class="sc" style="color: #5E5E5E;">!!</span>Start_Col2),</span>
<span id="cb2-13">              <span class="at" style="color: #657422;">Max_End =</span> <span class="fu" style="color: #4758AB;">max</span>(<span class="sc" style="color: #5E5E5E;">!!</span>End_Col2)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-14">    <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Start =</span> <span class="fu" style="color: #4758AB;">floor_date</span>(Min_Start, Int_Unit),</span>
<span id="cb2-15">           <span class="at" style="color: #657422;">End =</span> <span class="fu" style="color: #4758AB;">ceiling_date</span>(Max_End, Int_Unit))</span>
<span id="cb2-16">  </span>
<span id="cb2-17">  DF <span class="ot" style="color: #003B4F;">&lt;-</span> DF <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-18">    <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Single =</span> <span class="sc" style="color: #5E5E5E;">!!</span>Start_Col2 <span class="sc" style="color: #5E5E5E;">==</span> <span class="sc" style="color: #5E5E5E;">!!</span>End_Col2)</span>
<span id="cb2-19">  </span>
<span id="cb2-20">  Interval_Table <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">Interval_Start =</span> <span class="fu" style="color: #4758AB;">seq.POSIXt</span>(Start_End<span class="sc" style="color: #5E5E5E;">$</span>Start[<span class="dv" style="color: #AD0000;">1</span>], Start_End<span class="sc" style="color: #5E5E5E;">$</span>End[<span class="dv" style="color: #AD0000;">1</span>], <span class="at" style="color: #657422;">by =</span> <span class="fu" style="color: #4758AB;">str_c</span>(Int_Length, <span class="st" style="color: #20794D;">" "</span>, Int_Unit))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-21">    <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Interval_End =</span> <span class="fu" style="color: #4758AB;">lead</span>(Interval_Start)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-22">    <span class="fu" style="color: #4758AB;">filter</span>(<span class="sc" style="color: #5E5E5E;">!</span><span class="fu" style="color: #4758AB;">is.na</span>(Interval_End))</span>
<span id="cb2-23">  </span>
<span id="cb2-24">  by <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">join_by</span>(Interval_Start <span class="sc" style="color: #5E5E5E;">&lt;=</span> <span class="sc" style="color: #5E5E5E;">!!</span>End_Col2, Interval_End <span class="sc" style="color: #5E5E5E;">&gt;=</span> <span class="sc" style="color: #5E5E5E;">!!</span>Start_Col2)  </span>
<span id="cb2-25">  </span>
<span id="cb2-26">  Interval_Data_Table <span class="ot" style="color: #003B4F;">&lt;-</span> Interval_Table <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-27">    <span class="fu" style="color: #4758AB;">left_join</span>(DF, by) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-28">    <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">Seconds_Duration_Within_Interval =</span> <span class="fu" style="color: #4758AB;">if_else</span>(<span class="sc" style="color: #5E5E5E;">!!</span>End_Col2 <span class="sc" style="color: #5E5E5E;">&gt;</span> Interval_End, Interval_End, <span class="sc" style="color: #5E5E5E;">!!</span>End_Col2) <span class="sc" style="color: #5E5E5E;">-</span></span>
<span id="cb2-29">             <span class="fu" style="color: #4758AB;">if_else</span>(<span class="sc" style="color: #5E5E5E;">!!</span>Start_Col2 <span class="sc" style="color: #5E5E5E;">&lt;</span> Interval_Start, Interval_Start, <span class="sc" style="color: #5E5E5E;">!!</span>Start_Col2)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-30">    <span class="fu" style="color: #4758AB;">filter</span>(<span class="sc" style="color: #5E5E5E;">!</span>(Single <span class="sc" style="color: #5E5E5E;">&amp;</span> Interval_End <span class="sc" style="color: #5E5E5E;">==</span> <span class="sc" style="color: #5E5E5E;">!!</span>Start_Col2),</span>
<span id="cb2-31">           <span class="fu" style="color: #4758AB;">as.numeric</span>(Seconds_Duration_Within_Interval) <span class="sc" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb2-32">  </span>
<span id="cb2-33">  <span class="fu" style="color: #4758AB;">return</span>(Interval_Data_Table)</span>
<span id="cb2-34">}</span>
<span id="cb2-35"><span class="in" style="color: #5E5E5E;">```</span></span></code></pre></div>
</details>
<div class="table-responsive">
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Interval_Start</th>
<th style="text-align: left;">Interval_End</th>
<th style="text-align: left;">Employee_ID</th>
<th style="text-align: left;">Shift_ID</th>
<th style="text-align: left;">Shift_Start</th>
<th style="text-align: left;">Shift_End</th>
<th style="text-align: left;">Seconds_Duration_Within_Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">2019-04-01 06:00:00</td>
<td style="text-align: left;">2019-04-01 07:00:00</td>
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">E050603</td>
<td style="text-align: left;">2019-04-01 06:00:00</td>
<td style="text-align: left;">2019-04-01 14:00:00</td>
<td style="text-align: left;">3600 secs</td>
</tr>
<tr class="even">
<td style="text-align: left;">2019-04-01 07:00:00</td>
<td style="text-align: left;">2019-04-01 08:00:00</td>
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">E050603</td>
<td style="text-align: left;">2019-04-01 06:00:00</td>
<td style="text-align: left;">2019-04-01 14:00:00</td>
<td style="text-align: left;">3600 secs</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2019-04-01 08:00:00</td>
<td style="text-align: left;">2019-04-01 09:00:00</td>
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">E050603</td>
<td style="text-align: left;">2019-04-01 06:00:00</td>
<td style="text-align: left;">2019-04-01 14:00:00</td>
<td style="text-align: left;">3600 secs</td>
</tr>
<tr class="even">
<td style="text-align: left;">2019-04-01 09:00:00</td>
<td style="text-align: left;">2019-04-01 10:00:00</td>
<td style="text-align: left;">9001005</td>
<td style="text-align: left;">E050603</td>
<td style="text-align: left;">2019-04-01 06:00:00</td>
<td style="text-align: left;">2019-04-01 14:00:00</td>
<td style="text-align: left;">3600 secs</td>
</tr>
<tr class="odd">
<td style="text-align: left;">…</td>
<td style="text-align: left;">…</td>
<td style="text-align: left;">…</td>
<td style="text-align: left;">…</td>
<td style="text-align: left;">…</td>
<td style="text-align: left;">…</td>
<td style="text-align: left;">…</td>
</tr>
</tbody>
</table>
</div>
<p>The footage could be converted in a similar manner, and in this way I could break down both the shift data and the clip data into an hour-by-hour view and compare them to one another. Using this new format, I joined together the full tables of footage and shifts to determine how much footage was recorded with no corresponding shift in the timekeeping system.</p>
<div class="table-responsive">
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 34%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">HR_Location</th>
<th style="text-align: left;">Footage_Hours_No_Shift</th>
<th style="text-align: left;">Employee_IDs_With_Missing_Shift</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Alpha</td>
<td style="text-align: left;">1805</td>
<td style="text-align: left;">122</td>
</tr>
<tr class="even">
<td style="text-align: left;">Beta</td>
<td style="text-align: left;">3749</td>
<td style="text-align: left;">114</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Delta</td>
<td style="text-align: left;">1208</td>
<td style="text-align: left;">133</td>
</tr>
<tr class="even">
<td style="text-align: left;">Epsilon</td>
<td style="text-align: left;">2899</td>
<td style="text-align: left;">157</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gamma</td>
<td style="text-align: left;">4153</td>
<td style="text-align: left;">170</td>
</tr>
</tbody>
</table>
</div>
<p>To summarize what the table is telling us: Almost every employee has footage hours that do not match with logged shifts, totaling nearly 14,000 hours when you add up the Footage_Hours_No_Shift column. But what about the opposite case? How many shift hours were logged with no corresponding footage?</p>
<div class="table-responsive">
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 33%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">HR_Location</th>
<th style="text-align: left;">Shift_Hours_No_Footage</th>
<th style="text-align: left;">Employee_IDs_With_Missing_Footage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Alpha</td>
<td style="text-align: left;">7338</td>
<td style="text-align: left;">127</td>
</tr>
<tr class="even">
<td style="text-align: left;">Beta</td>
<td style="text-align: left;">6014</td>
<td style="text-align: left;">118</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Delta</td>
<td style="text-align: left;">12830</td>
<td style="text-align: left;">141</td>
</tr>
<tr class="even">
<td style="text-align: left;">Epsilon</td>
<td style="text-align: left;">9000</td>
<td style="text-align: left;">168</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gamma</td>
<td style="text-align: left;">11960</td>
<td style="text-align: left;">183</td>
</tr>
</tbody>
</table>
</div>
<p>Oh dear. Again, almost every employee has logged shift hours with no footage: 47,000 hours in total. To put it another way, that’s an entire work week per employee not showing up in camera footage.</p>
<p>At this point, we could probably rule out deliberate noncompliance. The clip data already implied that most employees were following the policy, and our facility leadership would surely have noticed a mass refusal large enough to show up this clearly in the data.</p>
<p>One way to check for deliberate noncompliance would be to first exclude shifts that contain zero footage whatsoever. This would rule out total mismatches, where – for whatever reason – the logged shifts had totally failed to overlap with recorded clips. For the remaining shifts that <em>do</em> contain footage, we could look at the proportion of the shift covered by footage. So, if an eight-hour shift had four hours of recorded footage associated with it, then we could say that 50% of the shift had been recorded. The following histogram is a distribution of the number of employees organized by the percent of their shift-hours they recorded (but only shifts that had a nonzero amount of footage).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/fig-2.png" class="img-fluid figure-img" alt="A histogram of average percent of shifts recorded, excluding shifts with no recorded footage."></p>
</figure>
</div>
<p>As it turned out, most employees recorded the majority of their matching shifts, a finding that roughly aligns with the initial clip analysis. So, what explains the 14,000 hours of footage with no shifts, and the 47,000 hours of shifts with no footage?</p>
</section>
<section id="causes-of-failure" class="level2">
<h2 class="anchored" data-anchor-id="causes-of-failure">Causes of failure</h2>
<p>Here, I believed, we had reached the end of what I could do with data alone, and so I presented these findings (or lack thereof) to executive leadership. The failure to gather reliable data from linking the clip data to the shift data prompted follow-ups into what exactly was going wrong. As it turned out, <em>many</em> things were going wrong.</p>
<p>First, a number of technical problems plagued the early rollout of the cameras:</p>
<ul>
<li><p>All of our facilities suffered from high turnover, and camera ownership was not consistently updated. Employees who no longer worked at the agency could therefore appear in the clip data – somebody else had taken over their camera but had not put their name and ID on it.</p></li>
<li><p>We had no way of telling if a camera was not recording due to being docked and recharging or not recording due to being switched off.</p></li>
<li><p>In the early days of the rollout, footage got assigned to an employee based on the owner of the <em>dock</em>, not the camera. In other words, if Employee A had recorded their shift with their camera but uploaded the footage using a dock assigned to Employee B then the footage would show up in the system as belonging to Employee B.</p></li>
</ul>
<p>The shift data was, unsurprisingly, even worse, and it was here we came across our most important finding. While the evidence showed that there wasn’t any widespread non-compliance with the use of the cameras, there <em>was</em> widespread non-compliance with the use of our shift management software. Details are included in the dropdown box below.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Quality issues in shift tracking data
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Our HR system, CAPPS, had a feature that tracked hours worked in order to calculate leave and overtime pay. However, CAPPS was a statewide application designed for 9–5 office workers, and could not capture the irregular working hours of our staff (much less aid in planning future shifts). We had obtained separate shift management software to fill these gaps, but not realized how inconsistently it was being used. All facilities were required to have their employees log their shifts, but some followed through on this better than others. And even for those that did make a good-faith effort at follow-through, quality control was nonexistent.</p>
<p>In CAPPS, time entry determined pay, so strong incentives existed to ensure accurate entry. But for our shift management software, no incentives existed at all for making sure that entries were correct. For example, a correctional officer could have a 40-hour work week scheduled in the shift software but miss the entire week due to an injury, and the software would still show them as having worked 40 hours that week. Nobody bothered to go back and correct these types of errors because there was no reason to.</p>
<p>The software was intended to be used proactively for planning purposes, not after-the-fact for logging and tracking purposes. Thus, it produced data that was totally inconsistent with actual hours worked, which became apparent when compared to data (like body-worn camera footage) that tracked actual hours on the floor.</p>
<p>In the end, we had to rethink a number of aspects of the shift software’s implementation. In the process of these fixes, leadership also came to make explicit that the software’s primary purpose was to help facilities schedule future shifts, not audit hours worked after the fact (which CAPPS already did, just on a day-by-day basis as opposed to an hour-by-hour basis). This analysis was the only time we attempted to use the shift data in this manner.</p>
</div>
</div>
</div>
</section>
<section id="what-we-learned-from-failure" class="level2">
<h2 class="anchored" data-anchor-id="what-we-learned-from-failure">What we learned from failure</h2>
<p>Whatever means we used to monitor compliance with the camera policy, we learned that it couldn’t be fully automated. The agency followed up this analysis with a random sampling approach, in which monitors would randomly select times of day they knew a given staff member would have to have their cameras turned on and actually watch the associated clips. This review process confirmed the first impressions from the statistical review above: most employees <em>were</em> making good faith attempts at complying with the policy despite technical glitches, short-staffing, and administrative confusion. It also confirmed that proactive monitoring of correctional officers was a human process which had to come from supervisors and staff.</p>
<p>The one piece of the analysis we did use going forward was the clip analysis (converted into a Power BI dashboard and included in the <a href="https://github.com/enndubbs/Body-Worn-Camera-Monitoring">GitHub repository</a> for this article), but only as a supplement for already-launched investigations, not a prompt for one. Body-worn camera footage remained immensely useful for investigations after-the-fact, but inconsistencies in clip data were not, in and of themselves, particularly noteworthy “red flags.” At the end of the day, analytics can contextualize and enhance human judgment, but it cannot replace it.</p>
<p>In academia, the bias in favor of positive findings is well-documented. The failure to find something, or a lack of statistical significance, does not lend itself to publication in the same way that a novel discovery does. But, in an applied setting, where results matter more than publication criteria, negative findings can be highly insightful. They can falsify erroneous assumptions, bring unknown problems to light, and prompt the creation of new processes and tools. In this context, a failure is only truly a failure if nothing is learned from it.</p>
<div class="article-btn">
<p><a href="../../../../../case-studies/index.html">Find more case studies</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<strong>Noah Wright</strong> is a data scientist with the Texas Juvenile Justice Department. He is interested in the applications of data science to public policy in the context of real-world constraints, and the ethics thereof (ethics being highly relevant in his line of work). He can be reached on <a href="https://www.linkedin.com/in/noahdwright/">LinkedIn</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Noah Wright
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/case-studies/posts/2023/11/16/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/case-studies/posts/2023/11/16/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Wright, Noah. 2023. “Learning from failure: ‘Red flags’ in body-worn camera data.” Real World Data Science, November 16, 2023. <a href="https://realworlddatascience.net/case-studies/posts/2023/11/16/learning-from-failure.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The underlying data for the analysis as presented in this article was requested through the Texas Public Information Act and went through TJJD’s approval process for ensuring anonymity of records. It is available on <a href="https://github.com/enndubbs/Body-Worn-Camera-Monitoring">GitHub</a> along with the rest of the code used to write this article.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Crime and justice</category>
  <category>Public policy</category>
  <category>Data quality</category>
  <category>Data analysis</category>
  <category>Monitoring</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/11/16/learning-from-failure.html</guid>
  <pubDate>Wed, 15 Nov 2023 23:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/11/16/images/bodycam-monitor.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>How to ‘open science’: A brief guide to principles and practices</title>
  <dc:creator>Isabel Sassoon</dc:creator>
  <link>https://realworlddatascience.net/ideas/posts/2023/11/06/how-to-open-science.html</link>
  <description><![CDATA[ 




<p><a href="https://www.fosteropenscience.eu/learning/what-is-open-science/#/id/5ab8ea32dd1827131b90e3ac">Open science</a> is about making your research freely accessible to others. This includes your data, your code and any outputs (such as reports or articles).</p>
<p>Many people in research, or working or studying in higher education, will be familiar with open science as a concept. As a lecturer, I was aware of it and frequently made use of open data for teaching and research, but it was not until it became a requirement from my funder that I took the opportunity to run my own research as open science by design.</p>
<p>Most tools that I was already familiar with could be used to support open science, but I soon realised that there were some steps and planning that I first needed to learn. As I discovered more about the processes and principles of open science, I came to see that making my research open would not require much additional time and effort. However, I felt that a succinct guide to open science would certainly help me – and others – to make the transition more easily. So, I set out to write such a guide.</p>
<p>This is the result! It is not meant to be an exhaustive document. Rather, I will explain the route I took to open science and what options are out there for others looking to follow suit.</p>
<section id="what-is-open-science" class="level2">
<h2 class="anchored" data-anchor-id="what-is-open-science">What is open science?</h2>
<p>“Open science refers to the process of making the content and process of producing evidence and claims transparent and accessible to others” <span class="citation" data-cites="munafo2017manifesto">(Munafò et al. 2017)</span>. The open science principles are:</p>
<dl>
<dt>Open source</dt>
<dd>
Any data, code or output is accessible and usable in software that is freely available and with an open license. What this means in practice is that, for example, when sharing data, the .csv format is used rather than .xlsx, as the latter requires closed source software (Microsoft Excel) to run.
</dd>
<dt>Open data</dt>
<dd>
Research data should be freely accessible. One approach to open data is to adhere to the FAIR Data Principles <span class="citation" data-cites="wilkinson2016fair">(Wilkinson et al. 2016)</span>. FAIR stands for Findable, Accessible, Interoperable, and Reusable, and these principles can be implemented as a step to help make your work open science. However, they are not the only way, nor are they a guarantee that your work will automatically meet the definition of “open science” if you implement them.
</dd>
<dt>Open access</dt>
<dd>
Access to published papers and/or outputs is freely available to all. This can be achieved, for example, by sharing published papers in a pre-print server.
</dd>
</dl>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
What is a pre-print server?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Pre-print servers are online repositories that enable you to share versions of your manuscript before or while your manuscript is under review. Examples of such repositories include <a href="https://arxiv.org/">ArXiv</a> and <a href="https://www.medrxiv.org/">MedRxiv</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/ideas/posts/2023/11/06/images/pre-print.png" class="img-fluid figure-img" alt="Screengrab of the MedRxiv page for a paper titled 'Why one size fits all is not enough when designing COVID-19 immunity certificates for domestic use: a UK wide cross-sectional online survey.'"></p>
</figure>
</div>
<div class="figure-caption" style="text-align: center;">
<p>Pre-print server example from MedRxiv.</p>
</div>
</div>
</div>
</div>
<p>One additional benefit of open science is that it supports <a href="https://realworlddatascience.net/case-studies/posts/2023/06/15/road-to-reproducible-research.html">reproducible research</a>. This means that others can download your data and code, re-run the analysis, and see if they obtain the same results. To get the full benefit of open science and promote reproducibility, code needs to be written with enough explanations or comments to help others understand the logic of the various stages of an analysis.</p>
</section>
<section id="steps-to-open-science" class="level2">
<h2 class="anchored" data-anchor-id="steps-to-open-science">Steps to open science</h2>
<p>In this section, I will outline steps you can take to easily make your research open science. There will be situations where it is not possible to make all aspects of research open – for example, due to privacy and consent issues related to data. It is still possible to share some elements of such projects, but potentially this involves additional work – to create suitable demo data, say, or generate synthetic data in order to provide data that has comparable trends but preserves privacy. It may also be possible to share the data when it is requested on a case-by-case basis. I am not going to cover this here, but it is worth considering whether open science is possible in each case.</p>
<section id="before-you-begin" class="level3">
<h3 class="anchored" data-anchor-id="before-you-begin">Before you begin…</h3>
<p>Pre-registering an analysis plan for your research helps establish that your research is confirmatory (hypothesis testing) rather than exploratory (hypothesis generating). If you have some hypotheses or research questions that are the foundation of your research, it is worth pre-registering. If your research is exploratory, pre-registration is not necessarily applicable. Although pre-registration in itself is not a requirement for open science, the process of pre-registration can all be completed within repositories such as the Open Science Framework (OSF). Pre-registering your analysis plan will add value and rigour to you research.</p>
<p>If your research doesn’t require pre-registration, jump straight to Step 1.</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
What is pre-registration?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Pre-registration involves completing a form before you start your analysis to explain the primary research questions, the covariates of interest, and the methods you plan to use and why. <span class="citation" data-cites="haroz_2022">Haroz (2022)</span> provides more detail on how apps like OSF, Zenodo and Figshare support pre-registration. <a href="https://www.youtube.com/watch?v=_505Oek-wHM">This video</a> also gives more details.</p>
<p>Below is an example of a pre-registration.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/ideas/posts/2023/11/06/images/pre-registration.png" class="img-fluid figure-img" alt="Screengrab of a pre-registration document, detailing a research project's hypothesis, study type, and study design."></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="step-1" class="level3">
<h3 class="anchored" data-anchor-id="step-1">Step 1</h3>
<p>Does your research plan require you to write a lot of code for analysis purposes, perhaps in collaboration with others? If the answer is <strong>No</strong>, skip to Step 2. If <strong>Yes</strong>:</p>
<ul>
<li>Consider setting up a GitHub repository (or repo), especially if this is a collaborative project and it is likely that more than one person will be working on the code. Don’t forget to invite your collaborators to join the repo!</li>
<li>GitHub repos can be set to private and then made public at the appropriate time, so development work can take place behind closed doors and then released to the wider world when ready.</li>
<li>Ensure that your code is commented properly so that it is reusable and, eventually, your results are reproducible.</li>
</ul>
</section>
<section id="step-2" class="level3">
<h3 class="anchored" data-anchor-id="step-2">Step 2</h3>
<p>GitHub is a great tool for developing code collaboratively, but it may not be right for you – or indeed the only tool to use – if you have a lot of other material to work with and release as part of your research project. If that’s the case:</p>
<ul>
<li><p>Set up an area for your project on an open science repository such as OSF, Zenodo or Figshare. (If you use OSF then setting up an OSF repository is quick and easy – head to <a href="https://osf.io/">osf.io</a>. OSF allows many integrations, including to GitHub, through the use of add-ons.)</p></li>
<li><p>You can start by setting your repository as private and then make it public at the appropriate time.</p></li>
<li><p>Upload all project files, and don’t forget to invite your collaborators.</p></li>
<li><p>Add ORCIDs for every team member.</p></li>
</ul>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
What is an ORCID?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An ORCID is a persistent digital identifier that you own and control. It allows you to connect your ID with your professional information – affiliations, grants, publications, peer reviews, and more. You can set one up at <a href="https://orcid.org/register">orcid.org</a>.</p>
</div>
</div>
</div>
</section>
<section id="step-3" class="level3">
<h3 class="anchored" data-anchor-id="step-3">Step 3</h3>
<p>If you are ready to submit your research to a journal or conference, consider the following steps before you submit:</p>
<ul>
<li>Check that there is enough information in GitHub (if using) and OSF (if using) about the project. This should include instructions for someone to be able to access your files, use the data and run the code.</li>
<li>Make the GitHub and/or OSF repositories publicly visible.</li>
<li>If submitting to a journal that requires anonymous links, generate them and copy them into the manuscript. (In OSF, for example, it is possible to create anonymous links to your repository in case of double-blind submission requirements.)</li>
<li>Share a copy of your manuscript on a pre-print server – but don’t forget to check the journal or conference policy on pre-prints before you do!</li>
</ul>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Apps and websites to support open science
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This is by no means a complete list but instead features the apps and websites that are commonly used when research projects include data and code.</p>
<section id="open-science-framework-osf" class="level5">
<h5 class="anchored" data-anchor-id="open-science-framework-osf">Open Science Framework (OSF)</h5>
<p>OSF is a free web app that supports researchers with sharing, archiving, registration and collaboration. The <a href="https://help.osf.io">Open Science Framework website</a> is worth checking out and includes a guide to help users get started. Once a project is public in the OSF it will have a DOI and a permanent link, so it can be cited. OSF can also support the tracking of versions of your file. One drawback can be that there is a limit on the maximum size of file that can be uploaded.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/ideas/posts/2023/11/06/images/osf.png" class="img-fluid figure-img" style="width:80.0%" alt="Screengrab of an Open Science Framework repository for a project titled 'Why one size fits all is not enough when designing COVID-19 immunity certificates for domestic use.'"></p>
</figure>
</div>
<div class="figure-caption" style="text-align: center;">
<p>Sample OSF repository.</p>
</div>
</section>
<section id="figshare" class="level5">
<h5 class="anchored" data-anchor-id="figshare">Figshare</h5>
<p>This web app supports storing and sharing research outputs (papers, FAIR data, and non-traditional research outputs). Like OSF, <a href="https://figshare.com">Figshare</a> provides a DOI for your files and is similarly limited in the maximum size of file that can be upload.</p>
</section>
<section id="zenodo" class="level5">
<h5 class="anchored" data-anchor-id="zenodo">Zenodo</h5>
<p>Another general purpose open repository. As with Figshare, <a href="https://zenodo.org">Zenodo</a> also provides a DOI.</p>
</section>
<section id="github" class="level5">
<h5 class="anchored" data-anchor-id="github">GitHub</h5>
<p>GitHub is a web app that offers distributed version control. It is very commonly used for software development, especially when there are multiple developers. Although you can share code and many file types through GitHub, accessing and collaborating on projects can be a daunting experience for those who are not familiar with the way GitHub works. Also, GitHub is not always required as it is possible to share your code through OSF, for example. If you want to know more about using GitHub in support of open science and reproducibility, read <a href="https://realworlddatascience.net/case-studies/posts/2023/06/15/road-to-reproducible-research.html">“The road to reproducible research”</a>.</p>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="example-my-own-route-to-open-science" class="level2">
<h2 class="anchored" data-anchor-id="example-my-own-route-to-open-science">Example: my own route to open science</h2>
<p>In my case, my project did not involve a heavy amount of coding or a large number of researchers, so I opted to use <a href="https://osf.io/jubv6/">OSF</a> to store the ethics approval documents, the survey questions (which drove the data collection), the data in .csv format, and the outputs. I also then linked this to <a href="https://brunel.figshare.com/articles/dataset/Why_one_size_fits_all_is_not_enough_when_designing_COVID-19_immunity_certificates_for_domestic_use_a_UK_wide_cross-sectional_online_survey/16962895">Figshare</a> from my institution and published the article on <a href="https://www.medrxiv.org/content/10.1101/2021.10.12.21264898v2">MedRxiv</a> at the same time as I submitted it to a journal for review. The paper was eventually published in <a href="https://bmjopen.bmj.com/content/12/4/e058317">BMJ Open</a>. The steps I took in this case were sufficient for the work to be recognised as embracing open science principles.</p>
</section>
<section id="plot-your-own-route-to-open-science" class="level2">
<h2 class="anchored" data-anchor-id="plot-your-own-route-to-open-science">Plot your own route to open science</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">flowchart TD
  D("- Set up GitHub repo
  - Set repo as private
  - Add collaborators")
  F("- Set up an OSF repository
  - Set project as private
  - Add collaborators and their ORCIDs")
  A(Pre-register statistical analysis plan?) -- Yes --&gt; B(Complete pre-registration through, e.g., Open Science Framework) --&gt; C(Does your research involve writing lots of code?) -- Yes --&gt; D --&gt; E(Do you plan to share data and other research material?) -- Yes --&gt; F --&gt; G(Research project is finished and ready to submit to journal or conference)
  A -- No --&gt; C -- No --&gt; E -- No --&gt; G
  G --&gt; H(Have you used repos?) -- Yes --&gt; I(Change repo settings - GitHub and/or OSF - to public) --&gt; J(Does publication permit sharing manuscripts to pre-print servers?) -- Yes --&gt; K(Submit to pre-print server) --&gt; L(Does publication require anonymous link to OSF repo for double-blind review?) -- Yes --&gt; M(Generate anonymous link and add to submission) --&gt; N(Submit your work)
  H -- No --&gt; J -- No --&gt; L -- No --&gt; N
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</section>
<section id="in-summary" class="level2">
<h2 class="anchored" data-anchor-id="in-summary">In summary…</h2>
<p>To make your research open science, you need to:</p>
<ul>
<li>Make any data you collect or generate available to download and reuse.</li>
<li>Pre-register your statistical analysis plan.*</li>
<li>Make your code available for download, and document it clearly so others can reuse it.</li>
<li>Make any supporting material and outputs available for download in formats that are open source.</li>
<li>If publishing to a journal or conference, share manuscripts in a pre-print server.*</li>
</ul>
<div class="figure-caption">
<p>* May not be relevant or applicable, depending on the nature of your work.</p>
</div>
<div class="article-btn">
<p><a href="../../../../../ideas/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<p><strong>Isabel Sassoon</strong> is a senior lecturer in computer science and data science at Brunel University London and a member of the <a href="https://realworlddatascience.net/viewpoints/editors-blog/posts/2022/10/18/meet-the-team.html">Real World Data Science editorial board</a>.</p>
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
<p>© 2023 Isabel Sassoon</p>
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://realworlddatascience.net/ideas/posts/2023/11/06/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://realworlddatascience.net/ideas/posts/2023/11/06/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a> Thumbnail photo by <a href="https://unsplash.com/@the_photoman?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Basil James</a> on <a href="https://unsplash.com/photos/gray-stainless-steel-padlock-iC4BsZQaREg?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
<p>Sassoon, Isabel. 2023. “How to ‘open science’: A brief guide to principles and practices.” Real World Data Science, November 6, 2023. <a href="https://realworlddatascience.net/ideas/posts/2023/11/06/how-to-open-science.html">URL</a></p>
</dd>
</dl>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-haroz_2022" class="csl-entry">
Haroz, Steve. 2022. <span>“Comparison of Preregistration Platforms.”</span> MetaArXiv. <a href="https://doi.org/10.31222/osf.io/zry2u">https://doi.org/10.31222/osf.io/zry2u</a>.
</div>
<div id="ref-munafo2017manifesto" class="csl-entry">
Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John Ioannidis. 2017. <span>“A Manifesto for Reproducible Science.”</span> <em>Nature Human Behaviour</em> 1 (1): 1–9.
</div>
<div id="ref-wilkinson2016fair" class="csl-entry">
Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. <span>“The FAIR Guiding Principles for Scientific Data Management and Stewardship.”</span> <em>Scientific Data</em> 3 (1): 1–9.
</div>
</div></section></div> ]]></description>
  <category>Open science</category>
  <category>Open source</category>
  <category>Open data</category>
  <category>Reproducible research</category>
  <guid>https://realworlddatascience.net/ideas/posts/2023/11/06/how-to-open-science.html</guid>
  <pubDate>Sun, 05 Nov 2023 23:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/ideas/posts/2023/11/06/images/basil-james-iC4BsZQaREg-unsplash.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>An AI for humanity</title>
  <dc:creator>Martin Goodson</dc:creator>
  <link>https://realworlddatascience.net/viewpoints/posts/2023/10/20/ai-for-humanity.html</link>
  <description><![CDATA[ 




<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container" style="margin-top: 0;">
<p>This is the text of a talk Martin Goodson gave to the European Commission in Brussels on October 10, 2023. It is republished with permission from the <a href="https://rssdsaisection.substack.com/p/an-ai-for-humanity">Royal Statistical Society Data Science and AI Section Newsletter Substack</a>. The views expressed are the author’s own and do not necessarily represent those of the RSS.</p>
</div>
</div>
</div>
<p>For years academics have published studies about the limits of automation by AI, suggesting that jobs requiring creativity were the least susceptible to automation. That <a href="https://www.businessinsider.com/lost-job-chatgpt-made-me-obsolete-copywriter-2023-7?r=US&amp;IR=T">turned</a>. <a href="https://www.washingtonpost.com/technology/2023/06/02/ai-taking-jobs/">out</a>. <a href="https://www.theguardian.com/film/2023/aug/21/ai-jobs-hollywood-writers-actors-strike">well</a>.</p>
<p>Actually, that’s not completely true: some said that jobs that need a long period of education, like teaching and healthcare, were going to be the hardest of all to automate. <a href="https://www.khanacademy.org/khan-labs">Oh</a>. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/GPT-4_medical_benchmarks.pdf">dear</a>.</p>
<p>Let’s face it, all predictions about the limits of AI have been hopelessly wrong. Maybe we need to accept that there aren’t going to be any limits. How is this going to affect our society?</p>
<p>Studies came out from <a href="https://www.nber.org/papers/w31161">Stanford</a> and <a href="https://www.science.org/doi/10.1126/science.adh2586">MIT</a> this year, looking at the potential of AI assistants to improve the productivity of office workers. Both came to the same conclusion – that the workers with the lowest ability and least experience were the ones who gained the most in productivity.</p>
<p>In other words, AI has made human knowledge and experience less valuable.</p>
<p>Researchers at Microsoft and Open AI <a href="https://arxiv.org/abs/2303.13375">wrote</a> something important on this phenomenon that I’d like to quote in full:</p>
<blockquote class="blockquote">
<p>Large swaths of modern society are predicated on a “grand bargain” in which professional classes invest years or even decades in technical education and training and are [afforded] the exclusive right to practice in their field, social prestige, and above-average compensation.</p>
<p>Technical disruption of this social contract can have implications not only for the medical field but for numerous other knowledge-intensive professions including law, banking, engineering, accounting, and others.</p>
</blockquote>
<p>Let’s talk about the fairness of this. Because the AI models didn’t invent medicine, accountancy or engineering. They didn’t learn anything directly from the world – human experts taught AI models how to do these things. And they [the human experts] did it without giving their permission, or even knowing that it was happening.</p>
<p>The large tech companies have sucked up all of human knowledge and culture and now provide access to it for the price of an API call. This is a huge transfer of power and value from humanity to the tech companies.</p>
<p>Biologists in the 1990s found themselves in a very similar position. Celera Genomics was trying to achieve commercial control over the human genome. To stop this happening, the publicly funded Human Genome Project (HGP) resolved to sequence the human genome and release the data for free on a daily basis, before Celera could patent any of it.</p>
<p>The HGP was criticised because of ethical concerns (including concerns about eugenics), and because it was thought to be a huge waste of money. The media attacked it, claiming that a publicly funded initiative could not possibly compete with the commercial sector. Fortunately for humanity, a group of scientists with a vision worked together to make it a success.</p>
<p>And it was a huge success: in purely economic terms it produced nearly $1 trillion in economic impacts for investment of about $4 billion. Apart from the economics, the Human Genome Project accelerated development of the genomic technologies that underlie things like mRNA vaccine technology.</p>
<p>The parallels to our current situation with AI are striking. With OpenAI, just like Celera, we have a commercial enterprise that launched with an open approach to data sharing but eventually changed to a more closed model.</p>
<p>We have commentators suggesting that a publicly funded project to create an open-source AI would be ethically dubious, a waste of money and beyond the competency of the public sector. Where the analogy breaks down is that unlike in the 1990s, we do not have any strong voices arguing on the other side, for openness and the creation of shared AI models for all humanity.</p>
<p>Public funding is needed for an “AI for humanity” project, modelled on the Human Genome Project. How else can we ensure the benefits of AI are spread widely across the global population and not concentrated in the hands of one or two all-powerful technology companies?</p>
<p>We’ll never know what the world would have looked like if we’d let Celera gain control over the human genome. Do we want to know a world where we let technology companies gain total control over artificial intelligence?</p>
<section id="faq" class="level2">
<h2 class="anchored" data-anchor-id="faq">FAQ</h2>
<section id="how-about-all-the-ethical-considerations-around-ai-shouldnt-we-consider-this-before-releasing-any-open-source-models" class="level5">
<h5 class="anchored" data-anchor-id="how-about-all-the-ethical-considerations-around-ai-shouldnt-we-consider-this-before-releasing-any-open-source-models">How about all the ethical considerations around AI – shouldn’t we consider this before releasing any open-source models?</h5>
<p>Of course. Obviously, there are ethical implications that need to be considered carefully, just as there were for the genome project. At the start of that project, the ethical, legal, and social issues (or ELSI) program was set up. The National Institutes of Health (NIH) devoted about 5% of their total Human Genome Project budgets to the ELSI program and it is now the largest bioethics program in the world. All important ethical issues were considered carefully and resolved without drama.</p>
</section>
<section id="arent-there-enough-community-efforts-to-build-open-source-ai-models-already" class="level5">
<h5 class="anchored" data-anchor-id="arent-there-enough-community-efforts-to-build-open-source-ai-models-already">Aren’t there enough community efforts to build open-source AI models already?</h5>
<p>There are good projects producing open-source large language models, like Llama 2 from Meta and Falcon from the TII in the United Arab Emirates. These are not quite as powerful as [Open AI’s] GPT-4 but they prove the concept that open-source models can approach the capabilities of the front-running commercial models; even when produced by a single well-funded lab (and a state-funded lab in the case of the TII). A coordinated international publicly funded project will be needed to surpass commercial models in performance.</p>
<p>In any case, do we want to be dependent on the whims of the famously civic-minded Mark Zuckerberg [CEO of Meta] for access to open-source AI models? We shouldn’t forget that the original Llama model was released with a restrictive licence that was eventually changed to something more open after a community outcry. We are lucky they made this decision. But the future of our societies needs to rely on more than luck.</p>
</section>
<section id="how-about-the-uk-government-ai-safety-summit-and-ai-safety-institute-wont-they-be-doing-similar-work" class="level5">
<h5 class="anchored" data-anchor-id="how-about-the-uk-government-ai-safety-summit-and-ai-safety-institute-wont-they-be-doing-similar-work">How about the UK Government AI Safety Summit and AI Safety Institute – won’t they be doing similar work?</h5>
<p>Absolutely not! The limit of the UK Government’s ambition seems to be to set the UK up as a sort of <a href="https://www.politico.eu/article/uk-pitch-ai-safety-institute-rishi-sunak/">evaluation and testing</a> station for AI models made in Silicon Valley. This is as far from the spirit of the Human Genome Project as it’s possible to be.</p>
<p>Sir John Sulston, the leader of the HGP in the UK, was a Nobel Prize-winning scientific hero who wanted to stop Celera Genomics from gaining monopolistic control over the human genome at all costs. The current UK ambition would be like reducing the Human Genome Project to merely testing Celera Genomics’ data for errors.</p>
</section>
<section id="how-will-an-international-ai-for-humanity-project-avoid-the-devaluation-of-human-knowledge-and-experience-and-consequent-job-losses" class="level5">
<h5 class="anchored" data-anchor-id="how-will-an-international-ai-for-humanity-project-avoid-the-devaluation-of-human-knowledge-and-experience-and-consequent-job-losses">How will an international ‘AI for humanity’ project avoid the devaluation of human knowledge and experience, and consequent job losses?</h5>
<p>It may not be possible to avoid this. But governments will at least be able to mitigate societal disruption if they can redistribute some of the wealth gained via AI (e.g., via universal basic income). They will not be able to do this if all of the wealth accrues to only one or two technology companies based in Silicon Valley.</p>
</section>
<section id="how-about-existential-risk" class="level5">
<h5 class="anchored" data-anchor-id="how-about-existential-risk">How about existential risk?</h5>
<p>‘Existential risk’ is a science fiction smokescreen generated by large tech companies to distract from the real issues. I cannot think of a better response than the words of Prof <a href="https://www.independent.co.uk/tech/rishi-sunak-university-of-oxford-san-francisco-government-people-b2349105.html">Sandra Wachter</a> at the University of Oxford: “Let’s focus on people’s jobs being replaced. These things are being completely sidelined by the Terminator scenario.”</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Martin Goodson will be speaking live at the Royal Statistical Society on October 31, 2023, as part of a panel discussion on “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” <a href="https://rss.org.uk/training-events/events/events-2023/rss-events/evaluating-artificial-intelligence-how-data-scienc/#fulleventinfo">Register now</a> for this free in-person debate. The event forms part of the <a href="https://aifringe.org/">AI Fringe</a> programme of activities, which runs alongside the UK Government’s <a href="https://www.gov.uk/government/topical-events/ai-safety-summit-2023">AI Safety Summit</a> (1–2 November).</p>
</div>
</div>
</div>
<div class="article-btn">
<p><a href="../../../../../viewpoints/index.html">Discover more Viewpoints</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<strong>Martin Goodson</strong> is the former chair of the <a href="https://rss.org.uk/membership/rss-groups-and-committees/sections/data-science-section/">RSS Data Science and AI Section</a> (2019–2022). He is the organiser of the <a href="https://www.meetup.com/london-machine-learning-meetup/">London Machine Learning Meetup</a>, the largest network of AI practitioners in Europe, with over 11,000 members. He is also the CEO of AI startup, <a href="https://www.evolution.ai/">Evolution AI</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-12">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Martin Goodson
</dd>
</dl>
<p>Thumbnail image by <a href="https://unsplash.com/@etiennegirardet?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Etienne Girardet</a> on <a href="https://unsplash.com/photos/a-red-wall-with-a-white-sticker-on-it-_HO6LmpGDl8?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>.</p>
</div>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>AI</category>
  <category>Open source</category>
  <guid>https://realworlddatascience.net/viewpoints/posts/2023/10/20/ai-for-humanity.html</guid>
  <pubDate>Thu, 19 Oct 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/viewpoints/posts/2023/10/20/images/etienne-girardet-_HO6LmpGDl8-unsplash.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>‘I fell in love with math, really, and fell into data science because of that’</title>
  <dc:creator>Brian Tarran</dc:creator>
  <link>https://realworlddatascience.net/careers/career-profiles/posts/2023/10/04/niclas-thomas.html</link>
  <description><![CDATA[ 




<p>A passion for maths and solving mathematical problems led Niclas Thomas to a PhD in machine learning with a focus on medical research. But then a conversation with a recruiter steered his career towards data science in the retail sphere. After stints at Tesco, Sainsbury’s, and Gousto, Thomas is now head of data science for Next, the clothing retailer.</p>
<p>In this interview with Real World Data Science, Thomas reflects on his career journey so far, from hands-on coding work to team leadership and management. He also argues for the importance of communication and storytelling as part of the data science skill set.</p>
<section id="transcript" class="level2">
<h2 class="anchored" data-anchor-id="transcript">Transcript</h2>
<div class="callout-warning callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container" style="margin-top: 0.75rem;">
<p>This transcript has been produced using <a href="https://otter.ai/">speech-to-text transcription software</a>. It has been only lightly edited to correct mistranscriptions and remove some repetitions.</p>
</div>
</div>
</div>
<p><strong>Brian Tarran</strong><br>
Niclas Thomas, thank you for joining us today. I hope you’re well.</p>
<p><strong>Niclas Thomas</strong><br>
I am indeed thanks. Thank you for having me.</p>
<p><strong>Brian Tarran</strong><br>
Today we’re meeting because we want to find out a little bit about your career in data science, how you got into it, what you’re doing now, where you see both your career and data science as a profession going next. So do you mind– can we start by giving us a brief introduction to who Niclas Thomas is?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah, of course. Yeah. So I’m currently working as head of data science at Next. My background is academia originally, a maths degree. I did my PhD in machine learning, and more in medical research, so more of an applied machine learning position where the idea was to try and predict, ultimately, predict disease from a given sample of data from blood – can you actually predict future disease? – which I think is a really interesting area; I love medical research. And then [I] switched over to more commercial role and worked in several retail data science roles: so, Tesco, Sainsbury’s, Gousto, and then now, as I said, currently head of data science at Next, where I run a team, and I imagine most listeners will be familiar with what Next do: a retail, a clothing brand on the whole, where the idea is, obviously to sell some great stuff, great products and put the right product in front of the right customer.</p>
<p><strong>Brian Tarran</strong><br>
Can you tell us, what does your job involve? What are your sort of main tasks and responsibilities in that role?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah, so I suppose I’m lucky enough to have been head of data science at several different companies: Sainsbury’s, Gousto, and Next. So it’s always interesting to compare the role of the head of data science in each of those three. At the moment, I think there’s a core focus on, well, ultimately making sure the teams are efficient as possible. And that really means just making sure our tech stack – what tools, what programming languages, what software we use on a day to day basis – is set up for success and make sure the team have what they need to be able to do the job as efficiently as possible, whether that’s using Python or R, whether that’s how we develop code, and how we work with other people as well, being a big part of that, then. So how do we work with other software engineers? How do we work with web developers, then, to make sure that the work we do actually gets in the hands of the business and ultimately in the hands of the customer. So that’s one aspect: it’s just making sure the team is set up for success, both in terms of the ways they work and what tools they have to work as well, then. I guess the other side of that coin is what we actually work on. So understanding the value of potential work we could do, and helping the team understand what that value is, and, and ultimately giving direction of what things we want to work on next. Obviously, that’s not my decision in isolation, but understanding on the one hand, what other stakeholders want to do, what my superiors wants to do, as well. And trying to put that all into the mix to understand these are the next best projects to work on given a finite amount of people to work on these problems. And then ultimately, then, the last part, then, is ultimately helping the team deliver those projects, those products as well then, which usually means calling on my experience of having solved these problems myself, either directly when I was earlier in my career or indirectly through leading others then or, you know, being the head of a team and working with some other great people and to learn from their experiences as well.</p>
<p><strong>Brian Tarran</strong><br>
What does data science mean to you, personally? I’m not asking you to define it for everybody. But for you, what is what is data science?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah, I wish I’d come up over the years with a great definition of this. But yeah, I mean, really, it just, I mean, at the very highest level, it just means using data to drive business value, I suppose, as I guess in my– which probably reflects the fact that it’s more of a business role that I have. But I think that in its broadest sense, I think that’s true: using data to drive insights and make decisions for the business. There are more, I guess, detailed definitions of that. So, for example, the way I’ve always differentiated between data analytics and data science is that if you want to make repeated decisions on a daily or weekly basis, then that’s when it becomes more about a data science question versus a data analytics question, because data analytics is generally about answering large one off ad hoc questions, rather than making the same decision over and over again and using methods appropriate for that. But, ultimately, that’s what data science means to me, I think: making repeated decisions using data and the scientific method to use data for good.</p>
<p><strong>Brian Tarran</strong><br>
And so what do you think is your most important skill as a data scientist given that definition that you have of data science?</p>
<p><strong>Niclas Thomas</strong><br>
In my role, I suppose communication ultimately becomes the most important thing. I’d say definitely earlier in my career, and I think if you’re the person actually delivering and implementing the algorithm, I think that the technical skill set obviously is really important then. But ultimately, I almost see my role as the head of data science as a hybrid– as a link between my team and the rest of the business, then. So it’s really about being able to, on the one hand, translate technical concepts into non technical descriptions of what we’re actually doing, making sure the rest of the business can understand and vice versa, then making sure I understand the business process and business terminology well enough to be able to translate that for the team, as and when needed, into a vision for a project, a product, then, and develop a strategy for that. So I think that the communication both in the strictest sense of being able to talk that through with, with my team, with other team members, with stakeholders, as well, but also more in the looser sense, then, of being able to define that strategy, being able to define what the roadmap for a particular project or a product might look like.</p>
<p><strong>Brian Tarran</strong><br>
Can you talk us through your so your education and your training that led up to your kind of first data science job, your first data science role.</p>
<p><strong>Niclas Thomas</strong><br>
I suppose the first time, the first time I– actually, I’d never heard about it, I think, when a recruiter approached me. This is probably going back into 2014, when I was maybe eight months into my postdoc after my PhD. I think– obviously it did exist before that, although I suppose the terminology wasn’t quite as widespread going back almost 10 years now where the term is a lot more rife. So my original background, I did a master’s in maths originally, four years. And then I remember being– the last year of that, then, I was applying for a few jobs, and I applied for one at the Met Office, where the focus obviously was predicting weather, forecasting. And I wasn’t successful in that job. But I did notice that the, on the job spec at the time, it was PhD preferred was one of the specs on that role. It was probably the first time I thought about taking on a PhD as more of a career move rather than as the natural progression to an academic career, more of a business career move if you like, then of actually how it can help you in more business settings. So that was at least when I decided to do my PhD and thought it’s certainly not going to be– and this was back in 2008, so at the time of the financial issues at the time when getting jobs was harder anyway, so it felt like a win-win of doing something that would be– I was clear I wanted to work in a data role of some sort. And that combined with the fact that I thought it would be a good career move and the financial climate at the time wasn’t brilliant. So I took on a PhD then. And then in terms of actually getting into, into my first data science position was, as I said, just after I finished my PhD, I had been working about six months, eight months as a postdoc, and then a recruiter just described a role that was available at Tesco at the time. And it sounded a lot of what I was doing in my current postdoc role at the time – making predictions based on data and exactly the same techniques – sounded really interesting. And it must have been the way the recruiter sold it at the time as well then, because it’s something I was really keen to take on and then made my move off the back of that then. So yeah, kind of moved into it a little bit, I guess, semi deliberately from taking a PhD on first, but always with the view of moving over to a business role at some point after that.</p>
<p><strong>Brian Tarran</strong><br>
But it wasn’t like you started out your further education thinking, “I want to be a data scientist, what do I need to do to kind of get there? What are the subjects I need to focus on? What are the topics I need to research?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah. Oh, absolutely. Yeah, it certainly wasn’t by design at the very start of my journey. I fell in love with math, really, and just fell into data science because of that, really, I loved numbers and loved solving maths problems. So that’s why I did a degree in it first of all, then and certainly, you know, even midway through my degree, then I wasn’t really sure what I wanted to do. It was more, as you say, just by chance, then, that there were a few opportune moments that came around then, that opportunities came around at the right time to fall into that career.</p>
<p><strong>Brian Tarran</strong><br>
Doing a PhD in machine learning as you did, that was quite a – in hindsight – a smart choice of PhD to pursue, I think, right?</p>
<p><strong>Niclas Thomas</strong><br>
I think so. Yeah, I suppose it was– still even at that stage it wasn’t necessarily, again, the terminology ‘data science’ wasn’t really around. Certainly, when I started my PhD in 2009 2010. It wasn’t really terminology, at least it may have been in usage a little bit in terms of being on, you know, if you look for jobs on LinkedIn or Indeed, but it certainly wasn’t terminology that that I would have been particularly familiar with.</p>
<p><strong>Brian Tarran</strong><br>
Your first job in data science was at Tesco. You mentioned that you were you were kind of recruited to that role there. How does it compare to your current role? So I guess, you know, what’s the difference between being a data scientist versus head of data science as you are now?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah, I think there are probably more similarities than differences, I would say. We were quite lucky in the setup in Tesco that the recruitment strategy seemed to be more focused around people who already had some experience in, generally, either already had business experience or a PhD. So we were fairly independent in solving our own– the project that we were working on and working on that. Not necessarily with the head of data science guiding us, you know, day by day, in terms of the actual nitty gritty and the technical detail, which is great, then. So it did mean that we had responsibility and ownership for our product quite early on. So yeah, I really enjoyed that. I suppose I was writing a lot more code in those days than I do now. I rarely, if ever write code at the moment. So I think that’s probably true for the last maybe three or four years, I think, only occasionally getting my hands dirty. And even when it is, it’s not really to build an algorithm, it’s more to inquire about what data we have to solve the algorithm then. So even when I do get my hands dirty, it’s more in the very early stages of the whole algorithm development lifecycle. So I think that’s probably the biggest difference is just the actual ownership of development there – probably expected, I would say, but it’s– I think that’s one of the beauties of being in your first job or two in data science. I think the– I think in most places I’ve seen, I think you’ll get ownership of, of the work, the stuff that you work on, on a day to day basis, quite early on. And you’ll be expected to contribute code and ideas for that as well, which I think most people would love. I certainly loved it at the time.</p>
<p><strong>Brian Tarran</strong><br>
What was the most important thing you learned in your first year in that job?</p>
<p><strong>Niclas Thomas</strong><br>
I think, again, it’s probably a lot around the ways of working, I would say – of the various ways you can [work], which I never really thought about it before. Working in academia, it was quite isolated, I suppose. You work on your own project, you work on your own work and don’t really– or at least, I found I didn’t really work with anyone else that much. Maybe that was the nature of my work as well, we’d obviously be dependent on people working in a lab to get data. But I think the day to day work, I was working quite in isolation, whereas the team aspect of working, I think, was a steep learning curve then – so agile methodology, and everything around that, which was very, very new to me. And the various ways you can do that. I’m generally not someone for overly putting processes in place in a team, only where necessary. But I think there’s some great learnings from that as well. It certainly started to shape how I think I would want to run a team if and when I got to that position.</p>
<p><strong>Brian Tarran</strong><br>
So, Nick, what have been your career highlights so far?</p>
<p><strong>Niclas Thomas</strong><br>
I think in terms of– there was one product we built in Sainsbury’s in particular. So in terms of, on a product level of replenishment. So how do you most efficiently get products from the back of the store onto the shelves of an individual store? And what’s the most optimal strategy to do that, which I love for a variety of reasons. A, it was one of the first full data science products that we had deployed and worked on as a team in Sainsbury’s. So there was that kind of milestone about it. I think it also stood out as a really nice move away from classic machine learning – i.e., making a prediction, a classification model – to something that was a bit more operations research based and more based on optimization. So using graph theory, making a graph network of a store. And using that to solve the problem of taking a route through the store, for example, a bit like a Google Maps for a store basically, was how we always pitched it to our stakeholders, and how can you choose the best route and again, moving more into a bit more of a vehicle routing problem, then: if you’ve got two different trolleys, how do you decide what items to put on trolley one versus trolley two? So there’s loads of interesting stuff on the technical side of things and it was, again, I felt it was probably one of the highlights – as well as the end product, it was also the one I worked on at the very start. So actually, the understanding whether it would be possible to do that, what kind of technical approach. So I think certainly from a product perspective, that’s probably stuck in my mind. Aside from that, on a more personal level, I guess, I did decide to write a book off the back of my PhD. Just mainly on my experiences from my PhD and postdoc. I mean, it’s not like a confessional. But more on the– just working with non data scientists and making it more accessible was really what I really focused on there. So having worked with clinicians, immunologists and others as part of the medical research that I did, I felt that data can be accessible if you pitch it in a way and make it easy to use. And so that was the purpose of what was largely an educational textbook.</p>
<p><strong>Brian Tarran</strong><br>
Do you want to give a short plug for the book, what it’s called and where people can find it?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah, so it’s, Data Science for Immunologists is the name of the book. It’s available on Amazon. I’m one of the two co-authors on that then. And we do have a website, <a href="https://datascienceforimmunologists.com/">datascienceforimmunologists.com</a>, as well then if you did want to visit and you can either buy the book, there’s a link on that website or just go straight to Amazon and it’s available there.</p>
<p><strong>Brian Tarran</strong><br>
This next question, we’ve gone from highlights to lowlights. Have there been any mistakes or regrets that you’ve had along the way in your data science career so far?</p>
<p><strong>Niclas Thomas</strong><br>
The main mistakes I think I’ve made before is not valuing, A, communication or soft skills, but B, the leadership and management as well then. And I think especially it’s something, when working at Gousto as well that was something that was a big focus of the team and something that I really took from my time there as well was the, I guess, the art of good management and good leadership, you know, what the difference is between the two. So I wouldn’t say there’s any one bang event that’s a mistake or regret, but it’s probably, as ever, it’s probably I would have put more emphasis on it sooner had I known that how important those skills would be.</p>
<p><strong>Brian Tarran</strong><br>
Yeah, but I think that’s understandable to a certain extent. If you’re coming from, I guess, a role that’s very hands on, doing things yourself, getting into the messy details of a project, it can sometimes be hard to kind of take a step back and adopt more of a kind of leadership, management position, can’t it?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah, definitely. Yeah, definitely. I would agree with that. And I think it’s also, I’d probably say for a lot of people starting out, and certainly it was for me, that the technical– the technical aspect is probably why you get into a role in data science in the first place, that you just love solving problems, basically, whether that’s with code or with pen and paper. And so that’s, that’s what you want to do. And getting your mind focused elsewhere away from that is probably not viewed as the most fun thing to do, I probably wouldn’t have, when I was starting out in 2014, 2015, I probably wouldn’t have thought it was as fun or as interesting to do that as I do now, maybe. So I think that’s the other reason why it probably doesn’t get as much focus earlier on in my career anyway, at least, as it probably deserved.</p>
<p><strong>Brian Tarran</strong><br>
How do you think your– how do you see your role, I guess, evolving over the rest of your career in data science?</p>
<p><strong>Niclas Thomas</strong><br>
I suppose on a personal level, for me it’s, I’m always thinking of what, 10 years down the line, do I still want to be focused just on data science? Or do I want to be focused on a data role, more broadly? I suppose that’s always the main question to ask. And so by that I mean, looking at data engineering as well, data analytics, and being responsible for a wider group. I think the way the field is going anyway, I think a lot more companies seem to move to vertical management rather than horizontal. So by that, I mean having heads of data in different areas of the business. So rather than having a head of data and a head of analytics, you might have a head of data for certain aspects of the business and another head of data then that’s responsible for both in other areas of the business, then. So either way, I think that the broadening of responsibilities and not just being responsible for data science is probably one way I would see my career potentially moving. At the moment, I love just focusing just on the data science, I’m really happy doing that now. But I think that could be one way that my focus changes in the future.</p>
<p><strong>Brian Tarran</strong><br>
What personal or professional advice would you give for anyone wanting to be a data scientist?</p>
<p><strong>Niclas Thomas</strong><br>
Yeah, so first of all, the balance between the soft and hard skills. I think I’ve alluded to it before, but the– don’t put too much– I mean, still emphasise on the technical skills are really important, but don’t feel like it’s the be all end all. I think just understanding the softer side of how you communicate, how you tell a story, for example, and storytelling with data, I think is really important. So I’d say that’s probably one focus area. I think that the second would probably, and maybe it’s a harder one to act on, but being passionate, I think, because whenever I’m looking to recruit anyone new into my team, I think it’s as much about understanding what the potential of that person is as is what is their current performance or where their current capability is – how good they could be in the future is arguably more important. And I think a lot of that comes to ultimately someone’s– whether they have a fixed or growth mindset. So by that, I mean, ultimately, do they want to learn or not, and if they really want to learn, as a lot of data scientists do, but if they have a huge passion for or about data science, and wanting to learn about just how to get better – whether that’s a better coder, better at maths, anything around that – then if you have that attitude, I think then it’s, A, you can have a great impact on our team, but B, I think it’s a sign of someone who can be a great performer in the future.</p>
<p><strong>Brian Tarran</strong><br>
So what do you think will be the main challenges facing data science as a field over the next few years?</p>
<p><strong>Niclas Thomas</strong><br>
I think probably, certainly, currently maybe living up to the hype, I suppose. And matching I suppose the classic Gartner Hype Cycle of, it feels like we’re probably at the stage where there’s a lot of– the hype has been around for a few years of data science now and I think making sure we tackle the right problems, I suppose, is one of the – and by ‘we’ I mean, Next as a business or whatever business we’re working in at the time – I think it’s making sure we’re working on the right things. Because I think a lot of people will be keen to have data scientists as part of their work and the product they’re trying to build. What is the best place to spend our time, and what projects we should be working on most I think is– becomes important then because, as I say, there’s a huge demand for data scientists time, I think, in every company. And so choosing where we spend that time wisely, I think, becomes the key challenge and the important decisions for, especially for a head of data science like myself to make then, to make sure we’re best using the team’s capacity, then.</p>
<div class="article-btn">
<p><a href="../../../../../../careers/career-profiles/index.html">Discover more Career profiles</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Royal Statistical Society
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/careers/career-profiles/posts/2023/10/04/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/careers/career-profiles/posts/2023/10/04/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Tarran, Brian. 2023. “‘I fell in love with math, really, and fell into data science because of that.’” Real World Data Science, October 4, 2023. <a href="https://realworlddatascience.net/careers/career-profiles/posts/2023/10/04/niclas-thomas.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>Leadership</category>
  <category>Management</category>
  <category>Communication</category>
  <guid>https://realworlddatascience.net/careers/career-profiles/posts/2023/10/04/niclas-thomas.html</guid>
  <pubDate>Tue, 03 Oct 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/careers/career-profiles/posts/2023/10/04/images/niclas-thomas.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading</title>
  <dc:creator>Ed Humpherson</dc:creator>
  <link>https://realworlddatascience.net/viewpoints/posts/2023/09/18/pseudo-data-science.html</link>
  <description><![CDATA[ 




<p>A typical article on data science hails new data sources, new tools, and new visualisations, and thereby supports the case for the value of data science.</p>
<p>But this article takes a different angle: it talks about potential pitfalls that can face data scientists. It is based on our work as the Office for Statistics Regulation (OSR), the UK’s regulator for official statistics. We see lots of great work done by statisticians in government. But we also see some of the challenges they face – and data scientists are also likely to encounter the same challenges.</p>
<p>The problems arise from the fact that neither statisticians nor data scientists do their work in isolation. The work usually takes places within organisations – businesses, government bodies, think tanks, academic institutions – and as a result, the statisticians and/or data scientists are not the only players who get to influence how data science is presented and used.</p>
<p>What are the pitfalls we see in our work as regulator?</p>
<section id="pseudo-data-science" class="level2">
<h2 class="anchored" data-anchor-id="pseudo-data-science">Pseudo data science</h2>
<p>The first type of pitfall is pseudo data science.</p>
<p>Pseudo data science is a term we use to describe attempts to pass off crude work as being more data science-y than it really is. That reflects a sense in public life that data science is new, innovative, somehow the Future. In this context, people who are not data scientists can be tempted to dress themselves up in the clothes of data science to enhance their credibility. This dressing up is usually well-intentioned – communications professionals who want to illuminate and explain complex issues in an engaging way.</p>
<p>The trouble is, it can sometimes backfire. In our work at OSR, we have over the last year seen several examples where organisations have sought to publish visualisations that look like they are the product of in-depth data analysis – when in fact they have been drawn by communications staff using graphic design packages. Examples include <a href="https://osr.statisticsauthority.gov.uk/correspondence/ed-humpherson-to-david-pares-treasury-inflation-infographic/">inflation</a>, <a href="https://uksa.statisticsauthority.gov.uk/correspondence/response-from-sir-robert-chote-to-andrew-gwynne-mp-dhsc-chart-on-nurses-pay/">nurses pay</a>, and <a href="https://uksa.statisticsauthority.gov.uk/correspondence/letter-to-rachel-reeves-mp-gdp-growth-chart/">comparisons of UK economic performance with other countries</a>. To be fair, whenever we have pointed out issues like this, organisations have responded well, putting in place new procedures to ensure that analysts sign off on this kind of visualisations. Nevertheless, we suspect that the temptations to indulge in pseudo data science will remain strong – and we may need to intervene on similar cases in future.</p>
</section>
<section id="unintelligent-transparency" class="level2">
<h2 class="anchored" data-anchor-id="unintelligent-transparency">Unintelligent transparency</h2>
<p>The second pitfall is a failure of <a href="https://osr.statisticsauthority.gov.uk/publication/regulatory-guidance-on-intelligent-transparency/">intelligent transparency</a>.</p>
<p>There is a raw form of transparency – quoting a single number (a naked number we call it); or dumping data out into the public domain with no explanation. This is not intelligent transparency. The latter involves being clear where data come from, what their source is, and making underlying data available so that others can understand and verify the statements that are being made. Raw transparency and naked numbers treat an audience with little respect; intelligent transparency helps the audience understand and appreciate what sits behind high level claims.</p>
<p>Data science outputs can sometimes seem to communications teams easy to cherry pick for the most attractive number. Again, like pseudo data science, this reflects largely good intentions – to communicate complex things through ideas. But it becomes easy for a single, unsupported number to be used and reused until it loses most of its meaning. We call this weaponization of data, and it is the antithesis of intelligent transparency. And there is a lot of it about – for example the way in which the former Prime Minister of the UK talked repeatedly about <a href="https://uksa.statisticsauthority.gov.uk/correspondence/sir-david-norgrove-to-prime-minister-employment-statistics/">employment</a>; or <a href="https://uksa.statisticsauthority.gov.uk/correspondence/response-from-sir-robert-chote-to-alex-cole-hamilton-msp-scottish-renewable-energy-statistics/">claims</a> about Scotland’s capacity for <a href="https://uksa.statisticsauthority.gov.uk/correspondence/sir-robert-chote-to-stephen-kerr-msp-renewable-energy/">renewable energy</a>. These examples indicate the pathology of weaponization that can impact data science outputs. They also act as a reminder that data scientists can counter weaponization of their own outputs by delivering engaging and insightful communication.</p>
</section>
<section id="context-collapse" class="level2">
<h2 class="anchored" data-anchor-id="context-collapse">Context collapse</h2>
<p>The third type of pitfall surrounds context collapse.</p>
<p>This idea comes from the work of the philosopher <a href="http://lucymcdonald.co.uk/wp-content/uploads/2023/02/Context-Collapse-Online-LMcDonald.pdf">Lucy McDonald</a> (who in turn has built on the ideas of <a href="https://www.danah.org/">danah boyd</a>). What is context collapse? Imagine a swimming pool – with neat divisions of the pool into different lanes. All is clearly labelled – fast, medium, slow – for lane swimmers, who are in turn separated from the splash area for families and the deep end for divers. Removing the lanes, and thus taking away any signposting, increases the likelihood for things to go wrong. The fast swimmers doing front crawl clash with the slower breaststroke swimmers; both are constantly having to avoid the families with young children; and all need to watch for the periodic big splashes created by the divers. This is the online communication environment, in which formerly private and casual statements can go viral; in which a brief statement in a media environment can be picked up on and circulated many times; and in which some bad actors (the divers) may wish to disrupt deliberately the debate by breaking all the rules.</p>
<p>How can this affect data science? It happens when individual bits of data are taken from their context, and used in service of a different, and bigger, argument. A good example is data on Covid vaccinations. Here, UK organisations like the Office for National Statistics and the UK Health Security Agency published comprehensive data in good faith about vaccinations and their impact. Some of the underlying data, however, was taken out of the broader context and used in isolation to support criticisms of vaccines – criticisms that the wider evidence base did not support.</p>
<p>The challenge then became how the organisations should respond. At an organisational level, they did not wish to withdraw the data – because that would reduce transparency. Instead they sought to both caveat their data more clearly; and directly rebut the more egregious misuses of the data. In a sense, then, what began as an individual analytical output became part of a broader organisational judgement on positioning in the face of misinformation.</p>
<p>It is fair to say that, against this third pitfall, there is not yet a clear consensus on how to address it. Practice is emerging all the time and we at OSR continue to support producers of data as they grapple with it.</p>
<p>There are other potential pitfalls to using data science. But what unites these three – pseudo data science; unintelligent transparency; and context collapse – is that they relate to situations where data science rubs up against broader organisational dynamics, around communications, presentation and organisational strategy.</p>
<p>And the meta-message is this: for data scientists to thrive in organisations, they need to be good at more than data science. They need to be skilled at working alongside and influencing colleagues from other functions. Only through this form of <a href="https://osr.statisticsauthority.gov.uk/analytical-leadership/">data leadership</a> can the pitfalls be dealt with effectively.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>This article is based on a presentation at the <a href="https://www.datascienceforhealthequity.com/">Data Science for Health Equity</a> group in May 2023.</p>
</div>
</div>
</div>
<div class="article-btn">
<p><a href="../../../../../viewpoints/index.html">Discover more Viewpoints</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<strong>Ed Humpherson</strong> is head of the Office for Statistics Regulation, which provides independent regulation of all official statistics in the UK. The aim of OSR is to enhance public confidence in the trustworthiness, quality and value of statistics produced by government.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Ed Humpherson
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/viewpoints/posts/2023/09/18/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/viewpoints/posts/2023/09/18/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Humpherson, Ed. 2023. “‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading.” Real World Data Science, September 18, 2023. <a href="https://realworlddatascience.net/viewpoints/posts/2023/09/18/pseudo-data-science.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>Communication</category>
  <category>Leadership</category>
  <category>Transparency</category>
  <guid>https://realworlddatascience.net/viewpoints/posts/2023/09/18/pseudo-data-science.html</guid>
  <pubDate>Sun, 17 Sep 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/viewpoints/posts/2023/09/18/images/distorted-data.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>For minorities, biased AI algorithms can damage almost every part of life</title>
  <dc:creator>Arshin Adib-Moghaddam</dc:creator>
  <link>https://realworlddatascience.net/viewpoints/posts/2023/09/06/biased-algorithms.html</link>
  <description><![CDATA[ 




<p>Bad data does not only produce bad outcomes. It can also help to suppress sections of society, for instance vulnerable women and minorities.</p>
<p>This is the argument of <a href="https://www.bloomsbury.com/us/is-artificial-intelligence-racist-9781350374423/">my new book</a> on the relationship between various forms of racism and sexism and artificial intelligence (AI). The problem is acute. Algorithms generally need to be exposed to data – often taken from the internet – in order to improve at whatever they do, such as <a href="https://www.theguardian.com/us-news/2022/may/11/artitifical-intelligence-job-applications-screen-robot-recruiters">screening job applications</a>, or underwriting mortgages.</p>
<p>But the training data often contains many of the biases that exist in the real world. For example, algorithms could learn that most people in a particular job role are male and therefore favour men in job applications. Our data is polluted by a set of myths from the age of <a href="https://en.wikipedia.org/wiki/Age_of_Enlightenment#:%7E:text=The%20Enlightenment%20included%20a%20range,separation%20of%20church%20and%20state.">“enlightenment”</a>, including biases that lead to <a href="https://www.gaytascience.com/transphobic-algorithms/">discrimination based on gender and sexual identity</a>.</p>
<p>Judging from the history in societies where racism has played a role in <a href="https://sk.sagepub.com/books/racism-from-slavery-to-advanced-capitalism">establishing the social and political order</a>, extending privileges to white males –- in Europe, North America and Australia, for instance –- it is simple science to assume that residues of racist discrimination feed into our technology.</p>
<p>In my research for the book, I have documented some prominent examples. Face recognition software <a href="https://www.washingtonpost.com/technology/2019/12/19/federal-study-confirms-racial-bias-many-facial-recognition-systems-casts-doubt-their-expanding-use/">more commonly misidentified black and Asian minorities</a>, leading to false arrests in the US and elsewhere.</p>
<p>Software used in the criminal justice system has predicted that black offenders would have <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">higher recidivism rates</a> than they did. There have been false healthcare decisions. <a href="https://www.science.org/doi/10.1126/science.aax2342">A study found that</a> of the black and white patients assigned the same health risk score by an algorithm used in US health management, the black patients were often sicker than their white counterparts.</p>
<p>This reduced the number of black patients identified for extra care by more than half. Because less money was spent on black patients who have the same level of need as white ones, the algorithm falsely concluded that black patients were healthier than equally sick white patients. Denial of mortgages for minority populations is facilitated by biased data sets. The list goes on.</p>
<section id="machines-dont-lie" class="level2">
<h2 class="anchored" data-anchor-id="machines-dont-lie">Machines don’t lie?</h2>
<p>Such oppressive algorithms intrude on almost every <a href="https://www.newscientist.com/article/mg25033390-200-the-essential-guide-to-the-algorithms-that-run-your-life/">area of our lives</a>. AI is making matters worse, as it is sold to us as essentially unbiased. We are told that machines don’t lie. Therefore, the logic goes, no one is to blame.</p>
<p>This pseudo-objectiveness is central to the AI-hype created by the Silicon Valley tech giants. It is easily discernible from the speeches of Elon Musk, Mark Zuckerberg and Bill Gates, even if now and then they <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">warn us about the projects</a> that they themselves are responsible for.</p>
<p>There are various unaddressed legal and ethical issues at stake. Who is accountable for the mistakes? Could someone claim compensation for an algorithm denying them parole based on their ethnic background in the same way that one might for a toaster that exploded in a kitchen?</p>
<p>The <a href="https://umdearborn.edu/news/ais-mysterious-black-box-problem-explained#:%7E:text=This%20inability%20for%20us%20to,when%20they%20produce%20unwanted%20outcomes.">opaque nature of AI technology</a> poses serious challenges to legal systems which have been built around individual or human accountability. On a more fundamental level, basic human rights are threatened, as legal accountability is blurred by the maze of technology placed between perpetrators and the various forms of discrimination that can be conveniently blamed on the machine.</p>
<p>Racism has always been a systematic strategy to order society. It builds, legitimises and enforces hierarchies between the haves and have nots.</p>
</section>
<section id="ethical-and-legal-vacuum" class="level2">
<h2 class="anchored" data-anchor-id="ethical-and-legal-vacuum">Ethical and legal vacuum</h2>
<p>In such a world, where it’s difficult to disentangle truth and reality from untruth, our privacy needs to be legally protected. The right to privacy and the concomitant ownership of our virtual and real-life data needs to be codified as a human right, not least in order to harvest the real opportunities that good AI harbours for human security.</p>
<p>But as it stands, the innovators are far ahead of us. Technology has outpaced legislation. The ethical and legal vacuum thus created is readily exploited by criminals, as this brave new AI world is largely anarchic.</p>
<p>Blindfolded by the mistakes of the past, we have entered a wild west without any sheriffs to police the violence of the digital world that’s enveloping our everyday lives. The tragedies are already happening on a daily basis.</p>
<p>It is time to counter the ethical, political and social costs with a concerted social movement in support of legislation. The first step is to educate ourselves about what is happening right now, as our lives will never be the same. It is our responsibility to plan the course of action for this new AI future. Only in this way can a good use of AI be codified in local, national and global institutions.</p>
<!-- Below is The Conversation's page counter tag. Please DO NOT REMOVE. -->
<p><img src="https://realworlddatascience.net/viewpoints/posts/2023/09/06/https:/counter.theconversation.com/content/211778/count.gif?distributor=republish-lightbox-basic" alt="The Conversation" width="1" height="1" style="border: none !important; box-shadow: none !important; margin: 0 !important; max-height: 1px !important; max-width: 1px !important; min-height: 1px !important; min-width: 1px !important; opacity: 0 !important; outline: none !important; padding: 0 !important" referrerpolicy="no-referrer-when-downgrade"></p>
<!-- End of code. If you don't see any code above, please get new code from the Advanced tab after you click the republish button. The page counter does not collect any personal data. More info: https://theconversation.com/republishing-guidelines -->
<div class="article-btn">
<p><a href="../../../../../viewpoints/index.html">Discover more Viewpoints</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<a href="https://theconversation.com/profiles/arshin-adib-moghaddam-211780">Arshin Adib-Moghaddam</a> is professor in global thought and comparative philosophies, <a href="https://theconversation.com/institutions/soas-university-of-london-975">SOAS, University of London</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-12">
<dl>
<dt>Copyright and licence</dt>
<dd>
This article is republished from <a href="https://theconversation.com">The Conversation</a> under a Creative Commons license. Read the <a href="https://theconversation.com/for-minorities-biased-ai-algorithms-can-damage-almost-every-part-of-life-211778">original article</a>.
</dd>
<dd>
<p>Image by <a href="http://alanwarburton.co.uk/">Alan Warburton</a> / © BBC / <a href="https://www.betterimagesofai.org">Better Images of AI</a> / Quantified Human / <a href="https://creativecommons.org/licenses/by/4.0/">Licenced by CC-BY 4.0</a>.</p>
</dd>
</dl>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>AI</category>
  <category>AI ethics</category>
  <category>Bias</category>
  <category>Ethics</category>
  <guid>https://realworlddatascience.net/viewpoints/posts/2023/09/06/biased-algorithms.html</guid>
  <pubDate>Tue, 05 Sep 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/viewpoints/posts/2023/09/06/images/AlanWarburton-QuantifiedHuman-991x724.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy</title>
  <dc:creator>Brian Tarran and Julia Lane</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/08/21/00-food-for-thought.html</link>
  <description><![CDATA[ 




<p>There’s a saying: “You are what you eat.” Its meaning is somewhat open to interpretation, as with many such sayings, but it is typically used to make the point that if you want to <em>be</em> well, you need to eat well. Nutrition scientists and dieticians spend their careers trying to figure out what “eating well” looks like – the foods the human body needs, in what quantities, and how best to consume them. Their research informs advice and guidance issued by health professionals and governments. Ultimately, though, the choice of what to eat falls to us – individuals and families – and our choices are often determined by our tastes, the availability of foodstuffs in our local stores, their price and affordability.</p>
<p>So, what exactly <em>do</em> we eat? Answers come from a variety of sources. In the United States, there are dietary recall studies such as the <a href="https://www.cdc.gov/nchs/nhanes/index.htm">National Health and Nutrition Examination Survey</a>, which asks a sample of respondents to report their food and beverage consumption over a set period of time. There are also organisations like <a href="https://www.iriworldwide.com/en-gb">IRI</a> that collect point-of-sale data from retail stores on the actual food and drink being sold to consumers. By and large, this information comes from barcodes on product packaging being scanned at checkouts, so it is often referred to as “scanner data”.</p>
<p>This data – from dietary recall studies and retail scanners – is valuable: once we know what people are eating, we can check the nutritional content of those foods and build up a picture of what the diet of a typical individual or family looks like and how it compares to the diet recommended by doctors and policymakers. And, if we know what other foodstuffs are available, how much they cost, and the nutritional value of those items, we can work out how much families need to spend, and on what, in order to eat well and, hopefully, be well.</p>
<p>Figuring all this out is where something called the Purchase to Plate Crosswalk (PPC) comes in. It’s a key tool for understanding the “<a href="https://www.sciencedirect.com/science/article/pii/S0889157521005445">healthfulness of retail food purchases</a>” and it does this by linking IRI scanner data on what people buy with data on the nutritional content of those foods, as recorded in the US Department of Agriculture’s Food and Nutrient Database for Dietary Studies (FNDDS). But there’s a catch: scanner data is collected about hundreds of thousands of food products, whereas the FNDDS has nutritional profile information for only a few thousand items. Linking these two datasets therefore gives rise to a one-to-many matching problem – a problem that takes several hundred person-hours to resolve.</p>
<p>What if machine learning can help? That question inspired a competition, the Food for Thought Challenge, organized by the Coleridge Initiative, a nonprofit organization working with governments to ensure that data are more effectively used for public decision-making. Researchers and data scientists were invited to use machine learning and natural language processing to more efficiently link data on supermarket products to nutrient databases.</p>
<p>This collection of articles tells the story of the <a href="https://coleridgeinitiative.org/projects/food-for-thought">Food for Thought Challenge</a>. We begin by exploring the <a href="../../../../../case-studies/posts/2023/08/21/01-purchase-to-plate.html">policy issues</a> that drive the development of the PPC – the need to understand the national diet, developing healthy diet plans, and costing up those plans – and the issues posed by record linkage. Next, we learn about <a href="../../../../../case-studies/posts/2023/08/21/02-competition-design.html">the nature of the challenge and the structure of the competition in more detail</a>, and then the <a href="../../../../../case-studies/posts/2023/08/21/03-first-place-winners.html">three</a> <a href="../../../../../case-studies/posts/2023/08/21/04-second-place-winners.html">winning</a> <a href="../../../../../case-studies/posts/2023/08/21/05-third-place-winners.html">teams</a> walk us through their solutions. We end the collection with some closing thoughts on <a href="../../../../../case-studies/posts/2023/08/21/06-value-of-competitions.html">the value of competitions for addressing data scientific challenges in the public sector</a>.</p>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/index.html">Find more case studies</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/01-purchase-to-plate.html">Part 1: The Purchase to Plate Suite →</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Brian Tarran</strong> is editor of Real World Data Science, and head of data science platform at the Royal Statistical Society.
</dd>
<dd>
<p><strong>Julia Lane</strong> is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative, whose goal is to use data to transform the way governments access and use data for the social good through training programs, research projects and a secure data facility. She recently served on the Advisory Committee on Data for Evidence Building and the National AI Research Resources Task Force.</p>
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Royal Statistical Society and Julia Lane
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@melaniesylim?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Melanie Lim</a> on <a href="https://unsplash.com/photos/246b6c6IeC0?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Tarran, Brian, and Julia Lane. 2023. “The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/00-food-for-thought.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



 ]]></description>
  <category>Machine learning</category>
  <category>Natural language processing</category>
  <category>Public policy</category>
  <category>Health and wellbeing</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/08/21/00-food-for-thought.html</guid>
  <pubDate>Sun, 20 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/00-shopping.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Food for Thought: The importance of the Purchase to Plate Suite</title>
  <dc:creator>Andrea Carlson and Thea Palmer Zimmerman</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/08/21/01-purchase-to-plate.html</link>
  <description><![CDATA[ 




<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Disclaimer
</div>
</div>
<div class="callout-body-container callout-body" style="margin-top: 0;">
<p>The findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA or US Government determination or policy. This research was supported by the US Department of Agriculture’s Economic Research Service and Center for Nutrition, Policy and Promotion. Findings should not be attributed to Circana (formerly IRI).</p>
</div>
</div>
<p>About 600,000 <a href="https://www.cdc.gov/nchs/fastats/leading-causes-of-death.htm">deaths per year in the United States</a> are related to chronic diseases that are linked to poor dietary choices. Many other individuals suffer from diet-related health conditions, which may limit their ability to work, learn, and be physically active <span class="citation" data-cites="usda_2020">(US Department of Agriculture and US Department of Health and Human Services 2020)</span>. In recognition of the link between diet and health, in 1974 the Senate Select Committee on Nutrition and Human Needs, originally formed to eliminate hunger, expanded its focus to improving eating habits, nutrition policy and the national diet. Since 1980, the Dietary Guidelines for Americans have been released every five years by the US Departments of Agriculture (USDA) and Health and Human Services (DHHS). The guidelines present “<a href="https://www.dietaryguidelines.gov/">advice on what to eat and drink to meet nutrient needs, promote health, and prevent disease</a>”.</p>
<p>Because there can be economic and social barriers to maintaining a healthy diet, USDA promotes <a href="https://www.usda.gov/nutrition-security">Food and Nutrition Security</a> so that everyone has consistent and equitable access to healthy, safe, and affordable foods that promote optimal health and well-being. A set of data tools called the <a href="https://www.ers.usda.gov/data-products/purchase-to-plate/">Purchase to Plate Suite</a> (PPS) supports these goals by enabling the update of the <a href="https://www.fns.usda.gov/snap/thriftyfoodplan#:~:text=What%20is%20the%20Thrifty%20Food,lowest%20cost%20of%20the%20four.">Thrifty Food Plan</a> (TFP), which estimates how much a budget-conscious family of four needs to spend on groceries to ensure a healthy diet. The TFP market basket – consisting of the specific amounts of various food categories required by the plan – forms the basis of the maximum allotment for the Supplemental Nutrition Assistance Program (SNAP, formerly known as the “Food Stamps” program), which provided financial support towards the cost of groceries for <a href="https://www.fns.usda.gov/pd/supplemental-nutrition-assistance-program-snap">over 41 million individuals in almost 22 million households in fiscal year 2022</a>.</p>
<p>The 2018 Farm Act (Agriculture Improvement Act of 2018) requires that USDA reevaluate the TFP every five years using current food composition, consumption patterns, dietary guidance, and food prices, and using approved scientific methods. USDA’s Economic Research Service (ERS) was charged with estimating the current food prices using retail food scanner data <span class="citation" data-cites="levin_et_al_2018 muth_et_al_2016">(Levin et al. 2018; Muth et al. 2016)</span> and utilized the PPS for this task. The most recent TFP update was released in August 2021 and the revised cost of the market basket was the first non-inflation adjustment increase in benefits for SNAP in over 40 years <span class="citation" data-cites="thrifty_food_plan_2021">(US Department of Agriculture 2021)</span>.</p>
<p>The PPS combines datasets to enhance research related to the economics of food and nutrition. There are four primary components of the suite:</p>
<ul>
<li>Purchase to Plate Crosswalk (PPC),</li>
<li>Purchase to Plate Price Tool (PPPT),</li>
<li>Purchase to Plate National Average Prices (PP-NAP) for the National Health and Nutrition Examination Survey (NHANES), and</li>
<li>Purchase to Plate Ingredient Tool (PPIT).</li>
</ul>
<p>The PPC allows researchers to measure the healthfulness of store purchases. On average <a href="https://www.ers.usda.gov/data-products/foodaps-national-household-food-acquisition-and-purchase-survey/summary-findings/#calories">US consumers acquire about 75% of their calories from retail stores</a>, and there are a number of studies linking the availability of foods at home to the healthfulness of the overall diet <span class="citation" data-cites="gattshall_et_al_2008 hanson_et_al_2005">(e.g., Gattshall et al. 2008; Hanson et al. 2005)</span>. Thus, understanding the healthfulness of store purchases allows us to understand differences in consumers who purchase healthy versus less healthy foods, and may contribute to better policies that promote healthier food purchases. While healthier diets are linked to a lower risk of disease outcomes <span class="citation" data-cites="REEDY2014881">(Reedy et al. 2014)</span>, other factors such as health care access may also be contributors <span class="citation" data-cites="cleary_et_al_2022">(Cleary, Liu, and Carlson 2022)</span>. The PPC also forms the basis of the price tool, PPPT – which allows researchers to estimate custom prices for dietary recall studies – and a new ERS data product, the <a href="https://www.ers.usda.gov/data-products/purchase-to-plate/">PP-NAP</a>. The national average prices from PP-NAP are used in reevaluating the TFP. By using the PP-NAP with 24-hour dietary recall information from surveys such as What We Eat in America (<a href="https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/food-surveys-research-group/docs/wweianhanes-overview/">WWEIA</a>) – the dietary component of the nationally representative <a href="https://www.cdc.gov/nchs/nhanes/index.htm">National Health and Nutrition Examination Survey</a>(NHANES)<sup>1</sup> – researchers can examine the relationship between the cost of food, dietary intake, and chronic diseases linked to poor diets. The price estimates also allow researchers to develop cost-effective healthy diets such as <a href="https://www.myplate.gov/myplate-kitchen/recipes">MyPlate Kitchen</a>. The final component of the Purchase to Plate Suite, the ingredient tool (PPIT), breaks dietary recall-reported foods back into purchasable ingredients, based on US retail food purchases. The PPIT is also used in the revaluation of the TFP, and by researchers who want to look at the relationship between reported ingestion of grocery items, cost and disease outcomes using WWEIA/NHANES. More information on the development of the PPC is available in two papers by Carlson et al. <span class="citation" data-cites="carlson_et_al_2019 carlson_et_al_2022">(2019, 2022)</span>.</p>
<p>The Food for Thought competition aimed to support the development of the PPC – and thus policy-oriented research – by linking retail food scanner data to the USDA nutrition data used to analyze NHANES dietary recall data, specifically the Food and Nutrient Database for Dietary Studies (FNDDS) <span class="citation" data-cites="fndds_2018 fndds_2020">(2018, 2020)</span>. In particular, the competition set out to use artificial intelligence (AI) to reduce human resources in creating the links for the PPC, while still maintaining the high-quality standards required for reevaluating the TFP and for data published by ERS (which is one of 13 Principle Statistical Agencies in the United States Federal Government).</p>
<section id="methods-used-to-date" class="level2">
<h2 class="anchored" data-anchor-id="methods-used-to-date">Methods used to date</h2>
<p>On the surface, the linking process may appear simple: both the FNDDS and retail food scanner data are databases of food. But the scanner data are produced for market research, and the FNDDS for dietary studies. The scanner data include about 350,000 items with sales each year, while the FNDDS has only 10,000–15,000 items. Scanner data relates to specific products, while FNDDS items are often more general. Both datasets have different hierarchical structures – the FNDDS hierarchy is based around major food groups: dairy; meat, poultry and seafood; eggs; nuts and legumes; grains; fruits; vegetables; fats and oils; and sugars, sweets, and beverages. Items fall into the groups regardless of preparation method or form. That is, broccoli prepared from frozen and from fresh both appear in the vegetable group, and for some fruits and vegetables, the fresh, frozen, canned and dried form are the same FNDDS item. Vegetable-based mixed dishes, such as broccoli and carrot stir-fry or soup, are also classified in the vegetable group. On the other hand, the scanner data classifies foods by grocery aisle. That is, the fresh and frozen broccoli are classified in different areas: produce and frozen vegetables. Similarly, when sold as a prepared food, the broccoli and carrot stir-fry may be found in the frozen entries, as a kit in either the frozen or produce section, refrigerated foods, or all of these.</p>
<p>To allow researchers to import the FNDDS nutrient data into the scanner data, a one-to-many match between FNDDS and scanner data items was needed. The food descriptions in the scanner data include brand names and package sizes and are written as a consumer would pronounce them – e.g., fresh and crisp broccoli florets, ready-cut, 10 oz – versus a more general FNDDS description such as “Broccoli, raw”. (Also linked to the “Broccoli, raw” code would be broccoli sold with stems attached, broccoli spears, and any other way raw broccoli is sold.) In the scanner data, the Universal Product Code (UPC) and the European Article Number (EAN) can link items between tables within the scanner data, as well as between datasets of grocery items, such as the USDA Global Branded Foods Product Database, a component of <a href="https://fdc.nal.usda.gov/index.html">USDA’s Food Data Central</a>. However, these codes are not related to the FNDDS codes, or any other column within the FNDDS. In other words, before development of the PPC, there were no established linking identifiers.</p>
<p>Figure 1 shows the process USDA uses to develop matches between scanner data and FNDDS.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/pt1-fig1.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt1-fig1.png" class="img-fluid figure-img" width="700"></a></p>
</figure>
</div>
<div class="figure-caption">
<p><strong>Figure 1:</strong> Process currently used to create the matches between the USDA Food and Nutrient Database for Dietary Studies (FNDDS) and the retail scanner data (labelled “IRI” for the IRI InfoScan and Consumer Network) product dictionaries. Source: Author provided.</p>
</div>
<p>We start the linking process by categorizing the scanner data items into homogeneous groups to make the first round of automated matching more efficient. To save time, we use the second lowest hierarchical category in the scanner data which generally divides items within a grocery aisle into homogenous groups such as produce, canned beans, baking mixes, and bread. Once the linking categories for scanner data are established, we select appropriate items from the FNDDS. Since the FNDDS is highly structured, this selection is usually straightforward.</p>
<p>Our next step is to use semantic matching to create a search table that aligns similar terms within the IRI product dictionary and FNDDS. This first requires that we extract attributes from the FNDDS descriptions into fields similar to those in the scanner data product dictionary. The FNDDS descriptions are found across multiple columns because they are added as the need arises to provide examples of brand names or alternative descriptions of foods which help code the foods WWEIA participants report eating. We manually create matching tables that link terms used in FNDDS to those used in the scanner data, organized by the fields defined in the restructured FNDDS. We then use this table as the basis of a probabilistic matching process. For example, when linking the produce group, “fresh” in the scanner data would be aligned with “raw” and “prepared from fresh” and NOT “prepared from frozen” in the FNDDS, and “broccoli florets” would also be aligned with “raw” and “broccoli”. Since the FNDDS is designed to code the foods individuals report eating, many of the foods in the FNDDS are already prepared and result in descriptions such as “broccoli, steamed, prepared from fresh” or “broccoli, boiled, prepared from frozen”.</p>
<p>Once the linking table is established, the probabilistic match process returns the single best possible match for each item in the scanner data. For example, a match between fresh broccoli florets and frozen broccoli would have a lower probability score than “broccoli, raw”. Because these matches form the basis of major USDA policies, we cannot accept an error rate of more than 5 percent, and lower is preferred. To reach that goal, nutritionists review every match to make sure the probabilistic match did not return a match between cauliflower florets and fresh broccoli, say, or that a broccoli and carrot stir-fry is not matched to a dish with broccoli, carrots, and chicken. The correct matches, such as the one between fresh broccoli florets and raw broccoli, are set aside while the items with an incorrect match, such as cauliflower florets and the broccoli and carrot stir-fry, are used to revise the search table. Revisions might include adding (NOT chicken) to the broccoli and carrot stir-fry dish. Mixed dishes — such as the broccoli and carrot stir-fry — pose particular challenges because there are a wide variety of similar products available in the grocery store. After a few rounds of revising the search table and running the probabilistic match process, it is more efficient to use a manual match, established by one nutritionist and reviewed by another, after which the match is assumed to be correct.</p>
<p>The process improved with each new wave of FNDDS and IRI data. Our first creation of the PPC linked the FNDDS 2011/12 to the 2013 IRI retail scanner data. Subsequent waves started with the previous search table and resulting matches were reviewed by nutritionists. We also used more fields in the IRI product dictionary to create the homogeneous linking groups and made modifications to these groups with each wave. During each wave we experimented with the number of rounds of probabilistic matching that was the most cost effective. For some linking groups it took less human time to manually match from the start, while for other groups it was more efficient to do multiple rounds of improvements to the search table. Starting with the most recent wave (matching FNDDS 2017/18 to the 2017 and 2018 retail scanner data), we assumed previous matches appearing in the newer data were correct. Although this assumption was good for most matches, a review demonstrated the need to review previous matches prior to removing the item from the list of scanner data items needing FNDDS matches. In the future we intend to explore methods developed by the participants of the Food for Thought competition.</p>
</section>
<section id="linking-challenges" class="level2">
<h2 class="anchored" data-anchor-id="linking-challenges">Linking challenges</h2>
<p>An ongoing challenge to the linking problem is that both the scanner data and the FNDDS undergo substantive changes each year, meaning that both the previous matches and search tables need to be reviewed and revised with each new effort, as tables that work with one cycle of FNDDS and scanner data will need revisions to use with the next cycle. Changes to the scanner data that impact our current method include dropped and added items, data corrections, and revisions to the categories that form the basis of the homogeneous linking groups. In addition, there are errors such as incorrect food descriptions, conflicting package size information, and changes in the item description from year to year. Since the FNDDS is designed to support dietary recall studies, revisions reflect both changes to available foods and the level of detail respondents can provide. These revisions result in dropped/added food codes, changes to food descriptions that impact which scanner data items match to the FNDDS items, and revisions to recipes used in the nutrient coding which impacts the number of retail ingredients available in the FNDDS.</p>
<p>Of the four parts of the PPS, establishing the matches is the most time-consuming task and constitutes at least 60 percent of the total budget. In the most recent round, we had 168 categories and each one went through 2-3 automated matching rounds; after each round, nutritionists spent an average of two hours reviewing the matches. This adds up to somewhere between 670 and 1,000 hours of review time. After the automated review, manual matching requires an additional 300 hours. Reducing the amount of time required to establish matches and link the FNDDS and retail scanner datasets may lead to significant time savings, resulting in faster data availability. That, in turn, could allow more timely policy-based research, and the mandated revision of the Thrifty Food Plan can continue with the most recent food price data.</p>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/00-food-for-thought.html">← Introduction</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/02-competition-design.html">Part 2: Competition design →</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Andrea Carlson</strong> is an agricultural economist in the Food Markets Branch of the Food Economics Division in USDA’s Economic Research Service. She is the project lead for the Purchase to Plate Suite, which allows users to import USDA nutrient and food composition data into retail food scanner data acquired by USDA and estimate individual food prices for dietary intake data.
</dd>
<dd>
<p><strong>Thea Palmer Zimmerman</strong> is a senior study director and research nutritionist at Westat.</p>
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Image credit</dt>
<dd>
Thumbnail photo by <a href="https://unsplash.com/@neonbrand?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Kenny Eliason</a> on <a href="https://unsplash.com/photos/SvhXD3kPSTY?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Carlson, Andrea, and Thea Palmer Zimmerman. 2023. “Food for Thought: The importance of the Purchase to Plate Suite.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/01-purchase-to-plate.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>
</section>
<section id="acknowledgements" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h2>
<p>The research presented in this compendium supports the Purchase to Plate Suite of data products. Carlson has been privileged to both develop and lead this project over the course of her career, but it is not a solo project. Many thanks to the Linkages Team from USDA’s Economic Research Service (Christopher Lowe, Mark Denbaly Elina Page, and Catherine Cullinane Thomas) the Center for Nutrition Policy and Promotion (Kristin Koegel, Kevin Kuczynski, Kevin Meyers Mathieu, TusaRebecca Pannucci), and our contractor Westat, Inc.&nbsp;(Thea Palmer Zimmerman, Carina E. Tornow, Amber Brown McFadden, Caitlin Carter, Viji Narayanaswamy, Lindsay McDougal, Elisha Lubar, Lynnea Brumby, Raquel Brown, and Maria Tamburri). Many others have supported this project over the years.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-carlson_et_al_2019" class="csl-entry">
Carlson, A. C., E. T. Page, T. P. Zimmerman, C. E. Tornow, and S. Hermansen. 2019. <span>“Linking USDA Nutrition Databases to IRI Household-Based and Store-Based Scanner Data.”</span> Technical bulletin 1952. US Department of Agriculture, Economic Research Service.
</div>
<div id="ref-carlson_et_al_2022" class="csl-entry">
Carlson, A. C., C. E. Tornow, E. T. Page, A. Brown McFadden, and T. Palmer Zimmerman. 2022. <span>“Development of the Purchase to Plate Crosswalk and Price Tool: Estimating Prices for the National Health and Nutrition Examination Survey (NHANES) Foods and Measuring the Healthfulness of Retail Food Purchases.”</span> <em>Journal of Food Composition and Analysis</em> 106: 104344. <a href="https://doi.org/10.1016/j.jfca.2021.104344">https://doi.org/10.1016/j.jfca.2021.104344</a>.
</div>
<div id="ref-cleary_et_al_2022" class="csl-entry">
Cleary, R., Y. Liu, and A. Carlson. 2022. <span>“Differences in the Distribution of Nutrition Between Households Above and Below Poverty.”</span> Agricultural and Applied Economic Association Annual Meeting. Anaheim, CA. <a href="https://ageconsearch.umn.edu/record/322267">https://ageconsearch.umn.edu/record/322267</a>.
</div>
<div id="ref-gattshall_et_al_2008" class="csl-entry">
Gattshall, M. L., J. A. Shoup, J. A. Marshall, L. A. Crane, and P. A. Estabrooks. 2008. <span>“Validation of a Survey Instrument to Assess Home Environments for Physical Activity and Healthy Eating in Overweight Children.”</span> <em>International Journal of Behavioral Nutrition and Physical Activity</em> 5 (3). <a href="https://doi.org/10.1186/1479-5868-5-3">https://doi.org/10.1186/1479-5868-5-3</a>.
</div>
<div id="ref-hanson_et_al_2005" class="csl-entry">
Hanson, N. I., D. Neumark-Sztainer, M. E. Eisenberg, M. Story, and M. Wall. 2005. <span>“Associations Between Parental Report of the Home Food Environment and Adolescent Intakes of Fruits, Vegetables and Dairy Foods.”</span> <em>Public Health Nutrition</em> 8 (1). <a href="https://doi.org/10.1079/PHN2005661">https://doi.org/10.1079/PHN2005661</a>.
</div>
<div id="ref-levin_et_al_2018" class="csl-entry">
Levin, D., D. Noriega, C. Dicken, A. Okrent, M. Harding, and M. Lovenheim. 2018. <span>“Examining Store Scanner Data: A Comparison of the IRI Infoscan Data with Other Data Sets, 2008-12.”</span> Technical bulletin 1949. US Department of Agriculture, Economic Research Service.
</div>
<div id="ref-muth_et_al_2016" class="csl-entry">
Muth, M. K., M. Sweitzer, D. Brown, K. Capogrossi, S. Karns, D. Levin, A. Okrent, P. Siegel, and C. Zhen. 2016. <span>“Understanding IRI Household-Based and Store-Based Scanner Data.”</span> Technical bulletin 1942. US Department of Agriculture, Economic Research Service.
</div>
<div id="ref-REEDY2014881" class="csl-entry">
Reedy, J., S. M. Krebs-Smith, P. E. Miller, A. D. Liese, L. L. Kahle, Y. Park, and A. F. Subar. 2014. <span>“Higher Diet Quality Is Associated with Decreased Risk of All-Cause, Cardiovascular Disease, and Cancer Mortality Among Older Adults.”</span> <em>The Journal of Nutrition</em> 144 (6): 881–89. <a href="https://doi.org/10.3945/jn.113.189407">https://doi.org/10.3945/jn.113.189407</a>.
</div>
<div id="ref-thrifty_food_plan_2021" class="csl-entry">
US Department of Agriculture. 2021. <span>“Thrifty Food Plan, 2021.”</span> Food and Nutrition Service 916. US Department of Agriculture. <a href="https://FNS.usda.gov/TFP">https://FNS.usda.gov/TFP</a>.
</div>
<div id="ref-fndds_2018" class="csl-entry">
US Department of Agriculture, Agricultural Research Service. 2018. <span>“USDA Food and Nutrient Database for Dietary Studies 2015-2016.”</span> US Department of Agriculture, Agricultural Research Service. <a href="https://www.ars.usda.gov/nea/bhnrc/fsrg">https://www.ars.usda.gov/nea/bhnrc/fsrg</a>.
</div>
<div id="ref-fndds_2020" class="csl-entry">
———. 2020. <span>“USDA Food and Nutrient Database for Dietary Studies 2017-2018.”</span> US Department of Agriculture, Agricultural Research Service. <a href="https://www.ars.usda.gov/nea/bhnrc/fsrg">https://www.ars.usda.gov/nea/bhnrc/fsrg</a>.
</div>
<div id="ref-usda_2020" class="csl-entry">
US Department of Agriculture and US Department of Health and Human Services. 2020. <span>“Dietary Guidelines for Americans, 2020-2025.”</span> 9th edition. <span>US Department of Agriculture and US Department of Health and Human Services</span>. <a href="https://DietaryGuidelines.gov">https://DietaryGuidelines.gov</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>NHANES is a multi-module continuous survey conducted by the Centers for Disease Control and Prevention. In addition to the WWEIA, NHANES includes a four-hour complete medical exam including a health history, and a blood and urine analysis.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Machine learning</category>
  <category>Natural language processing</category>
  <category>Public policy</category>
  <category>Health and wellbeing</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/08/21/01-purchase-to-plate.html</guid>
  <pubDate>Sun, 20 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/01-pps.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Food for Thought: Competition and challenge design</title>
  <dc:creator>Zheyuan Zhang and Uyen Le</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/08/21/02-competition-design.html</link>
  <description><![CDATA[ 




<p>Since 2014, the professional services firm Westat, Inc.&nbsp;has been developing the Purchase to Plate Crosswalk (PPC) for the United States Department of Agriculture (USDA) Economic Research Service (ERS). The PPC links the retail food transactions database from IRI’s InfoScan service and the USDA Food and Nutrient Database for Dietary Studies (FNDDS). However, the current linkage process uses only partly automated data matching, meaning it is resource intensive, time consuming, and requires manual review.</p>
<p>With sponsorship from ERS, Westat partnered with the Coleridge Initiative to host the Food for Thought competition to challenge researchers and data scientists to use machine learning and natural language processing to find accurate and efficient methods for creating the PPC. Figure 1 provides a visual overview of the challenge set by the competition.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/pt2-fig1.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt2-fig1.png" class="img-fluid figure-img" width="700"></a></p>
</figure>
</div>
<div class="figure-caption">
<p><strong>Figure 1:</strong> Overview of the Food for Thought Competition Challenge.</p>
</div>
<p>The one-to-many matching task that is central to the competition throws up many challenges for researchers to wrestle with. Because IRI data contains food transactions collected from partnered retail establishments for over 350,000 items, the matchings need to be made based on limited data features, including categories, providers, and semantically inconsistent descriptions that consist of short phrases. Consider this hypothetical example: IRI product-related information about a (fictional) “Cheesy Hashbrowns Hamburger Helper, 5.5 Oz Box” needs to be linked to FNDDS nutrition-related information found under “Mixed dishes – meat, poultry, seafood: Mixed meat dishes”. Figure 2 demonstrates how the two databases are linked with each other to create the PPC. As can be seen, there is no common word that easily indicates that “Cheesy Hashbrowns Hamburger Helper…” should be matched with “Mixed dishes…”, and such cases exist in all IRI tables used for the challenge, from 2012 through 2018.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/pt2-fig2.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt2-fig2.png" class="img-fluid figure-img" width="700"></a></p>
</figure>
</div>
<div class="figure-caption">
<p><strong>Figure 2:</strong> Each universal product code (UPC) from the IRI data could match to only one ensemble code (EC) from the FNDDS data, whereas one EC code could match to multiple UPCs.</p>
</div>
<p>Also, because nutritionists or food scientists will always need to review the matching, regardless of the matching method used, it was important that our evaluation of proposed matching methods focused both on the accuracy of prediction models and also on metrics that would lead participants to develop models that facilitate qualified reviewers to reduce their workloads.</p>
<p>Organising the competition was also a challenge in its own right, for data privacy reasons. IRI scanner data contains sensitive information, such as store name, location, unit price, and weekly quantity sold for each item. This ruled out using existing online platforms like Kaggle, DrivenData or AIcrowd to host the competition, and instead required a private secure data enclave to ensure the safe use of sensitive and confidential data assets. The need for such an environment imposed capacity constraints on the competition, meaning only dozens of teams could be invited to take part, whereas on open platforms it is common to have thousands of teams competing and sharing ideas and code.</p>
<section id="competition-structure" class="level2">
<h2 class="anchored" data-anchor-id="competition-structure">Competition structure</h2>
<p>The competition ran over 10 months and consisted of three separate challenges: two interim, one final. Applications opened in September 2021, and the competition started in January 2022. Submission deadlines for the first and second interim challenges were in July and September 2022, respectively. For these rounds, participants submitted preliminary solutions for evaluation based solely on quantitative metrics, and two awards of $10,000 were given to the highest-scoring teams. The deadline for the final challenge was in October 2022. Here, solutions were evaluated by the scientific review board based on three judging criteria: quantitative metrics, transferability, and innovation. First, second, and third place winners received awards of $30,000, $1,500, and $1,000 respectively. Final presentations were given at the Food for Thought symposium in December 2022.</p>
<p>The competition was run entirely within the Coleridge Initiative’s Administrative Data Research Facility (ADRF), which was established by the United States Census Bureau to inform the decision-making of the Commission on Evidence-Based Policy under the Evidence Act. ADRF follows the Five Safes Framework: safe projects, safe people, safe data, safe settings, and safe outputs.</p>
<p>In keeping with this framework, participants were provided with ADRF login credentials after signing the relevant data use agreements during the onboarding process. All participants were required to agree to the ADRF terms of use, to complete security training, and to pass a security training assessment prior to accessing the challenge data. Participants’ access within ADRF was limited to the challenge environment and data only. There was no internet access, so Coleridge Initiative ensured that any packages requested by teams were available for use within the environment after passing security review. All codes and documentation were only allowed to be exported outside ADRF after export reviews from both Coleridge Initiative and USDA staff. At the end of each challenge, the teams submitted write-ups and supporting files by placing all the necessary submission files in their ADRF team folder. Detailed submission instructions are available via the <a href="https://github.com/realworlddatascience/realworlddatascience.github.io/tree/main/case-studies/posts/2023/08/21/_code">Real World Data Science GitHub repository</a>.</p>
</section>
<section id="metrics" class="level2">
<h2 class="anchored" data-anchor-id="metrics">Metrics</h2>
<p>Submissions were evaluated by Coleridge Initiative and technical review and subject review boards based on the following criteria:</p>
<ul>
<li><strong>Quantitative metrics</strong> were used to measure the predictive accuracy and runtime of the model.<br>
</li>
<li><strong>Transferability</strong> measured the quality of documentation and code, and the ability of individuals who are not involved in model development to replicate and implement the team’s approach.<br>
</li>
<li><strong>Innovation</strong> measured novelty and creativity of the model in addressing the linkage problem.</li>
</ul>
<p>Technical review was overseen by faculty members from computer science and engineering departments of top US universities. Subject review was handled by subject matter experts from USDA and Westat.</p>
<p>From a quantitative perspective, the most common way to evaluate machine learning competition submissions is to use model predictive accuracy. However, single metrics are typically incomplete descriptions of real-world tasks, and they can easily hide significant differences between models which simple predictive accuracy cannot capture. To select the most appropriate official challenge metrics, Coleridge Initiative reviewed the literature on the use of evaluation measures in both classification and ranking task machine learning competitions. Success at 5 (S@5) and Normalized Discounted Cumulative Gain at 5 (NDCG@5) scores were ultimately used as the quantitative metrics.</p>
<p>The metrics were applied as follows: models proposed by each team were tasked with outputting five potential FNDDS matches for each IRI code, with potential FNDDS matches ordered from most likely to least likely. S@5 and NDCG@5 scores are broadly similar – both measure whether a correct match is present in the five proposed matches that participants were asked to identify. However, S@5 does not take rank position into account and only considers whether the five proposed FNDDS matches contain the correct FNDDS response. NDCG@5 does take rank into account and also measures how highly the correct FNDDS response is ranked among the five proposed matches. Both measures range from 0 to 1 (or 0% to 100%). Models get a “full credit” for S@5 as long as they contain the correct FNDDS option. NDCG@5 penalizes models when the correct match is ranked lower on the list of 5 proposed matches.</p>
</section>
<section id="technical-description" class="level2">
<h2 class="anchored" data-anchor-id="technical-description">Technical description</h2>
<section id="environment-setup" class="level5">
<h5 class="anchored" data-anchor-id="environment-setup">Environment setup</h5>
<p>Coleridge Initiative solicited technical requirements from participants at the challenge application stage to prepare the ADRF environment as much as possible before the competition began. Each team was asked to share anticipated workspace specifications and software library requests in their application package. From this we identified, reviewed, and installed the requested Python and R packages, libraries, and library components (e.g., pre-trained models, training data) that were not yet available within ADRF.</p>
<p>The setup of graphics processing units (GPUs) was also a critical part of competition preparation. We created an environment with 16 gibibyte (GiB) of GPU memory for each team. Our technology team met with multiple teams several times to discuss computing environment configurations to ensure the GPU could work properly. None of these efforts was wasted: without GPU access, it would be impossible for teams to use state-of-the-art pre-trained models such as the Bidirectional Encoder Representations from Transformers <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(BERT, Devlin et al. 2018)</span>.</p>
<p>We completed the setup of new team workspaces, each customized to the individual team’s resource and library requirements, including GPU configuration. The isolation and customization of workspaces was vital because teams may request different versions of libraries that potentially have version conflict with other libraries. We ensured the configurations were all set before the challenge began because such data challenges are bursty in nature <span class="citation" data-cites="macavaney_et_al_2021">(Macavaney et al. 2021)</span>, and handling support requests in the private data enclave risked causing delays. We hoped to avoid receiving too many requests in the beginning phase of the competition in order to give participants a better experience, though we did of course provide participants with instructions on how to request additional libraries during the challenge period.</p>
</section>
<section id="supporting-materials" class="level5">
<h5 class="anchored" data-anchor-id="supporting-materials">Supporting materials</h5>
<p>In addition to environment preparation, we made available a list of supporting documentation, including IRI, PPC, and FNDDS codebooks, technical reports, and related publications that could help teams understand the challenge datasets. The FNDDS codebook pooled information on variable availability, coding, and descriptions across dataset files and years. It also included internal Westat food category coding difficulty ratings and notes on created PPC codes and provided UPC code, EC code, and general dataset remarks and observations that may take time for analysts to discover on their own.</p>
<p>We developed a baseline model to demonstrate the challenge task and the expected outputs – both outside of ADRF using FNDDS and fictitious data in place of IRI data, and an analogous model using FNDDS and IRI data within the ADRF secure environment. Moreover, we provided the teams with an evaluation script to read in their submissions and evaluate them for predictive accuracy against the public test set using S@5 and NDCG@5 challenge metrics. Finally, we held multiple webinars during the course of the challenge to explain next steps, address participant questions, solicit feedback, and provide general support. Multiple teams also met with our technology team to clarify ADRF-related questions or troubleshoot technical issues.</p>
<p>(Baseline model, toolkits, and evaluation script are available from the <a href="https://github.com/realworlddatascience/realworlddatascience.github.io/tree/main/case-studies/posts/2023/08/21/_code">Real World Data Science GitHub repository</a>.)</p>
</section>
<section id="data-splitting" class="level5">
<h5 class="anchored" data-anchor-id="data-splitting">Data splitting</h5>
<p>To mimic the real-world scenario, the competition used 2012–2016 IRI data as the training set, and the 2017–2018 IRI data as the test set, since the data change over time and USDA could provide the most recent data available. To make sure that models were generalizable and not just overfit to the test set, we split the test set into private and public test sets. In this way, we guaranteed that the models were evaluated on completely hidden data. In order to keep the similar distribution of the two sets, we first divided the data into five quintiles based on EC code frequencies and then randomly sampled 80% of records in each group without repetition for placement into the private test set. Later in the competition, because of the computation limit, we further shrank the private test set to 40% of its original size using the same data-splitting method.</p>
</section>
<section id="judging" class="level5">
<h5 class="anchored" data-anchor-id="judging">Judging</h5>
<p>In the first two rounds, submissions were evaluated based on the quantitative metrics, as previously mentioned above. Coleridge Initiative was responsible for running the evaluation script, making sure not to re-train the model or modify the configs in any way, and only applying the model to predict the private test set. Prediction results were then compared against ground truth to get the private scores.</p>
<p>The final challenge was reviewed by the scientific review board on all three judging criteria. Submitted models were first evaluated by Coleridge Initiative in the same way as in the first two rounds. The runtime of models was also recorded as an assessment of model cost. The scientific review boards then assessed the models by the quality of documentation, the quality of code, and the ability to replicate and implement the team’s approach, and scored the models for innovation and creativity in addressing the linkage problem. Lastly, scores were summarized and the scientific review board discussed and decided the winners of the competition.</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>The next few articles in this collection walk readers through the solutions proposed by competition finalists. Figure 3 provides a brief summary.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/pt2-fig3.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt2-fig3.png" class="img-fluid figure-img" width="700"></a></p>
</figure>
</div>
<div class="figure-caption">
<p><strong>Figure 3:</strong> Top competitors and their solutions to the Food for Thought challenge.</p>
</div>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons learned</h2>
<p>It was undoubtedly challenging for teams to work with highly secured data in a private data enclave for this data challenge. We solicited feedback from teams and summarized the issues that we experienced throughout the competitions, together with the solutions to resolve those issues. Below are our main lessons learned and we hope this summary can serve to inform future competitions.</p>
<ul>
<li><p><strong>Environmental factors:</strong> The installation and setup of packages, libraries, and resources, as well as the configuration of GPUs, system dependencies, and workspace design were expected to take a long time as each team had their own needs. To accelerate the process, we requested a list of specific package and environment requirements from the teams in advance. However, due to the complexity of the system configuration required by the teams, environment setup took longer than expected. Thus, the challenge deadlines had to be postponed a few times to accommodate this.</p></li>
<li><p><strong>Time commitment:</strong> Twelve teams were selected to participate in the challenge, but only three teams remained in the final challenge. Other than one team that was disqualified for violating the ADRF terms of use agreement, eight dropped out because of other commitments and insufficient time to meaningfully participate. To ensure security, ADRF does not allow jobs to run in the backend, which also adds to the time commitment of teams. To encourage teams to participate in the final challenge, we gave out additional awards for second and third places.</p></li>
<li><p><strong>Computing resource limit:</strong> One issue encountered in evaluating submitted models was computing environment resource limits due to the secured nature of the data enclave. The original private test dataset is four times larger than the public test dataset, making it unfeasible to evaluate. To overcome this issue, given the fixed resource constraints, we decided to reduce the private test set to 40% of its original size. It would have been helpful, though, if the competition had set a model running time limit at the outset, so that participants could build simpler yet effective models.</p></li>
<li><p><strong>Supporting code:</strong> Although the initial baseline model we provided was extremely simple, we found this helped participants a lot in the initial phase – yet there is space to improve. To be specific, supporting codes should be constructed so that all relevant data tables are used and specify the main function to run the code, especially how the model should be tested. The teams only used the main table, which was the only table that was used in the baseline model, for training and did not touch the other supporting table. If we included the other table in the baseline model, it could help participants to have a better use of this data as well. In addition, a baseline model should be intuitive for the participants to follow, allowing evaluators to easily replace the public test set with the private test set without any programming modifications.</p></li>
</ul>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/01-purchase-to-plate.html">← Part 1: Purchase to Plate</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/03-first-place-winners.html">Part 3: First place winners →</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Zheyuan Zhang</strong> and <strong>Uyen Le</strong> are research scientists at the Coleridge Initiative.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Zheyuan Zhang and Uyen Le
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Zhang, Zheyuan, and Uyen Le. 2023. “Food for Thought: Competition and challenge design.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/02-competition-design.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DBLP:journals/corr/abs-1810-04805" class="csl-entry">
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2018. <span>“<span>BERT:</span> Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>CoRR</em> abs/1810.04805. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-macavaney_et_al_2021" class="csl-entry">
Macavaney, S., A. Mittu, G. Coppersmith, J. Leintz, and P. Resnik. 2021. <span>“Community-Level Research on Suicidality Prediction in a Secure Environment: Overview of the CLPsych 2021 Shared Task.”</span> In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access.
</div>
</div></section></div> ]]></description>
  <category>Machine learning</category>
  <category>Natural language processing</category>
  <category>Public policy</category>
  <category>Health and wellbeing</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/08/21/02-competition-design.html</guid>
  <pubDate>Sun, 20 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt2-intro.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Food for Thought: First place winners – Auburn Big Data</title>
  <dc:creator>Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/08/21/03-first-place-winners.html</link>
  <description><![CDATA[ 




<p>The Auburn Big Data team from Auburn University consists of five members, including three assistant professors: Dr Wenying Li of the Department of Agricultural Economics and Rural Sociology, Dr Jingyi Zheng of the Department of Mathematics and Statistics, and Dr Shubhra Kanti Karmaker of the Department of Computer Science and Software Engineering. Additionally, the team comprises two PhD students, Naman Bansal and Alex Knipper, who are affiliated with Dr Karmaker’s big data lab at Auburn University.</p>
<p>It is estimated that our team has spent approximately 1,400 hours on this project.</p>
<section id="our-perspective-on-the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="our-perspective-on-the-challenge">Our perspective on the challenge</h2>
<p>At the start of this competition, we decided to test three general approaches, in the order listed:</p>
<ol type="1">
<li><p>A heuristic approach, where we use only the data and a defined similarity metric to predict which FNDDS label a given IRI item should have.</p></li>
<li><p>A simpler modeling approach, where we train a simple statistical classifier, like a random forest <span class="citation" data-cites="10.1007/978-3-030-03146-6_86">(Parmar, Katariya, and Patel 2019)</span>, logistic regression, etc., to predict the FNDDS label for a given IRI item. For this method, we opted to use a random forest as our statistical model, as it was a simpler model to use as a baseline, having shown decent performance in a wide range of classification tasks. As it turned out, this approach was quite robust and accurate, so we kept it as our main model for this approach.</p></li>
<li><p>A large language modeling approach, where we train a model like BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(Devlin et al. 2018)</span> to map the descriptions for given IRI and FNDDS items to the FNDDS category the supplied IRI item belongs to.</p></li>
</ol>
</section>
<section id="our-approach" class="level2">
<h2 class="anchored" data-anchor-id="our-approach">Our approach</h2>
<p>As we explored the data provided, we opted to use the given 2017–2018 PPC dataset as our primary dataset for both training and testing. To ensure a fair evaluation of the model, we randomly split the dataset into 60% training samples and 40% testing samples, making sure our training process never sees the testing dataset. For evaluating our models, we adopted the competition’s metrics: Success@5 and NDCG@5. After months of testing, our statistical classifier (approach #2) proved itself to be the model that both processes the data fastest and achieves the highest performance on our testing metrics.</p>
<p>This approach, at a high level, takes in the provided data (among other configuration parameters), formats the data in a computer-readable format – converting the IRI and FNDDS descriptions to a numerical representation with word embeddings <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805 mikolov2013efficient pennington-etal-2014-glove">(2018; Mikolov et al. 2013; Pennington, Socher, and Manning 2014)</span> and then using that numerical representation to calculate the distances between each description – and then trains a classification model (random forest <span class="citation" data-cites="10.1007/978-3-030-03146-6_86">(2019)</span>/neural network <span class="citation" data-cites="SCHMIDHUBER201585">(Schmidhuber 2015)</span>) that can predict an FNDDS label for a given IRI item.</p>
<p>In terms of data, our approach uses the FNDDS/IRI descriptions, combining them into a single “description” field, and the IRI item’s categorical items – department, aisle, category, product, brand, manufacturer, and parent company – to further discern between items.</p>
<p>While most industrial methods require use of a graphics processing unit (graphics card, or GPU) to perform this kind of processing, our primary method only requires the computer’s internal processor (CPU) to function properly. With that in mind, to achieve the best possible performance on our test metrics, the most time-consuming operations are run in parallel. The time taken to train our primary model can likely be further improved if we parallelize these operations across a GPU, with the only downside being the imposition of a GPU requirement for systems aiming to run this method.</p>
<p>In addition to our primary method, our team has worked with alternate approaches on the GPU (using BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(2018)</span>, neural networks <span class="citation" data-cites="SCHMIDHUBER201585">(2015)</span>, etc.) to either: 1) speed up the time it takes to process and make inferences for the data, achieving similar performance on our test metrics, or 2) achieve higher performance, likely at a cost to the time it takes to process everything. Our reasoning behind doing so is that if a simple statistical model performs well, then a larger language model should be able to demonstrate a higher performance on our test metrics without much of an increase in training time. At the current time, these methods are still unable to match the performance/efficiency tradeoff of our primary method.</p>
<p>After exploring alternate methods to no avail, our team then decided to focus again on our primary method, the random forest <span class="citation" data-cites="10.1007/978-3-030-03146-6_86">(2019)</span>, and a secondary method, feed-forward neural network mapping our input features (X) to the FNDDS labels (Y) <span class="citation" data-cites="SCHMIDHUBER201585">(2015)</span>, to optimize their training hyperparameters for the dataset. Our aim in this is to see which of our already-implemented, easier-to-run downstream methods would better optimize the performance/efficiency tradeoff after having its training parameters optimized to the fullest. This has resulted in a marginal increase in training time (+20-30 minutes) and a roughly 5% increase in performance for our still-highest performing model, the random forest.</p>
<p>Overall, our primary method – the random forest – gave us an approximate training time (including data pre-processing) of 4 hours 30 minutes for our ~38,000 IRI item training set, and an approximate inference time of 15 minutes on our testing set of ~15,000 IRI items. Furthermore, our method gave us a Success@5 score of .789 and an NDCG@5 score of .705 on our testing set.</p>
<section id="key-features" class="level5">
<h5 class="anchored" data-anchor-id="key-features">Key features</h5>
<p>Here is a list of the key features we utilize, along with what type of data we treat it as.</p>
<ul>
<li>FNDDS
<ul>
<li>food_code – identifier</li>
<li>main_food_description – text</li>
<li>additional_food_description – text</li>
<li>ingredient_description – text</li>
</ul></li>
<li>IRI
<ul>
<li>upc – identifier</li>
<li>upcdesc – text</li>
<li>dept – categorical</li>
<li>aisle – categorical</li>
<li>category – categorical</li>
<li>product – categorical</li>
<li>brand – categorical</li>
<li>manufacturer – categorical</li>
<li>parent – categorical</li>
</ul></li>
</ul>
<p>The intuition behind using these particular features is that the text-based descriptions provide the majority of the “meaning” of the item. By converting each description to a numerical representation <span class="citation" data-cites="mikolov2013efficient pennington-etal-2014-glove">(2013; 2014)</span>, we can then calculate the similarity between each “meaning” to determine which FNDDS label is most similar to the IRI item provided. However, that alone is not enough. The categorical features on the IRI item help to further enhance the model’s classifications using the logic and categories people use in places like grocery stores. For example, if given an item whose aisle was “fruit” and brand was “Dole”, the item could be reasonably expected to be something like “peaches” over something like “broccoli”.</p>
</section>
<section id="feature-selection" class="level5">
<h5 class="anchored" data-anchor-id="feature-selection">Feature selection</h5>
<p>Aforementioned intuition aside, our feature selection was rather naive, in that we manually examined the data and removed any redundant text features before doing anything else. After that, we decided to use description fields as “text” data to comprise the main “meaning” of the item, represented numerically after converting the text using a word embedding <span class="citation" data-cites="mikolov2013efficient pennington-etal-2014-glove">(2013; 2014)</span>. We also decided to use the non-description fields (aisle, category, etc.) as “categorical” data that would be turned into its own numerical representation, allowing our model to more easily discern between items using similar systems to people.</p>
</section>
<section id="feature-transformations" class="level5">
<h5 class="anchored" data-anchor-id="feature-transformations">Feature transformations</h5>
<p>Our feature transformations are also relatively simple. First, we combine all description fields for each item to make one large description, and then use a word embedding method (like GloVe <span class="citation" data-cites="pennington-etal-2014-glove">(2014)</span> or BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(2018)</span>) to convert the description into a numerical representation, resulting in a 300-dimensional GloVe or 768-dimensional BERT vector of numbers for each description. Then, for each IRI item, we calculate the cosine and Euclidean distances from each FNDDS item, resulting in two vectors, both equal in length to the original FNDDS data (in this case, two vectors of length ~7,300). The intuition behind this is that while cosine and Euclidean distances can tell us similar things, providing both of these sets of distances to the model should allow it to pick up on a more nuanced set of relationships between the IRI and FNDDS items.</p>
<p>For categorical data, we take all unique values in each field and assign them an ID number. While that is often not the best practice for making a numerical representation out of categorical data <span class="citation" data-cites="10.5120/ijca2017915495">(Potdar, Pardawala, and Pai 2017)</span>, it seemed to work for the downstream model.</p>
<p>Regardless, the aforementioned feature transformations give us (ad hoc) ~14,900 features if we use GloVe and ~15,300 features if we use BERT. Both feature sets can then be sent to the downstream random forest/neural network to start classifying items.</p>
<p>It should be noted that processing the data is by far the most time-consuming part of our method. The data processing times for each embedding are as follows:</p>
<ul>
<li>GloVe: ~3 hours</li>
<li>BERT: ~6 hours</li>
</ul>
<p>Due to BERT both taking so long to process data and performing lower than our GloVe embeddings on the classification task, we opt to use GloVe embeddings for our primary method. Our only theoretical explanation here is that since BERT is better at context-dependent tasks <span class="citation" data-cites="10.1145/3443279.3443304">(Wang, Nulty, and Lillis 2021)</span>, it likely will expect something similar to well-structured sentences as input, which is not what the IRI/FNDDS descriptions are. Rather, GloVe – being a method that depends less on context <span class="citation" data-cites="mikolov2013efficient pennington-etal-2014-glove">(2013; 2014)</span> – should excel better when the input text is not a well-formed sentence.</p>
</section>
<section id="training-methods" class="level5">
<h5 class="anchored" data-anchor-id="training-methods">Training methods</h5>
<p>Once the data has been processed, we collect the following data for each IRI item:</p>
<ul>
<li>UPC code</li>
<li>Description (converted to numerical representation)</li>
<li>Categorical variables (converted to numerical representation)</li>
<li>Distances to each FNDDS item</li>
</ul>
<p>Once that has been collected for each IRI item, we can finally use our classification model. We initialize our model and begin the training process with the IRI data mentioned above and the target FNDDS labels for each one, so the model knows what the “correct” answer is for the given data. Once the model has trained on our training dataset, we save the model and it is ready for use.</p>
<p>This part of training takes much less time than preparing the data, since calculating the embeddings takes a lot more computation than a random forest model. The training times for each method are as follows:</p>
<ul>
<li>Random Forest: ~1 hour 15 minutes</li>
<li>Neural Network: ~25 minutes</li>
</ul>
<p>Despite the neural network taking far less time to train than the random forest, it still scores lower on the scoring metrics than the random forest, so we opt to continue using the random forest model as our primary method.</p>
</section>
<section id="general-approach-to-developing-the-model" class="level5">
<h5 class="anchored" data-anchor-id="general-approach-to-developing-the-model">General approach to developing the model</h5>
<p>Since the linkage problem involves mapping tens of thousands of items to a smaller category set of a few thousand items, we decided to frame this problem as a multi-class classification problem <span class="citation" data-cites="aly2005survey">(Aly 2005)</span>, where we then rank the top “k” most probable class mappings, as requested by the competition ruleset.</p>
<p>Most of the usable data available to us is text data, so we need a method that can use that text-based information to accurately map classes based on the aforementioned text information. To best accomplish this, we opt to use word embedding techniques to calculate an average numerical representation for each text description (both IRI and FNDDS), so we can calculate distances between each description, giving our model a sense of how similar each description is.</p>
</section>
<section id="the-key-trick-to-the-model" class="level5">
<h5 class="anchored" data-anchor-id="the-key-trick-to-the-model">The key “trick” to the model</h5>
<p>Since text descriptions hold the most information that can be used to link between an IRI item and an FNDDS item, finding a way to calculate the similarity between each description is paramount to making this method work.</p>
<p>Both distance calculation methods used in this work, cosine and Euclidean distance, are very similar in the type of information encoded, the only major difference being that cosine distance is implicitly normalized and Euclidean distance is not <span class="citation" data-cites="10.1145/967900.968151">(Qian et al. 2004)</span>.</p>
</section>
<section id="notable-observations" class="level5">
<h5 class="anchored" data-anchor-id="notable-observations">Notable observations</h5>
<p>Just by building the ranking using the cosine similarities between each IRI item and all FNDDS items, we can achieve a Success@5 performance of 0.234 and an NDCG@5 performance of 0.312. The other features are provided and the random forest classifier is used to add some extra discriminative power to the model.</p>
</section>
<section id="data-disclaimer" class="level5">
<h5 class="anchored" data-anchor-id="data-disclaimer">Data disclaimer</h5>
<p>Our current method only uses the data readily available from the 2017–2018 dataset, which we acknowledge is intended for testing. To remedy this, we further split this dataset into train/test sets and report results on our unseen test subset for our primary performance metrics. This gives a decent look into how the model will perform on unseen data.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Find the code in the <a href="https://github.com/realworlddatascience/realworlddatascience.github.io/tree/main/case-studies/posts/2023/08/21/_code">Real World Data Science GitHub repository</a>.</p>
</div>
</div>
</div>
</section>
</section>
<section id="our-results" class="level2">
<h2 class="anchored" data-anchor-id="our-results">Our results</h2>
<section id="approximate-training-time" class="level5">
<h5 class="anchored" data-anchor-id="approximate-training-time">Approximate training time</h5>
<p>Overall, our approximate training time for our primary method is 4 hours 30 minutes broken down (approximately) as follows:</p>
<ol type="1">
<li>Reading data from database: 30 seconds</li>
<li>Calculating ~7,300 FNDDS description embeddings: 15 minutes 45 seconds</li>
<li>Calculating ~38,000 IRI description embeddings and similarity scores: 2 hours 20 minutes 45 seconds</li>
<li>Formatting calculated data for the random forest classifier: 35 minutes</li>
<li>Training the random forest classifier: 1 hour 15 minutes</li>
</ol>
</section>
<section id="approximate-inference-time" class="level5">
<h5 class="anchored" data-anchor-id="approximate-inference-time">Approximate inference time</h5>
<p>Our approximate inference time for our primary method is 15 minutes to make inferences for ~15,000 IRI items.</p>
</section>
<section id="s5-ndcg5-performance" class="level5">
<h5 class="anchored" data-anchor-id="s5-ndcg5-performance">S@5 &amp; NDCG@5 performance</h5>
<p>This is how our best-performing model (GloVe + random forest) performs at the current time on the testing set:</p>
<ul>
<li>NDCG@5: 0.705</li>
<li>Success@5: 0.789</li>
</ul>
<p>When we evaluate that same model on the full PPC dataset we were provided (~38,000 items), we get the following scores:</p>
<ul>
<li>NDCG@5: 0.879</li>
<li>Success@5: 0.916</li>
</ul>
<p>(Note: The full PPC dataset contains approximately 15,000 items that we used to train the model, so these scores are not as representative of our method’s performance as the previous scores.)</p>
</section>
</section>
<section id="future-workrefinement" class="level2">
<h2 class="anchored" data-anchor-id="future-workrefinement">Future work/refinement</h2>
<p>As mentioned previously, we only used the given 2017–2018 PPC dataset as our primary dataset for both training and testing. Going forward, we would like to include datasets from previous years as well, which we believe would further increase our model performance. Additionally, the datasets generated from this research have the potential to inform and support additional studies from a variety of perspectives, including nutrition, consumer research, and public health. Further research utilizing these datasets has the potential to make significant contributions to our understanding of consumer behavior and the role of food and nutrient consumption in overall health and well-being.</p>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons learned</h2>
<p>It was interesting that the random forest model performed better than the vanilla neural network model. This shows that a simple solution can work better, depending on the application. This observation is in line with the well-established principle in machine learning that the choice of model should be guided by the nature of the problem and the characteristics of the data. In this case, the random forest model, being a simpler and more interpretable model, was better suited to the problem at hand and was able to outperform the more complex neural network model. These results underscore the importance of careful model selection and the need to consider both the complexity of the model and the specific requirements of the problem when choosing an algorithm for a particular application.</p>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/02-competition-design.html">← Part 2: Competition design</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/04-second-place-winners.html">Part 4: Second place winners →</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Alex Knipper</strong> and <strong>Naman Bansal</strong> are PhD students, and <strong>Jingyi Zheng</strong>, <strong>Wenying Li</strong>, and <strong>Shubhra Kanti Karmaker</strong> are assistant professors at Auburn University.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@nicotitto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">nrd</a> on <a href="https://unsplash.com/photos/D6Tu_L3chLE?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Knipper, Alex, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker. 2023. “Food for Thought: First place winners – Auburn Big Data.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/03-first-place-winners.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-aly2005survey" class="csl-entry">
Aly, M. 2005. <span>“Survey on Multiclass Classification Methods, Tech. Rep.”</span> <em>California Institute of Technology</em>.
</div>
<div id="ref-DBLP:journals/corr/abs-1810-04805" class="csl-entry">
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2018. <span>“<span>BERT:</span> Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>CoRR</em> abs/1810.04805. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-10.1007/978-3-030-03146-6_86" class="csl-entry">
Parmar, A., R. Katariya, and V. Patel. 2019. <span>“A Review on Random Forest: An Ensemble Classifier.”</span> In <em>International Conference on Intelligent Data Communication Technologies and Internet of Things (ICICI) 2018</em>, edited by J. Hemanth, X. Fernando, P. Lafata, and Z. Baig, 758–63. Cham: Springer International Publishing.
</div>
<div id="ref-pennington-etal-2014-glove" class="csl-entry">
Pennington, J., R. Socher, and C. Manning. 2014. <span>“<span>G</span>lo<span>V</span>e: Global Vectors for Word Representation.”</span> In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (<span>EMNLP</span>)</em>, 1532–43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-10.5120/ijca2017915495" class="csl-entry">
Potdar, K., T. S. Pardawala, and C. D. Pai. 2017. <span>“A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers.”</span> <em>International Journal of Computer Applications</em> 175 (4): 7–9. <a href="https://doi.org/10.5120/ijca2017915495">https://doi.org/10.5120/ijca2017915495</a>.
</div>
<div id="ref-10.1145/967900.968151" class="csl-entry">
Qian, G., S. Sural, Y. Gu, and S. Pramanik. 2004. <span>“Similarity Between Euclidean and Cosine Angle Distance for Nearest Neighbor Queries.”</span> In <em>Proceedings of the 2004 ACM Symposium on Applied Computing</em>, 1232–37. SAC ’04. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/967900.968151">https://doi.org/10.1145/967900.968151</a>.
</div>
<div id="ref-SCHMIDHUBER201585" class="csl-entry">
Schmidhuber, J. 2015. <span>“Deep Learning in Neural Networks: An Overview.”</span> <em>Neural Networks</em> 61: 85–117. https://doi.org/<a href="https://doi.org/10.1016/j.neunet.2014.09.003">https://doi.org/10.1016/j.neunet.2014.09.003</a>.
</div>
<div id="ref-10.1145/3443279.3443304" class="csl-entry">
Wang, C., P. Nulty, and D. Lillis. 2021. <span>“A Comparative Study on Word Embeddings in Deep Learning for Text Classification.”</span> In <em>Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval</em>, 37–46. NLPIR ’20. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3443279.3443304">https://doi.org/10.1145/3443279.3443304</a>.
</div>
</div></section></div> ]]></description>
  <category>Machine learning</category>
  <category>Natural language processing</category>
  <category>Public policy</category>
  <category>Health and wellbeing</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/08/21/03-first-place-winners.html</guid>
  <pubDate>Sun, 20 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/03-auburn.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Food for Thought: Second place winners – DeepFFTLink</title>
  <dc:creator>Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/08/21/04-second-place-winners.html</link>
  <description><![CDATA[ 




<p>DeepFFTLink team members: Yang Wu and Kai Zhang are PhD students at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student at Indiana University Bloomington. Xuhong Zhang is an assistant professor at Indiana University Bloomington. Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute.</p>
<section id="perspective-on-the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="perspective-on-the-challenge">Perspective on the challenge</h2>
<p>Text matching is an essential task in natural language processing <span class="citation" data-cites="DBLP:journals/corr/PangLGXWC16">(NLP, Pang et al. 2016)</span>, while record linkage across different sources is an essential task in data science. Machine learning techniques allow people to combine data faster and cheaper than using manual linkage. However, in the context of the Food for Thought challenge, existing methods for matching universal product codes (UPCs) to ensemble codes (ECs) require every UPC to be compared with every EC code (Figure 1a). Such approaches can be computationally expensive in the training process when data is noisy. Here, we propose an ensemble model with a category-based adapter to tackle this problem, drawing on the category information included in UPC and EC data. The category-based adapter allows UPCs to be first matched with only a small and reliable set of ECs (Figure 1b). Then, an ensemble model will be deployed to make predictions for UPC-EC matching. Our proposed approach can achieve competitive performance compared with state-of-the-art models.</p>
<div class="quarto-layout-panel" style="padding-top: 1em; margin-bottom: 0;">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-figure quarto-figure-center" style="flex-basis: 50.0%;justify-content: center;">
<figure class="figure">
<p><a href="images/pt4-fig1a.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt4-fig1a.png" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">(a)</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center" style="flex-basis: 50.0%;justify-content: center;">
<figure class="figure">
<p><a href="images/pt4-fig1b.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt4-fig1b.png" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">(b)</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="figure-caption">
<p><strong>Figure 1:</strong> A toy example of our method. Panel (a) shows the traditional matching method, while (b) is our proposed ensemble model with category-based adapter. With the help of the adapter, UPC 1 only needs to be matched with EC 1 and EC 3.</p>
</div>
</section>
<section id="our-approach" class="level2">
<h2 class="anchored" data-anchor-id="our-approach">Our approach</h2>
<p>We propose a two-step framework to address this problem. To begin with, we use a category-based adapter to get reliable candidate ECs for each UPC. Then, an ensemble model <span class="citation" data-cites="10.1007/3-540-45014-9_1">(Dietterich 2000)</span> is deployed to make a prediction for each UPC-EC pair.</p>
<section id="category-based-adapter" class="level5">
<h5 class="anchored" data-anchor-id="category-based-adapter">Category-based adapter</h5>
<p>By using 2015–2016 UPC-EC data, we created a knowledge base, which is a UPC category–EC pair-wised table for generating candidate ECs. Within this setting, each UPC category is, on average, related to only 32 ECs. This knowledge base is then used as context to further filter the candidate ECs. Note that there are some new ECs generated year by year, which can also be part of the potential ECs in the UPC-EC matching task, since the contextual information of new ECs does not exist in our knowledge base.</p>
</section>
<section id="ensembled-model" class="level5">
<h5 class="anchored" data-anchor-id="ensembled-model">Ensembled model</h5>
<p>We ensemble the base-string match and BERT models. BERT is a deep learning model for natural language processing <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(Devlin et al. 2018)</span>. In the base-string match model, we used the Term Frequency-Inverse Document Frequency (TFIDF) of each UPC and EC description as features to calculate a pairwise cosine similarity, which is a distance between instances. Meanwhile, we used features extracted from UPC and EC descriptions to fine-tune the BERT base model and calculated the cosine similarity of embeddings between each UPC and EC. Then we rank ECs based on their similarity scores with the UPC.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/pt4-fig2.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt4-fig2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="figure-caption">
<p><strong>Figure 2:</strong> The framework of our proposed model. A two-step strategy is used to make the final prediction.</p>
</div>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container" style="margin-top: 2rem;">
<p>Find the code in the <a href="https://github.com/realworlddatascience/realworlddatascience.github.io/tree/main/case-studies/posts/2023/08/21/_code">Real World Data Science GitHub repository</a>.</p>
</div>
</div>
</div>
</section>
</section>
<section id="our-results" class="level2">
<h2 class="anchored" data-anchor-id="our-results">Our results</h2>
<p>We randomly selected 500 samples from the 2017–2018 UPC-EC data to train the ensembled weight for each model. Two functions were adapted to make a fusion of base-string and BERT models:</p>
<p><span id="eq-first"><img src="https://latex.codecogs.com/png.latex?%0AC%20=%20a%20*%20X%20+%20b%20*%20Y%20%20%0A%5Ctag%7B1%7D"></span></p>
<p><span id="eq-second"><img src="https://latex.codecogs.com/png.latex?%0AC%20=%20%20a%20*%20log(X)%20+%20b%20*%20log(Y)%20%5Ctext%7B.%20%7D%0A%5Ctag%7B2%7D"></span></p>
<p><img src="https://latex.codecogs.com/png.latex?C"> denotes the final confidence score. <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> represent <em>base_string_similarity_score</em> and <em>BERT_similarity_score</em>, respectively. <img src="https://latex.codecogs.com/png.latex?a"> and <img src="https://latex.codecogs.com/png.latex?b"> are corresponding model weights for base_string and BERT models.</p>
<p>A better Success@5 is achieved with function (1). The ensembled weights for the base-string model and BERT model are 0.738 and 0.262, respectively. The experiment result indicates that the base_string model contributes more than the BERT model when the ensemble model makes predictions. The prediction result for the 2017–2018 data is:</p>
<ul>
<li>Success@5: 0.727</li>
<li>NDCG@5: 0.528</li>
</ul>
<p>Computation time is 6 hours.</p>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future work</h2>
<p>Our next step will focus on adding the newly generated EC data into our knowledge base, which allows the model to be more stable to make predictions for UPC-EC matching. Our model is an unsupervised method, which does not need labels for each instance. We use cosine similarity to rank the matches, so no labels are needed in the training process. However, our future work will try to label some instances to handle the UPC-EC matching task in a supervised manner.</p>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons learned</h2>
<ol type="1">
<li><p><strong>If the data is not complex, simple models may outperform complex models.</strong> For example, in our experiment, we found that the base-string model outperforms single RoBERTa <span class="citation" data-cites="DBLP:journals/corr/abs-1907-11692">(Liu et al. 2019)</span> or BERT models. However, our ensemble model can outperform each individual model since model fusion allows information aggregation from multiple models.</p></li>
<li><p><strong>Multi-label models may not work well on UPC-EC data.</strong> In our early work, we tried to consider the UPC-EC matching task as a multi-label problem, e.g., we labeled each EC as a binary label which indicated whether the EC was an appropriate match or not. We mapped UPC and EC pairs into a multi-label table. However, we find that the UPC and EC keeps a one-to-one relation for most UPCs. The model performance of a multi-label model, i.e., Label-Specific Attention Network <span class="citation" data-cites="xiao-etal-2019-label">(LSAN, Xiao et al. 2019)</span>, is lower than base-string model on both Success@5 and NDCG@5 metrics.</p></li>
</ol>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/03-first-place-winners.html">← Part 3: First place winners</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/05-third-place-winners.html">Part 5: Third place winners →</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Yang Wu</strong> and <strong>Kai Zhang</strong> are PhD students, and <strong>Xiaozhong Liu</strong> is an associate professor at Worcester Polytechnic Institute. <strong>Aishwarya Budhkar</strong> is a PhD student and <strong>Xuhong Zhang</strong> is an assistant professor at Indiana University Bloomington.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@hansonluu?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Hanson Lu</a> on <a href="https://unsplash.com/photos/sq5P00L7lXc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Wu, Yang, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu. 2023. “Food for Thought: Second place winners – DeepFFTLink.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/04-second-place-winners.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DBLP:journals/corr/abs-1810-04805" class="csl-entry">
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2018. <span>“<span>BERT:</span> Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>CoRR</em> abs/1810.04805. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-10.1007/3-540-45014-9_1" class="csl-entry">
Dietterich, T. G. 2000. <span>“Ensemble Methods in Machine Learning.”</span> In <em>Multiple Classifier Systems</em>, 1–15. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-DBLP:journals/corr/abs-1907-11692" class="csl-entry">
Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. 2019. <span>“RoBERTa: <span>A</span> Robustly Optimized <span>BERT</span> Pretraining Approach.”</span> <em>CoRR</em> abs/1907.11692. <a href="http://arxiv.org/abs/1907.11692">http://arxiv.org/abs/1907.11692</a>.
</div>
<div id="ref-DBLP:journals/corr/PangLGXWC16" class="csl-entry">
Pang, L., Y. Lan, J. Guo, J. Xu, S. Wan, and X. Cheng. 2016. <span>“Text Matching as Image Recognition.”</span> <em>CoRR</em> abs/1602.06359. <a href="http://arxiv.org/abs/1602.06359">http://arxiv.org/abs/1602.06359</a>.
</div>
<div id="ref-xiao-etal-2019-label" class="csl-entry">
Xiao, L., X. Huang, B. Chen, and L. Jing. 2019. <span>“Label-Specific Document Representation for Multi-Label Text Classification.”</span> In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 466–75. Hong Kong, China: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D19-1044">https://doi.org/10.18653/v1/D19-1044</a>.
</div>
</div></section></div> ]]></description>
  <category>Machine learning</category>
  <category>Natural language processing</category>
  <category>Public policy</category>
  <category>Health and wellbeing</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/08/21/04-second-place-winners.html</guid>
  <pubDate>Sun, 20 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/04-deepfftlink.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Food for Thought: Third place winners – Loyola Marymount</title>
  <dc:creator>Yifan Hu and Mandy Korpusik</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/08/21/05-third-place-winners.html</link>
  <description><![CDATA[ 




<p>Undergraduate student Yifan (Rosetta) Hu was responsible for writing the Python script that pre-processes the 2015–2016 UPC, EC, and PPC data for training neural network models. Her script randomly sampled five negative EC descriptions for every positive match between a UPC and EC code. Professor Mandy Korpusik performed the remaining work, including setting up the environment, training the BERT model, and evaluation. Hu spent roughly 10 hours on the competition, and Korpusik spent roughly 40 hours of work (and many additional hours running and monitoring the training and testing scripts).</p>
<section id="our-perspective-on-the-challenge" class="level2">
<h2 class="anchored" data-anchor-id="our-perspective-on-the-challenge">Our perspective on the challenge</h2>
<p>The goal of this challenge is to use machine learning and natural language processing (NLP) to link language-based entries in the IRI and FNDDS databases. Our proposed approach is based on our prior work using deep learning models to map users’ natural language meal descriptions to the FNDDS database <span class="citation" data-cites="7953245">(Korpusik, Collins, and Glass 2017b)</span> to retrieve nutrition information in a spoken diet tracking system. In the past, we found a trade-off between accuracy and cost, leading us to select convolutional neural networks over recurrent long short-term memory (LSTM) networks – with nearly 10x as many parameters and 2x the training time required, LSTMs achieved slightly lower performance on semantic tagging and food database mapping on meals in the breakfast category. Here, we propose to investigate state-of-the-art transformers, specifically the contextual embedding model (i.e., the entire sentence is used as context to generate the embedding) known as BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(Bidirectional Encoder Representations from Transformers, Devlin et al. 2018)</span>.</p>
<section id="related-work" class="level5">
<h5 class="anchored" data-anchor-id="related-work">Related work</h5>
<p>Within the past few years, several papers have come out that learn contextual representations of sentences, where the entire sentence is used to generate embeddings.</p>
<p>ELMo <span class="citation" data-cites="DBLP:journals/corr/abs-1802-05365">(Peters et al. 2018)</span> uses a linear combination of vectors extracted from intermediate layer representations of a bidirectional LSTM trained on a large text corpus as a language model; in this feature-based approach, the ELMo vector of the full input sentence is concatenated with the standard context-independent token representations and passed through a task-dependent model for final prediction. This showed performance improvement over state-of-the-art on six NLP tasks, including question answering, textual entailment, and sentiment analysis.</p>
<p>OpenAI GPT <span class="citation" data-cites="radford2018improving">(Radford et al. 2018)</span> is a fine-tuning approach, where they first pre-train a multi-layer transformer <span class="citation" data-cites="NIPS2017_3f5ee243">(Vaswani et al. 2017)</span> as a language model on a large text corpus, and then conduct supervised fine-tuning on the specific task of interest, with a linear softmax layer on top of the pre-trained transformer.</p>
<p>Google’s BERT <span class="citation" data-cites="DBLP:journals/corr/abs-1810-04805">(2018)</span> is a fine-tuning approach similar to GPT, but with the key difference that instead of combining separately trained forward and backward transformers, they instead use a masked language model for pre-training, where they randomly masked out input tokens and predicted only those tokens. They demonstrated state-of-the-art performance on 11 NLP tasks, including the CoNLL 2003 named entity recognition task, which is similar to our semantic tagging task.</p>
<p>Finally, many models have recently been developed that improve upon BERT, including RoBERTa <span class="citation" data-cites="DBLP:journals/corr/abs-1907-11692">(which improves BERT’s pre-training by using bigger batches and more data, Y. Liu et al. 2019)</span>, XLNet <span class="citation" data-cites="NEURIPS2019_dc6a7e65">(which uses Transformer-XL and avoids BERT’s pretrain-finetune discrepancy through learning a truly bidirectional context via permutations over the factorization order, Yang et al. 2019)</span>, and ALBERT <span class="citation" data-cites="DBLP:journals/corr/abs-1909-11942">(a lightweight BERT, Lan et al. 2019)</span>.</p>
<p>In our prior work on language understanding for nutrition <span class="citation" data-cites="7078635 7472843 7902155 korpusik17_interspeech 8461769 8721137">(Korpusik et al. 2014, 2016; Korpusik and Glass 2017, 2018, 2019; Korpusik, Collins, and Glass 2017a)</span>, we used a similar binary classification approach for learning embeddings, which were then used at test time to map from user-described meals to USDA food database matches, but with convolutional neural networks (CNNs) instead of BERT. (BERT was not created until 2018, and due to limited memory available for deployment, we needed a smaller model than even BERT base, which has 100 million parameters.) Further work demonstrated that BERT outperformed CNNs on several language understanding tasks, including nutrition <span class="citation" data-cites="korpusik19_interspeech">(Korpusik, Liu, and Glass 2019)</span>.</p>
</section>
</section>
<section id="our-approach" class="level2">
<h2 class="anchored" data-anchor-id="our-approach">Our approach</h2>
<p>Our approach is to fine-tune a large pre-trained BERT language model on the food data. BERT was originally trained on a massive amount of text for a language modelling task (i.e., predicting which word should come next in a sentence). It relies on a transformer model, which uses an “attention” mechanism to identify which words the model should pay the most “attention” to. We are specifically using BERT for binary sequence classification, which refers to predicting a label (i.e., classification) for a sequence of words. In our case, during fine-tuning (i.e., training the model further on our own dataset) we will feed the model pairs of sentences (where one sentence is the UPC description of a food item and the other is the EC description of another food item), and the model will perform binary classification, predicting whether the sentences are a match (i.e., 1) or not (i.e., 0). We start with the 2015–2016 ground truth PPC data for positive examples, and five randomly sampled negative examples per positive example.</p>
<section id="training-methods" class="level5">
<h5 class="anchored" data-anchor-id="training-methods">Training methods</h5>
<p>Since we used a neural network model, the only features passed into our model were the tokenized words themselves of the EC and UPC food descriptions – we did not conduct any manual feature engineering <span class="citation" data-cites="dong_liu">(Dong and Liu 2018)</span>. The model was trained on a 90/10 split into 90% training and 10% validation data, where the validation data was used as a test set to fine-tune the model’s hyperparameters. We started with a randomly sampled set of 16,000 pairs, batch size of 16 (i.e., the model would train on batches of 16 samples at a time), AdamW <span class="citation" data-cites="DBLP:journals/corr/abs-1711-05101">(Loshchilov and Hutter 2017)</span> as the optimizer (which adaptively updates the learning rate, or how large the update should be to the model’s parameters), a linear schedule with warmup <span class="citation" data-cites="DBLP:journals/corr/abs-1908-03265">(i.e., starting with a small learning rate in the first few epochs of training due to large variance in early stages of training, L. Liu et al. 2019)</span>, and one epoch (i.e., the number of times the model passes through all the training data). We then added the next randomly sampled set of 16,000 pairs to get a model trained on 32,000 data points. Finally, we reached a total of 48,000 data samples used for training. Each pair of sequences was tokenized with the pre-trained BERT tokenizer, with the special CLS and SEP tokens (where CLS is a learned vector that is typically passed to downstream layers for final classification, and SEP is a learned vector that separates two input sequences), and was padded with zeros to the maximum length input sequence of 240 tokens, so that each input sequence would be the same length.</p>
</section>
<section id="model-development-approach" class="level5">
<h5 class="anchored" data-anchor-id="model-development-approach">Model development approach</h5>
<p>We faced many challenges due to the secure nature of the ADRF environment. Since our approach relies on BERT, we were blocked by errors due to the local BERT installation. Typically, BERT is downloaded from the web as the program runs. However, for this challenge, BERT must be installed locally for security reasons. To fix the errors, the BERT models needed to be installed with <code>git lfs clone</code> instead of git.</p>
<p>Second, we were unable to retrieve the test data from the database due to SQLAlchemy errors. We found a workaround by using DBeaver directly to save database tables as Excel spreadsheets, rather than accessing the database tables through Python.</p>
<p>Finally, we needed a GPU in order to efficiently train our BERT models. However, we initially only had a CPU, so there was a delay due to setting up the GPU configuration. Once the GPU image was set up, there was still a CUDA error when running the BERT model during training. We determined that the model was too big to fit into GPU memory, so we found a workaround using gradient checkpointing (trading off computation speed for memory) with the transformers library’s Trainer and TrainingArguments. Unfortunately, the version of transformers we were using did not have these tools, and the library was not updated until less than a week before the deadline, so we still had to train the model on the CPU.</p>
<p>To deal with the inability to run jobs in the background, our process was checkpointing our models every five batches, and saving the model predictions during evaluation to a csv file every five batches as well.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Find the code in the <a href="https://github.com/realworlddatascience/realworlddatascience.github.io/tree/main/case-studies/posts/2023/08/21/_code">Real World Data Science GitHub repository</a>.</p>
</div>
</div>
</div>
</section>
</section>
<section id="our-results" class="level2">
<h2 class="anchored" data-anchor-id="our-results">Our results</h2>
<p>After training, the 48K model (so-called because it was trained on 48,000 data samples) was used at test time via ranking all possible 2017–18 EC descriptions given an unseen UPC description. The rankings were obtained through the model’s output value – the higher the output (or confidence), the more highly we ranked that EC description. To speed up the ranking process, we used blocking (i.e., only ranking a subset of all possible matches), specifically with exact word matches (using only the first six words in the UPC description, which appeared to be the most important), and fed all possible matches through the model in one batch per UPC description. Since we still did not have sufficient time to complete evaluation on the full set of test UPC descriptions, we implemented an expedited evaluation that only considered the first 10 matching EC descriptions in the BERT ranking process (which we call BERT-FAST). We also report results for the slower evaluation method that considers all EC descriptions that match at least one of the first six words in a given UPC description, but note that these results are based on just a small subset of the total test set. See Table 1 below for our results, where the <span class="citation" data-cites="5">(<strong>5?</strong>)</span> indicates how often the correct match was ranked among the top-5. See Table 2 for an estimate of how long it takes to train and test the model on a CPU.<br>
<br>
<br>
</p>
<div class="figure-caption">
<p><strong>Table 1:</strong> S@5 and NCDG@5 for BERT, both for fast evaluation over the whole test set, and slower evaluation on a smaller subset (711 UPCs out of 37,693 total).</p>
</div>
<table class="table">
<thead>
<tr class="header">
<th>Model</th>
<th style="text-align: center;">Success@5</th>
<th style="text-align: center;">NDCG@5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-FAST</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">0.047</td>
</tr>
<tr class="even">
<td>BERT-SLOW</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.412</td>
</tr>
</tbody>
</table>
<p><br>
</p>
<div class="figure-caption">
<p><strong>Table 2:</strong> An estimate of the time required to train and test the model.</p>
</div>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training (on 48K samples)</td>
<td>16 hours</td>
</tr>
<tr class="even">
<td>Testing (BERT-FAST)</td>
<td>52 hours</td>
</tr>
<tr class="odd">
<td>Testing (BERT-SLOW)</td>
<td>63 days</td>
</tr>
</tbody>
</table>
<p><br>
</p>
</section>
<section id="future-workrefinement" class="level2">
<h2 class="anchored" data-anchor-id="future-workrefinement">Future work/refinement</h2>
<p>In the future, with more time available, we would train on all data, not just our limited dataset of 48,000 pairs, as well as perform evaluation on the held-out test set with the full set of possible EC matches that have one or more words in common with the UPC description. We would compare against baseline word embedding methods such as word2vec <span class="citation" data-cites="DBLP:journals/corr/abs-1712-09405">(Mikolov et al. 2017)</span> and Glove <span class="citation" data-cites="pennington-etal-2014-glove">(Pennington, Socher, and Manning 2014)</span>, and we would explore hierarchical prediction methods for improving efficiency and accuracy. Specifically, we would first train a classifier to predict the generic food category, and then train finer-grained models to predict specific foods within a general food category. Finally, we are exploring multi-modal transformer-based approaches that allow two input modalities (i.e., food images and text descriptions of a meal) for predicting the best UPC match.</p>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons learned</h2>
<p>We recommend that future challenges provide every team with both a CPU and a GPU in their workspace, to avoid transitioning from one to the other midway through the challenge. In addition, if possible, it would be very helpful to provide a mechanism for running jobs in the background. Finally, it may be useful for teams to submit snippets of code along with library package names, in order for the installations to be tested properly beforehand.</p>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/04-second-place-winners.html">← Part 4: Second place winners</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/06-value-of-competitions.html">Part 6: The value of competitions →</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Yifan (Rosetta) Hu</strong> is an undergraduate student and <strong>Mandy Korpusik</strong> is an assistant professor of computer science at Loyola Marymount University’s Seaver College of Science and Engineering.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Yifan Hu and Mandy Korpusik
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@pvsbond?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Peter Bond</a> on <a href="https://unsplash.com/photos/KfvknMhkmw0?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Hu, Yifan, and Mandy Korpusik. 2023. “Food for Thought: Third place winners – Loyola Marymount.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/05-third-place-winners.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DBLP:journals/corr/abs-1810-04805" class="csl-entry">
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2018. <span>“<span>BERT:</span> Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>CoRR</em> abs/1810.04805. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-dong_liu" class="csl-entry">
Dong, G., and H. Liu, eds. 2018. <em>Feature Engineering for Machine Learning and Data Analytics</em>. First edition. CRC Press.
</div>
<div id="ref-korpusik17_interspeech" class="csl-entry">
Korpusik, M., Z. Collins, and J. Glass. 2017a. <span>“<span class="nocase">Character-Based Embedding Models and Reranking Strategies for Understanding Natural Language Meal Descriptions</span>.”</span> In <em>Proceedings of Interspeech</em>, 3320–24. <a href="https://doi.org/10.21437/Interspeech.2017-422">https://doi.org/10.21437/Interspeech.2017-422</a>.
</div>
<div id="ref-7953245" class="csl-entry">
———. 2017b. <span>“Semantic Mapping of Natural Language Input to Database Entries via Convolutional Neural Networks.”</span> In <em>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 5685–89. <a href="https://doi.org/10.1109/ICASSP.2017.7953245">https://doi.org/10.1109/ICASSP.2017.7953245</a>.
</div>
<div id="ref-7902155" class="csl-entry">
Korpusik, M., and J. Glass. 2017. <span>“Spoken Language Understanding for a Nutrition Dialogue System.”</span> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 25 (7): 1450–61. <a href="https://doi.org/10.1109/TASLP.2017.2694699">https://doi.org/10.1109/TASLP.2017.2694699</a>.
</div>
<div id="ref-8461769" class="csl-entry">
———. 2018. <span>“Convolutional Neural Networks and Multitask Strategies for Semantic Mapping of Natural Language Input to a Structured Database.”</span> In <em>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 6174–78. <a href="https://doi.org/10.1109/ICASSP.2018.8461769">https://doi.org/10.1109/ICASSP.2018.8461769</a>.
</div>
<div id="ref-8721137" class="csl-entry">
———. 2019. <span>“Deep Learning for Database Mapping and Asking Clarification Questions in Dialogue Systems.”</span> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 27 (8): 1321–34. <a href="https://doi.org/10.1109/TASLP.2019.2918618">https://doi.org/10.1109/TASLP.2019.2918618</a>.
</div>
<div id="ref-7472843" class="csl-entry">
Korpusik, M., C. Huang, M. Price, and J. Glass. 2016. <span>“Distributional Semantics for Understanding Spoken Meal Descriptions.”</span> In <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 6070–74. <a href="https://doi.org/10.1109/ICASSP.2016.7472843">https://doi.org/10.1109/ICASSP.2016.7472843</a>.
</div>
<div id="ref-korpusik19_interspeech" class="csl-entry">
Korpusik, M., Z. Liu, and J. Glass. 2019. <span>“<span class="nocase">A Comparison of Deep Learning Methods for Language Understanding</span>.”</span> In <em>Proceedings of Interspeech</em>, 849–53. <a href="https://doi.org/10.21437/Interspeech.2019-1262">https://doi.org/10.21437/Interspeech.2019-1262</a>.
</div>
<div id="ref-7078635" class="csl-entry">
Korpusik, M., N. Schmidt, J. Drexler, S. Cyphers, and J. Glass. 2014. <span>“Data Collection and Language Understanding of Food Descriptions.”</span> In <em>2014 IEEE Spoken Language Technology Workshop (SLT)</em>, 560–65. <a href="https://doi.org/10.1109/SLT.2014.7078635">https://doi.org/10.1109/SLT.2014.7078635</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-1909-11942" class="csl-entry">
Lan, Z., M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. 2019. <span>“<span>ALBERT:</span> <span>A</span> Lite <span>BERT</span> for Self-Supervised Learning of Language Representations.”</span> <em>CoRR</em> abs/1909.11942. <a href="http://arxiv.org/abs/1909.11942">http://arxiv.org/abs/1909.11942</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-1908-03265" class="csl-entry">
Liu, L., H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. 2019. <span>“On the Variance of the Adaptive Learning Rate and Beyond.”</span> <em>CoRR</em> abs/1908.03265. <a href="http://arxiv.org/abs/1908.03265">http://arxiv.org/abs/1908.03265</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-1907-11692" class="csl-entry">
Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. 2019. <span>“RoBERTa: <span>A</span> Robustly Optimized <span>BERT</span> Pretraining Approach.”</span> <em>CoRR</em> abs/1907.11692. <a href="http://arxiv.org/abs/1907.11692">http://arxiv.org/abs/1907.11692</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-1711-05101" class="csl-entry">
Loshchilov, I., and F. Hutter. 2017. <span>“Fixing Weight Decay Regularization in Adam.”</span> <em>CoRR</em> abs/1711.05101. <a href="http://arxiv.org/abs/1711.05101">http://arxiv.org/abs/1711.05101</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-1712-09405" class="csl-entry">
Mikolov, T., E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin. 2017. <span>“Advances in Pre-Training Distributed Word Representations.”</span> <em>CoRR</em> abs/1712.09405. <a href="http://arxiv.org/abs/1712.09405">http://arxiv.org/abs/1712.09405</a>.
</div>
<div id="ref-pennington-etal-2014-glove" class="csl-entry">
Pennington, J., R. Socher, and C. Manning. 2014. <span>“<span>G</span>lo<span>V</span>e: Global Vectors for Word Representation.”</span> In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (<span>EMNLP</span>)</em>, 1532–43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-1802-05365" class="csl-entry">
Peters, M. E., M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. 2018. <span>“Deep Contextualized Word Representations.”</span> <em>CoRR</em> abs/1802.05365. <a href="http://arxiv.org/abs/1802.05365">http://arxiv.org/abs/1802.05365</a>.
</div>
<div id="ref-radford2018improving" class="csl-entry">
Radford, A., K. Narasimhan, T. Salimans, and I. Sutskever. 2018. <span>“Improving Language Understanding by Generative Pre-Training.”</span> <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>.
</div>
<div id="ref-NIPS2017_3f5ee243" class="csl-entry">
Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. <span>“Attention Is All You Need.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.
</div>
<div id="ref-NEURIPS2019_dc6a7e65" class="csl-entry">
Yang, Z., Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. 2019. <span>“XLNet: Generalized Autoregressive Pretraining for Language Understanding.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf</a>.
</div>
</div></section></div> ]]></description>
  <category>Machine learning</category>
  <category>Natural language processing</category>
  <category>Public policy</category>
  <category>Health and wellbeing</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/08/21/05-third-place-winners.html</guid>
  <pubDate>Sun, 20 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/05-lm.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Food for Thought: The value of competitions for confidential data</title>
  <dc:creator>Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik</dc:creator>
  <link>https://realworlddatascience.net/case-studies/posts/2023/08/21/06-value-of-competitions.html</link>
  <description><![CDATA[ 




<p>We are witnessing a sea change in data collection practices by both governments and businesses – from purposeful collection (through surveys and censuses, for example) to opportunistic (drawing on web and social media data, and administrative datasets). This shift has made clear the importance of record linkage – a government might, for example, look to link records held by its various departments to understand how citizens make use of the gamut of public services.</p>
<p>However, creating manual linkages between datasets can be prohibitively expensive, time consuming, and subject to human constraints and bias. Machine learning (ML) techniques offer the potential to combine data better, faster, and more cheaply. But, as the recently released <a href="https://www.ai.gov/wp-content/uploads/2023/01/NAIRR-TF-Final-Report-2023.pdf">National AI Research Resources Task Force report</a> highlights, it is important to have an open and transparent approach to ensure that unintended biases do not occur.</p>
<p>In other words, ML tools are not a substitute for thoughtful analysis. Both private and public producers of a linked dataset have to determine the level of linkage quality – such as what precision/recall tradeoff is best for the intended purpose (that is, the balance between false-positive links and failure to cover links that should be there), how much processing time and cost is acceptable, and how to address coverage issues. The challenge is made more difficult by the idiosyncrasies of heterogeneous datasets, and more difficult yet when datasets to be linked include confidential data <span class="citation" data-cites="10.1257/jel.20171350 DBLP:books/sp/ChristenRS20">(Christensen and Miguel 2018; Christen, Ranbaduge, and Schnell 2020)</span>.</p>
<p>And, of course, an ML solution is never the end of the road: many data linkage scenarios are highly dynamic, involving use cases, datasets, and technical ecosystems that change and evolve over time; effective use of ML in practice necessitates an ongoing and continuous investment <span class="citation" data-cites="DBLP:journals/corr/abs-2112-01716">(Koch et al. 2021)</span>. Because techniques are constantly improving, producers need to keep abreast of new approaches. A model that is working well today may no longer work in a year because of changes in the data, or because the organizational needs have changed so that a certain type of error is no longer acceptable. As Sculley et al.&nbsp;point out, “it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning” <span class="citation" data-cites="43146">(Sculley et al. 2014)</span>.</p>
<p>Also important is that record linkage is not seen as a technical problem relegated to the realm of computer scientists to solve. The full engagement of domain experts in designing the optimization problem, identifying measures of success, and evaluating the quality of the results is absolutely critical, as is building an understanding of the pros and cons of different measures <span class="citation" data-cites="10.1371/journal.pone.0249833 10.1007/s11222-017-9746-6">(Schafer et al. 2021; Hand and Christen 2018)</span>. There will need to be much learning by doing in “sandbox” environments, and back and forth communication across communities to achieve successful outcomes, as noted in the <a href="https://www.bea.gov/system/files/2022-10/acdeb-year-2-report.pdf">recommendations of the Advisory Committee on Data for Evidence Building</a> (a screenshot of which is shown in Figure 1).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/pt6-fig1.png"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/pt6-fig1.png" class="img-fluid figure-img" width="700"></a></p>
</figure>
</div>
<div class="figure-caption">
<p><strong>Figure 1:</strong> A recommendation for building an “innovation sandbox” as part of the creation of a new National Secure Data Service in the United States.</p>
</div>
<p>Despite the importance of trial and error and transparency about linkage quality, there is no handbook that guides domain experts in how to design such sandboxes. There is a very real need for agreed-upon, domain-independent guidelines, or better yet, official standards to evaluate sandboxes. Those standards would define “who” could and would conduct the evaluation, and help guarantee independence and repeatability. And while innovation challenges have been embraced by the federal government, the devil can be very much in the details <span class="citation" data-cites="4138bca6-f7b7-3af8-a96c-5e2544823c5c">(Williams 2012)</span>.</p>
<p>It is for this reason that the approach taken in the Food for Thought linkage competition, and described in this compendium, provides an important first step towards a well specified, replicable framework for achieving high quality outcomes. In that respect it joins other recent efforts to bring together community-level research on shared sensitive data <span class="citation" data-cites="macavaney-etal-2021-community tsakalidis-etal-2022-overview">(MacAvaney et al. 2021; Tsakalidis et al. 2022)</span>. This competition, like those, helped bring to the foreground both the opportunities and challenges of doing research in secure sandboxes with sensitive data. Notably, these exercises highlight a kind of cultural tension between secure, managed environments, on the one hand, and unfettered machine learning research, on the other. The need for flexibility and agility in computational research bumps up against the need for advance planning and careful step-by-step processes in environments with well-defined data governance rules, and one of the key lessons learned is that the tradeoffs here need to be recognized and planned for.</p>
<p>This particular competition was important for a number of other reasons. Thanks to its organization as a competition, complete with prizes and bragging rights for strongly performing teams, it attracted new eyes from computer science and data science to think about how to address a critical real-world linkage problem. It offered the potential to produce approaches that were scalable, transparent, and reproducible. The engagement of domain experts and statisticians meant that it will be possible to conduct an informed error analysis, to explicitly relate the performance metrics in the task to the problem being solved in the real world, and to bring in the expertise of survey methodologists to think about the possible adjustments. And because it identified different approaches of addressing the same problem, it created an environment for new innovative ideas.</p>
<p>More generally, in addition to the excitement of the new approaches, this exercise laid bare the fragility of linkages in general and highlighted the importance of secure sandboxes for confidential data. While the promise of privacy preserving technologies is alluring as <a href="https://www.bea.gov/system/files/2022-10/acdeb-year-2-report.pdf">an alternative to bringing confidential data together in one place</a>, such approaches are likely too immature to deploy ad hoc until a better understanding is established of how to translate real-world problems and their associated data into well-defined tasks, how to measure quality, and particularly how to assess the impact of match quality on different subgroups <span class="citation" data-cites="10.1145/3433638">(Domingo-Ferrer, Sánchez, and Blanco-Justicia 2021)</span>. The scientific profession has gone through too painful a lesson with the premature application of differential privacy techniques to ignore the lessons that can be learned from a careful and systematic analysis of different approaches <span class="citation" data-cites="10.1145/3433638 van_riper 10.1257/pandp.20191107 giles2022faking">(2021; Van Riper et al. 2020; Ruggles et al. 2019; Giles et al. 2022)</span>.</p>
<p>We hope that the articles in this collection provide not only the first steps towards a handbook of best practices, but also an inspiration to share lessons learned, so that success can be emulated, and failures understood and avoided.</p>
<div class="nav-btn-container">
<div class="grid">
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/posts/2023/08/21/05-third-place-winners.html">← Part 5: Third place winners</a></p>
</div>
</div>
<div class="g-col-12 g-col-sm-6">
<div class="nav-btn">
<p><a href="../../../../../case-studies/index.html">Find more case studies</a></p>
</div>
</div>
</div>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<strong>Steven Bedrick</strong> is an associate professor in Oregon Health and Science University’s Department of Medical Informatics and Clinical Epidemiology.
</dd>
<dd>
<p><strong>Ophir Frieder</strong> is a professor in Georgetown University’s Department of Computer Science, and in the Department of Biostatistics, Bioinformatics &amp; Biomathematics at Georgetown University Medical Center.</p>
</dd>
<dd>
<p><strong>Julia Lane</strong> is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative.</p>
</dd>
</dl>
<p><strong>Philip Resnik</strong> holds a joint appointment as professor in the University of Maryland Institute for Advanced Computer Studies and the Department of Linguistics, and an affiliate professor appointment in computer science.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;vertical-align:text-bottom;"><img src="https://realworlddatascience.net/case-studies/posts/2023/08/21/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail photo by <a href="https://unsplash.com/@alexandru_tugui?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Alexandru Tugui</a> on <a href="https://unsplash.com/photos/-inuQpBGbgI?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Bedrick, Steven, Ophir Frieder, Julia Lane, and Philip Resnik. 2023. “Food for Thought: The value of competitions for confidential data.” Real World Data Science, August 21, 2023. <a href="https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/06-value-of-competitions.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DBLP:books/sp/ChristenRS20" class="csl-entry">
Christen, P., T. Ranbaduge, and R. Schnell. 2020. <em>Linking Sensitive Data - Methods and Techniques for Practical Privacy-Preserving Information Sharing</em>. Springer. <a href="https://doi.org/10.1007/978-3-030-59706-1">https://doi.org/10.1007/978-3-030-59706-1</a>.
</div>
<div id="ref-10.1257/jel.20171350" class="csl-entry">
Christensen, G., and E. Miguel. 2018. <span>“Transparency, Reproducibility, and the Credibility of Economics Research.”</span> <em>Journal of Economic Literature</em> 56 (3): 920–80. <a href="https://doi.org/10.1257/jel.20171350">https://doi.org/10.1257/jel.20171350</a>.
</div>
<div id="ref-10.1145/3433638" class="csl-entry">
Domingo-Ferrer, J., D. Sánchez, and A. Blanco-Justicia. 2021. <span>“The Limits of Differential Privacy (and Its Misuse in Data Release and Machine Learning).”</span> <em>Communications of the ACM</em> 64 (7): 33–35. <a href="https://doi.org/10.1145/3433638">https://doi.org/10.1145/3433638</a>.
</div>
<div id="ref-giles2022faking" class="csl-entry">
Giles, O., K. Hosseini, G. Mingas, O. Strickson, L. Bowler, C. Rangel Smith, H. Wilde, et al. 2022. <span>“Faking Feature Importance: A Cautionary Tale on the Use of Differentially-Private Synthetic Data.”</span> <a href="https://arxiv.org/abs/2203.01363">https://arxiv.org/abs/2203.01363</a>.
</div>
<div id="ref-10.1007/s11222-017-9746-6" class="csl-entry">
Hand, D., and P. Christen. 2018. <span>“A Note on Using the f-Measure for Evaluating Record Linkage Algorithms.”</span> <em>Statistics and Computing</em> 28 (3): 539–47. <a href="https://doi.org/10.1007/s11222-017-9746-6">https://doi.org/10.1007/s11222-017-9746-6</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-2112-01716" class="csl-entry">
Koch, B., E. Denton, A. Hanna, and J. G. Foster. 2021. <span>“Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research.”</span> <em>CoRR</em> abs/2112.01716. <a href="https://arxiv.org/abs/2112.01716">https://arxiv.org/abs/2112.01716</a>.
</div>
<div id="ref-macavaney-etal-2021-community" class="csl-entry">
MacAvaney, S., A. Mittu, G. Coppersmith, J. Leintz, and P. Resnik. 2021. <span>“Community-Level Research on Suicidality Prediction in a Secure Environment: Overview of the <span>CLP</span>sych 2021 Shared Task.”</span> In <em>Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</em>, 70–80. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.clpsych-1.7">https://doi.org/10.18653/v1/2021.clpsych-1.7</a>.
</div>
<div id="ref-10.1257/pandp.20191107" class="csl-entry">
Ruggles, S., C. Fitch, D. Magnuson, and J. Schroeder. 2019. <span>“Differential Privacy and Census Data: Implications for Social and Economic Research.”</span> <em>AEA Papers and Proceedings</em> 109 (May): 403–8. <a href="https://doi.org/10.1257/pandp.20191107">https://doi.org/10.1257/pandp.20191107</a>.
</div>
<div id="ref-10.1371/journal.pone.0249833" class="csl-entry">
Schafer, K. M., G. Kennedy, A. Gallyer, and P. Resnik. 2021. <span>“A Direct Comparison of Theory-Driven and Machine Learning Prediction of Suicide: A Meta-Analysis.”</span> <em>PLOS ONE</em> 16 (4): 1–23. <a href="https://doi.org/10.1371/journal.pone.0249833">https://doi.org/10.1371/journal.pone.0249833</a>.
</div>
<div id="ref-43146" class="csl-entry">
Sculley, D., G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young. 2014. <span>“Machine Learning: The High Interest Credit Card of Technical Debt.”</span> In <em>SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)</em>.
</div>
<div id="ref-tsakalidis-etal-2022-overview" class="csl-entry">
Tsakalidis, A., J. Chim, I. M. Bilal, A. Zirikly, D. Atzil-Slonim, F. Nanni, P. Resnik, et al. 2022. <span>“Overview of the <span>CLP</span>sych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts.”</span> In <em>Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology</em>, 184–98. Seattle, USA: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2022.clpsych-1.16">https://doi.org/10.18653/v1/2022.clpsych-1.16</a>.
</div>
<div id="ref-van_riper" class="csl-entry">
Van Riper, D., T. Kugler, J. Schroeder, and S. Ruggles. 2020. <span>“Differential Privacy and Racial Residential Segregation.”</span> In <em>2020 APPAM Fall Research Conference</em>.
</div>
<div id="ref-4138bca6-f7b7-3af8-a96c-5e2544823c5c" class="csl-entry">
Williams, H. 2012. <span>“Innovation Inducement Prizes: Connecting Research to Policy.”</span> <em>Journal of Policy Analysis and Management</em> 31 (3): 752–76. <a href="http://www.jstor.org/stable/41653827">http://www.jstor.org/stable/41653827</a>.
</div>
</div></section></div> ]]></description>
  <category>Machine learning</category>
  <category>Natural language processing</category>
  <category>Public policy</category>
  <category>Health and wellbeing</category>
  <guid>https://realworlddatascience.net/case-studies/posts/2023/08/21/06-value-of-competitions.html</guid>
  <pubDate>Sun, 20 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/case-studies/posts/2023/08/21/images/06-value-of-competitions.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Where do AI, data science, and computer games intersect?</title>
  <dc:creator>Alice-Maria Toader and Liam Brierley</dc:creator>
  <link>https://realworlddatascience.net/viewpoints/posts/2023/08/17/data-science-and-games.html</link>
  <description><![CDATA[ 




<p>Game studios have cemented their place among the fastest-growing media industries. In recognition of this, we hosted an event in June through the <a href="https://rss.org.uk/membership/rss-groups-and-committees/groups/merseyside/">Royal Statistical Society (RSS) Merseyside Local Group</a> to explore AI and data science in computer game development. This was an amazing opportunity to engage with a different, in-vogue domain that has unique ties to data science. We showcased two fantastic presentations covering both academic and industry perspectives.</p>
<p>Stanley Wang, a data scientist at SEGA Europe, opened the event by showing the methods that SEGA uses to collect, process, and apply data on player decisions in-game. It was a revealing glimpse at how smoothly in-game data collection is integrated into SEGA’s digital platforms and the ways these data can be used to engage game-centred communities – for example, running special celebrations once milestones are hit for in-game events (revenue made, goals scored, etc.) or offering real-time integration with streaming platforms so viewers can see detailed statistics on in-game progress. Stanley showed one particular example where data collection fed directly into development decisions for <em>Endless Space</em>, a competitive strategy game where players vie for galactic conquest. During the beta (a period where a game is available to play but still considered in-testing before commercial release), SEGA were able to monitor how well-balanced the playable alien factions were based on real-time win rate data, which led to improvements to game mechanics for the final release.</p>
<p>We also learned how SEGA’s data science teams are using clustering methods to identify different game-playing behaviours in <em>Two Point Hospital</em>, a simulation game where players design, build, and manage a hospital through various scenarios. After compiling high-dimensional in-game data such as objectives achieved, treatment of staff, and even furniture choices, various clustering algorithms (including <a href="https://towardsdatascience.com/a-practical-guide-on-k-means-clustering-ca3bef3c853d">k-means clustering</a>) were used to identify common sets of player behaviour. Stanley highlighted that when using these sorts of <em>unsupervised learning methods</em>, it’s useful to get insights from multiple models to inform methodological decisions like number of clusters chosen or how to treat outliers. SEGA identified four distinct types of player from these analyses, which you can hear more about from Stanley in the video below. The approach allowed the company to better understand gamers’ motivations and experiences with a view to designing future game content.</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/KAg3YDHvvqE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Our second speaker, Dr Konstantinos Tsakalidis, a lecturer in the Department of Computer Science at the University of Liverpool, presented exciting new ideas to teach computer games developers of the future. Dr Tsakalidis walked us through the curriculum for a dynamic new undergraduate program that reflects the latest software development technologies and the theory behind them. The course outline was designed around building knowledge and practice from the fundamentals upwards, starting from game physics as a prerequisite for game mechanics, game mechanics being a prerequisite for game content, and game content being a prerequisite for game AI. Combined with the continuous active involvement of students at each stage, this represented a great model of <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8049623/">constructivist teaching</a>. Dr Tsakalidis also proposed that practical game development (and subsequent assessments) should follow the latest <a href="https://www.datacamp.com/podcast/data-science-and-ai-in-the-gaming-industry">research on data science and AI in computer games</a>.</p>
<div class="article-btn">
<p><a href="../../../../../viewpoints/index.html">Discover more Viewpoints</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the author</dt>
<dd>
<strong>Alice-Maria Toader</strong> is a PhD student at the University of Liverpool and a committee member of the RSS Merseyside Local Group. <strong>Liam Brierley</strong> is a research fellow in health data science at the University of Liverpool and chair of the RSS Merseyside Local Group.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Alice-Maria Toader and Liam Brierley
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/viewpoints/posts/2023/08/17/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/viewpoints/posts/2023/08/17/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail image by <a href="https://unsplash.com/@jezar?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Jezael Melgoza</a> on <a href="https://unsplash.com/photos/FOx3_4_2O1E?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Toader, Alice-Maria and Liam Brierley. 2023. “Where do AI, data science, and computer games intersect?” Real World Data Science, August 17, 2023. <a href="https://realworlddatascience.net/viewpoints/posts/2023/08/17/data-science-and-games.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



 ]]></description>
  <category>Events</category>
  <category>Video games</category>
  <category>Education</category>
  <guid>https://realworlddatascience.net/viewpoints/posts/2023/08/17/data-science-and-games.html</guid>
  <pubDate>Wed, 16 Aug 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/viewpoints/posts/2023/08/17/images/sega-store.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Heading to a conference this summer? Share your learnings here</title>
  <dc:creator>Brian Tarran</dc:creator>
  <link>https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/17/dsb-live.html</link>
  <description><![CDATA[ 




<p>Three major events in the statistics and data science calendar are taking place over the next few months, and we want to give the wider community the opportunity to sample some of the exciting ideas being discussed. For that, we need your help!</p>
<p>If you’re a student or early career researcher and you’re attending one or all of the…</p>
<ul>
<li><a href="https://www.isi2023.org/">World Statistics Congress</a></li>
<li><a href="https://ww2.amstat.org/meetings/jsm/2023/">Joint Statistical Meetings</a></li>
<li><a href="https://rss.org.uk/training-events/conference-2023/">Royal Statistical Society International Conference</a></li>
</ul>
<p>…we invite you to write about your favourite paper or session as a “Bites” post.</p>
<p>Bites posts are digestible, engaging, non-technical summaries of research papers and presentations, written for an undergraduate-level audience. The goal is to draw attention to key findings, potential applications, and the wider implications of new ideas and developments in statistics and data science. <a href="https://realworlddatascience.net/contributor-docs/datasciencebites.html">Advice and guidance on how to write a Bites post can be found here</a>, and <a href="https://realworlddatascience.net/ideas/datasciencebites/">example posts can be found on our DataScienceBites page</a>.</p>
<p>For our summer conference coverage, Real World Data Science is partnering with our friends at <a href="https://mathstatbites.org/">MathStatBites</a>. If you write a Bites post and it is accepted for publication, the post will appear on one or both of our sites – depending on the focus of the research you’re writing about.</p>
<p>If you have any questions, please feel free to <a href="https://realworlddatascience.net/contact.html">contact us</a>. Otherwise, safe travels, enjoy the conference(s), and we look forward to hearing from you soon!</p>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About DataScienceBites</dt>
<dd>
<a href="../../../../../../ideas/datasciencebites/index.html"><strong>DataScienceBites</strong></a> is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to <a href="../../../../../../contributor-docs/datasciencebites.html">become a contributor</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Royal Statistical Society
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/17/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/17/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>. Thumbnail image includes photo by <a href="https://unsplash.com/@productschool?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Product School</a> on <a href="https://unsplash.com/photos/nOvIa_x_tfo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Tarran, Brian. 2023. “Heading to a conference this summer? Share your learnings here.” Real World Data Science, July 17, 2023. <a href="https://realworlddatascience.net/news-and-views/datasciencebites/posts/2023/07/17/dsb-live.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



 ]]></description>
  <category>Content ideas</category>
  <category>Call for contributions</category>
  <category>Events</category>
  <guid>https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/17/dsb-live.html</guid>
  <pubDate>Sun, 16 Jul 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/17/images/dsb-live.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Choosing the right forecast</title>
  <dc:creator>Brian King</dc:creator>
  <link>https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/13/choosing-right-forecast.html</link>
  <description><![CDATA[ 




<p>Nobel laureate Niels Bohr is famously quoted as saying, “Prediction is very difficult, especially if it’s about the future.” The science (or perhaps the art) of forecasting is no easy task and lends itself to a large amount of uncertainty. For this reason, practitioners interested in prediction have increasingly migrated to probabilistic forecasting, where an entire distribution is given as the forecast instead of a single number, thus fully quantifying the inherent uncertainty. In such a setting, traditional metrics of assessing and comparing predictive performance, such as mean squared error (MSE), are no longer appropriate. Instead, proper scoring rules are utilized to evaluate and rank forecast methods. A scoring rule is a function that takes a predictive distribution along with an observed value and outputs a real number called the score. Such a rule is said to be proper if the expected score is maximized when the predictive distribution is the same as the distribution from which the observation was drawn.</p>
<p>Many proper scoring rules exist, such as the continuous ranked probability score (CRPS) and the logarithmic score. Choosing which rule to use is not necessarily straightforward. Furthermore, forecast methods are often selected not based on a single score, but rather averages of scores from many probabilistic forecasts, which can introduce new challenges affecting how one might rank competing forecasts. In the paper under discussion, <a href="https://projecteuclid.org/journals/statistical-science/volume-38/issue-1/Local-scale-invariance-and-robustness-of-proper-scoring-rules/10.1214/22-STS864.short">Bolin and Wallin</a> define several properties of scoring rules that help clarify how the rules behave when multiple forecast scores are averaged. Additionally, they introduce a new class of proper rules that aims to overcome some of the deficiencies of other common scoring rules.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
About the paper
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body" style="margin-top: 0rem;">
<p><strong>Title:</strong> Local scale invariance and robustness of proper scoring rules</p>
<p><strong>Author(s) and year:</strong> David Bolin and Jonas Wallin (2023)</p>
<p><strong>Status:</strong> Published in <em>Statistical Science</em>, DOI: <a href="https://projecteuclid.org/journals/statistical-science/volume-38/issue-1/Local-scale-invariance-and-robustness-of-proper-scoring-rules/10.1214/22-STS864.short">10.1214/22-STS864</a>.</p>
</div>
</div>
</div>
<p>The authors argue that situations are often encountered where forecasts are derived and subsequently averaged for observations with different inherent variability. One example might be financial data, such as stock returns, where there are commonly periods with much higher variance (known as volatility in the financial setting). Such processes can be represented using a model known as stochastic volatility, where the variance of observed data evolves randomly over time. Figure 1 plots an example path of the data-generating process under such a model. When data exhibits this varying uncertainty, many proper scoring rules will assign a score whose magnitude changes for those observations with more variability, a characteristic the authors term scale dependence. Some rules will ‘punish’ observations with higher uncertainty, and others may ‘reward’ such observations. Hence, when averaging multiple scores, observations will not be treated symmetrically, which the authors argue can “lead to unintuitive forecast rankings.”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/13/images/fig1.png" class="img-fluid figure-img" alt="Left panel of figure shows a time series of volatility, rendered as a line chart. Right panel shows the resulting observations under a standard stochastic volatility model, in scatter plot form."></p>
</figure>
</div>
<div class="figure-caption" style="text-align: center;">
<p><strong>Figure 1:</strong> Left, a time series of volatility, and right, the resulting observations under a standard stochastic volatility model.</p>
</div>
<p>Thus, an ideal scoring rule will not suffer from scale dependence. The lack of scale dependence is a property that the authors term local scale invariance. The logarithmic score possesses this attribute, but the CRPS and other scoring rules, like the Hyvärinen score, do not. To address this issue, the authors propose a new class of scoring rules which exhibits local scale invariance. Among this class is a scoring rule dubbed the scaled CRPS (SCRPS), which features many of the desirable qualities of the CRPS but overcomes the scale dependence issue.</p>
<p>Of course, if local scale invariance is all that matters, then we could just use the logarithmic score in all scenarios. But there is another issue to consider when averaging forecast scores – the presence of outliers. In many scenarios, we might encounter observations that are very far outside the normal range, and we don’t want our average forecast performance measure to be greatly thrown off if such an oddity is observed. In other words, we want our proper scoring rules to be robust. In their article, Bolin and Wallin formalize the concept of robustness for scoring rules and show that, in many cases, the logarithmic score is not robust. Yet they also prove their proposed class of scaled scoring rules is not generally robust, although they show that the scoring rules can be modified to be robust (a new scoring rule they term robust SCRPS). Under such a modification, however, the scoring rule would no longer be local scale invariant in the strict sense. Indeed, under the proposed definitions of local scale invariance and robustness, finding a scoring rule that can simultaneously satisfy both criteria seems difficult. The authors conjecture that it may even be impossible.</p>
<p>Hence, this paper raises many questions for future consideration but achieves its goal of showing that evaluating probabilistic forecasts by averaging proper scoring rules is not necessarily a simple matter. Different scoring rules will lead to different rankings of forecasting methods, and the underlying properties of each scoring rule must be considered on a case-by-case basis. Although not discussed in this summary, the authors also compare scoring rules in several scenarios and present the theory behind the ideas examined here. For interested readers who want to dig more into these ideas, check out <a href="https://projecteuclid.org/journals/statistical-science/volume-38/issue-1/Local-scale-invariance-and-robustness-of-proper-scoring-rules/10.1214/22-STS864.short">the full paper published in <em>Statistical Science</em></a>.</p>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-6">
<dl>
<dt>About the author</dt>
<dd>
<strong>Brian King</strong> is currently a senior machine learning research engineer at Arm, working on applying machine learning to hardware verification. He recently completed his PhD in statistics at Rice University, where his research focused on Bayesian modeling and forecasting for time series of counts.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>About DataScienceBites</dt>
<dd>
<a href="../../../../../../ideas/datasciencebites/index.html"><strong>DataScienceBites</strong></a> is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to <a href="../../../../../../contributor-docs/datasciencebites.html">become a contributor</a>.
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Brian King
</dd>
</dl>
<p>This post is republished with permission from <a href="https://mathstatbites.org/choosing-the-right-forecast/">MathStatBites</a>. Thumbnail image by <a href="https://unsplash.com/@bdchu614?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Brendan Church</a> on <a href="https://unsplash.com/photos/pKeF6Tt3c08?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
King, Brian. 2023. “Choosing the right forecast.” Real World Data Science, July 13, 2023. <a href="https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/13/choosing-right-forecast.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>



 ]]></description>
  <category>Forecasting</category>
  <category>Prediction</category>
  <category>Robustness</category>
  <guid>https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/13/choosing-right-forecast.html</guid>
  <pubDate>Wed, 12 Jul 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/ideas/datasciencebites/posts/2023/07/13/images/brendan-church-pKeF6Tt3c08-unsplash.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Trusted AI: translating AI ethics from theory into practice</title>
  <dc:creator>Maxine Setiawan and Mira Pijselman</dc:creator>
  <link>https://realworlddatascience.net/ideas/posts/2023/07/03/trusted-AI.html</link>
  <description><![CDATA[ 




<p>With artificial intelligence (AI) becoming increasingly prevalent across sectors, so too have conversations about AI ethics. AI ethics provides a repeatable and comprehensive way to assess what we should and should not be doing with AI, and sets out how we ought to design, use, and govern AI products in accordance with key principles. Ethical frameworks are essential to derive sustainable value from AI products and services and build trust.</p>
<p>A myriad of AI tools that leverage automated or semi-automated decision-making processes have raised important questions that have become foundational in the AI ethics community, such as ‘What does it mean for an algorithm to be fair?’ As an example, AI tools that are used in recruitment may perpetuate biases arising from historical training data. If a model used to generate a shortlist of applicants has been trained on data from past candidates, say, and those candidates – both successful and unsuccessful – are predominantly men, historical patterns that contain various biases will perpetuate to become algorithmic biases that form the model’s decisions. Thus, the model may algorithmically discriminate against women or gender minorities, as individuals from these groups are not well represented in the training data.</p>
<p>To ensure the safe and responsible use of AI, the focus moving forward needs to be on the operationalisation of AI ethics into the day-to-day development lifecycle. But, what does this look like in practice? And how might you get started as an ethical AI practitioner? In this article, we unpack these questions and give you, the data scientist, a foundation to begin your journey towards trusted AI. Read along to get an overview of key principles that you should be aware of, what they mean, their underlying technical grounding, and what implementation might look like practically.</p>
<section id="ethical-ai-principles" class="level2">
<h2 class="anchored" data-anchor-id="ethical-ai-principles">Ethical AI principles</h2>
<p>You have likely heard of several principles in relation to ethical AI, such as fairness or transparency. The context in which you’ve encountered such principles is most probably due to their inclusion in a broader ethical framework. Some of the most popular ethical AI frameworks include the <a href="https://www.nist.gov/itl/ai-risk-management-framework">National Institute of Standards and Technology’s AI Risk Management Framework</a>, the <a href="https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework-2020">UK Data Ethics Framework</a>, and the <a href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">European Commission’s Ethics Guidelines for Trustworthy AI</a>. Among these and many other frameworks, we can run into what <a href="https://dx.doi.org/10.2139/ssrn.3831321">Floridi and Cowls (2019)</a> call “principle proliferation,” whereby it becomes overwhelming for those contributing to AI programmes to know where to begin with ethics due to an excess of choice (p.&nbsp;2).</p>
<p>At the time of writing, there is no single universally accepted standard that dictates which essential ethical AI values or principles should be adhered to during AI development and deployment. However, there are common themes that emerge. In our organisation, EY, we’ve learned from the variety of principles, frameworks, and white papers in the AI ethics community and developed our own Trusted AI Framework comprising five key attributes that we believe assure the trustworthiness of AI:</p>
<ul>
<li>Transparent</li>
<li>Explainable</li>
<li>Unbiased</li>
<li>Resilient</li>
<li>High-performing</li>
</ul>
<p>In this article, we take a deeper dive into the first three attributes – transparency, explainability, and unbiasedness (or fairness). These are areas where data scientists can act as critical enablers of ethical AI when they have the right knowledge and toolkits at their disposal.</p>
<section id="transparency" class="level3">
<h3 class="anchored" data-anchor-id="transparency">Transparency</h3>
<p>Transparency is the ability to provide meaningful information to foster awareness and understanding of an AI system. It starts with documenting AI systems in a way that is accessible for a broad audience with a spectrum of technical abilities. It is a simple yet powerful way to build trust in AI. It empowers non-technical stakeholders to critically evaluate AI development decisions, thereby unlocking multi-disciplinary insights that can mitigate reputational or performance risks. Further, it also builds trust with society, as it can enable everyday users to interrogate AI design decisions, product capabilities, and system limitations, thereby permitting users to make informed judgements about technology. Unfortunately, transparency is often misunderstood as disclosing trade secrets or proprietary information, such as source code and datasets. However, transparency can be achieved without disclosing such technically complex information. Instead, it can be as simple as disclosing where and when an AI system is being used, or for what purposes a model should be employed.</p>
<p>But what exactly does “documenting AI systems” look like? Documentation should consist of a mix of technical components (system architecture, dataset selection determination, model selection techniques, etc.) and non-technical components (business case, product purpose of use, alignment to overall AI strategy, etc.). The research community has recommended AI documentation standards, such as <a href="https://arxiv.org/pdf/1803.09010.pdf">datasheets for datasets</a> and <a href="https://arxiv.org/pdf/1810.03993.pdf">model cards for model reporting</a>. You can liken datasheets or model cards to the importance placed upon commenting your code – the more information there is available around decisions throughout model development, the greater the certainty that these artefacts will be understood and used as intended moving forward. Proper documentation and governance will help ensure accountability, improve internal and external oversight, and initiate discussions around model optimisation goals and their trade-offs, such as including fairness and accuracy in optimisation objectives.</p>
<p>With upcoming AI regulations, transparency requirements will become more integral. For example, the <a href="https://artificialintelligenceact.eu/">European Union (EU) AI Act</a> introduces specific transparency obligations, such as bot disclosures, for both users and providers of AI systems, which would allow users to opt out of interacting with an AI system. Furthermore, in higher risk use cases, specific technical documentation is needed, which would include details of a system’s intended purpose and descriptions of its development process.</p>
</section>
<section id="explainability" class="level3">
<h3 class="anchored" data-anchor-id="explainability">Explainability</h3>
<p>Once transparency is enabled, explainability is a natural next step, especially when an AI product is implemented in a more regulated or high-risk environment. Explainability is the ability to express why an AI system reached a particular decision or understand the features that affect model behaviour. Explainability is a key concern within the field of explainable AI, which, as a discipline, strives to improve trustworthiness by enabling a better understanding of a system’s underlying logic via a suite of technical methods.</p>
<p>Fundamentally, different model architectures mean that some models are more interpretable than others, as the steps used to evaluate their predictions are easier for humans to comprehend. Decision trees, for example, have more human-interpretable characteristics than deep learning models. Different model architectures also mean that there are interpretation tools that are only applicable to certain models, such as regression weights in a linear model.</p>
<p>Another approach to consider, then, is model-agnostic interpretation, which encompasses both global interpretability (explanation of an entire model’s behaviour) and local interpretability (explanation of a single prediction). While there are fast-developing techniques and tools for model-agnostic interpretability, let’s take a look at two of the more popular methods available:</p>
<ul>
<li><dl>
<dt><a href="https://dl.acm.org/doi/10.1145/2939672.2939778">Local interpretable model-agnostic explanations (LIME)</a></dt>
<dd>
This is an explanation technique that trains local surrogate models, using explainable models such as Lasso or decision trees, to approximate the predictions of a model that is not interpretable by design in order to explain individual model predictions. The idea is to use interpretable features from the surrogate models to create human-friendly explanations where the underlying model cannot. For example, in an image classification model that detects a flower in an image, LIME is able to highlight the parts of the image that explain why the model classifies the image as a flower (see illustration below). This provides an interpretable explanation between the input variable and prediction, which is an essential part of interpretability.
</dd>
</dl></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/lime.png"><img src="https://realworlddatascience.net/ideas/posts/2023/07/03/images/lime.png" class="img-fluid figure-img" alt="Illustration of explainable AI processes using LIME on an image classification AI system. In this example, an image classification system receives an image of a sunflower and classifies it as a flower with 70% likelihood. The LIME approach then sees parts of the input image perturbed, or masked, leading to different classification likelihoods from the AI system. From this, a model is able to determine the parts of the input image that best explain the initial classification of 'flower'."></a></p>
<p></p><figcaption class="figure-caption">Illustration of explainable AI processes using LIME on an image classification AI system. Adapted from “Local Interpretable Model-Agnostic Explanations (LIME): An Introduction” and <a href="https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/">O’Reilly</a>.</figcaption><p></p>
</figure>
</div>
<ul>
<li><dl>
<dt><a href="https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">Shapley Additive Explanations (SHAP)</a></dt>
<dd>
SHAP (<a href="https://github.com/slundberg/shap">GitHub repo</a>) uses tools and theoretical foundations from game theory, one of which is Shapley values. It works by assigning each feature an importance value for a particular prediction to numerically explain the contribution of various features to a model’s output. For example, in a model that predicts flu, SHAP calculates the importance of sneezing as a feature by removing and adding the subset of other features, leading to different combinations of features that contribute to the prediction. This method provides interpretable solutions for more complex models similar to the equivalent of “weights” in linear models.
</dd>
</dl></li>
</ul>
</section>
<section id="fairness" class="level3">
<h3 class="anchored" data-anchor-id="fairness">Fairness</h3>
<p>The area of AI ethics that is central to impending AI regulations, such as the EU AI Act and the New York City AI Law,<sup>1</sup> is fairness. AI models are inherently biased because of their underlying training data.<sup>2</sup> Thus, when we speak of fairness in the context of AI ethics, we are referring to a combination of technical and non-technical ways to minimise the impacts of algorithmic bias.</p>
<p>Let’s begin with the technical approaches to fairness. To achieve equitable, reliable, and fair decisions, a diverse and balanced set of examples is needed in training datasets. However, data often contains disparities that, if left unchecked, can perpetuate algorithmic biases and harms. There are various approaches to detect sources of bias, guarantee fairness, or “debias” models. To strive for algorithmic fairness, many papers have proposed various quantitative measures of fairness, with some based on unstated assumptions about fairness in society. Unfortunately, <a href="https://arxiv.org/pdf/1609.07236.pdf">these assumptions are often mutually incompatible</a>, making it difficult to compare fairness metrics to one another – consider, for example, the longstanding debate between equality of outcome and equality of treatment.</p>
<p>Although metrics incompatibilities exist, fairness broadly focuses on equality of opportunity (group fairness), and equality of outcome (individual fairness) to prevent discrimination against certain attributes. Drawing definitions from legal frameworks, the term “protected attribute” refers to the characteristics that are often protected under anti-discrimination laws, such as gender or race. Mathematically, the following metrics are often used to demonstrate scores that support fairness:</p>
<ul>
<li><dl>
<dt>Statistical parity</dt>
<dd>
This measure seeks to uncover whether a model is fair towards protected attributes by measuring the difference between the majority and protected class in receiving a favourable outcome. A value of 0 demonstrates the model to be fair.
</dd>
</dl></li>
<li><dl>
<dt>Disparate impact</dt>
<dd>
This compares the percentage of favourable outcomes for the monitored group to the percentage of favourable outcomes for a reference group. The groups compared can be the majority group and minority group, and this score will highlight in whose direction decisions are biased. For example, if a model grants loans to 60% of people in a middle-aged group and only 50% for those of other age cohorts, then the disparate impact is 0.8, which indicates a positive bias towards the middle-aged group and an adverse impact on the remaining cohorts.
</dd>
</dl></li>
<li><dl>
<dt>Equality of odds</dt>
<dd>
This measures the balance of the true positive rate and false positive rate between protected and unprotected groups, which seeks to uncover whether a model performs similarly for the two groups.
</dd>
</dl></li>
</ul>
<p>It is important to remember that statistics are only one side of the fairness problem for machine learning, and one that treats the symptoms of bias as opposed to the underlying causes. In addition to the aforementioned technical approaches, there are a variety of non-technical measures that teams developing AI systems can adopt to augment fairness and inclusion:</p>
<ul>
<li><dl>
<dt>Definition of fairness</dt>
<dd>
Organisations that develop or use AI systems need to define, practically, what it means to be fair. Although there are various quantitative fairness measures, these are based on assumptions of fairness in society, which could be defined for each specific use case.
</dd>
</dl></li>
<li><dl>
<dt>Diversity on teams</dt>
<dd>
There’s been a sharpened focus on the value of team diversity to areas such as productivity and creativity. The same is true for ethics. Ensuring that product teams are composed of a broad cross-section of identities can help to organically drive fairness through diversity of thought and experience.
</dd>
</dl></li>
<li><dl>
<dt>Education and self-reflection</dt>
<dd>
Developing knowledge within individuals and teams about the socio-technical aspects of AI – that is, the ways in which AI shapes our social, political, economic, and environmental lives. The more critical a person can be as a data scientist in questioning why something is being built, the more likely they are to proactively recognise risks surrounding fairness.
</dd>
</dl></li>
<li><dl>
<dt>Consider the end user</dt>
<dd>
Imagine that you are on a development team building an AI solution for a problem in the agricultural sector pertaining to livestock health. Who is best suited to solving the problem: a data scientist or a farmer? As a data scientist, you may have the tools to develop a solution, but given your distance from the end user, you are unlikely to intimately understand the problem in the same way a farmer would. If you cannot understand the problem, you cannot hope to find a solution, much less an ethical one. Recognising the importance of consulting individuals that are representative of end users is key to ensure that your design is fair.
</dd>
</dl></li>
<li><dl>
<dt>AI ethics review boards</dt>
<dd>
Data science teams should not operate in isolation. Increasingly, organisations are establishing AI ethics review boards or similar forums that are intended to act as checks on the design decisions made throughout AI development. Does your organisation have one?
</dd>
</dl></li>
</ul>
</section>
</section>
<section id="in-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="in-conclusion">In conclusion</h2>
<p>These three areas – transparency, explainability, and fairness – are the starting points to embed and operationalise AI ethics in technical development. Transparency relies on both technical and non-technical documentation to facilitate discussions with non-technical stakeholders, as well as to create and enforce accountabilities. Explainability helps to build trust in AI output by vesting us with an ability to explain “why”. Finally, adopting both technical and non-technical measures of fairness can ensure that AI products in development do not adversely impact certain groups.</p>
<p>In addition to these three areas of AI ethics, within EY we have two other focus areas – resilience and high-performance – that form part of our Trusted AI Framework. We will discuss these in a future article. We’re also keen to explore topics such as generating trust in generative AI! Until then, please share your stories of developing ethical AI projects in the comments below. How are you translating AI ethics from theory into practice?</p>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Further reading
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body" style="margin-top: 2.25rem;">
<p>For further technical reading, we suggest:</p>
<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a>, by Christoph Molnar</li>
<li><a href="https://fairmlbook.org/">Fairness and Machine Learning</a>, by Solon Barocas, Moritz Hardt, and Arvind Narayanan</li>
</ul>
<p>For further socio-technical reading on AI and data ethics, we suggest:</p>
<ul>
<li><em>The Age of Surveillance Capitalism</em>, by Shoshana Zuboff</li>
<li><em>Invisible Women</em>, by Caroline Criado Pérez</li>
<li><em>Race after Technology</em>, by Ruha Benjamin</li>
<li><em>Algorithms of Oppression</em>, by Safiya Noble</li>
<li><em>Atlas of AI</em>, by Kate Crawford</li>
<li><em>Weapons of Math Destruction</em>, by Cathy O’Neil</li>
<li><em>Data Feminism</em>, by Catherine D’Ignazio and Lauren Klein</li>
</ul>
</div>
</div>
</div>
<div class="article-btn">
<p><a href="../../../../../ideas/index.html">Explore more data science ideas</a></p>
</div>
<div class="further-info">
<div class="grid">
<div class="g-col-12 g-col-md-12">
<dl>
<dt>About the authors</dt>
<dd>
<a href="https://www.ey.com/en_uk/people/maxine-setiawan">Maxine Setiawan</a> is a social data scientist specialising in trusted AI, and AI and data risk in EY UK&amp;I. With her multi-disciplinary background, she works to help clients understand and manage risks from their data and AI systems, and to ensure AI governance that is fair, accountable, and trustworthy. Maxine holds an MSc in social data science from the University of Oxford.
</dd>
<dd>
<p><a href="https://www.ey.com/en_uk/people/mira-pijselman">Mira Pijselman</a> is the digital ethics lead for EY UK&amp;I, where she focuses on the responsible governance of key emerging technologies, including artificial intelligence, quantum technologies, and the metaverse. A social scientist and philosopher by training, she helps clients to map, understand, secure, and capitalise on their data and technology potential safely. Mira holds an MSc in the social science of the internet from the University of Oxford.</p>
</dd>
</dl>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>Copyright and licence</dt>
<dd>
© 2023 Maxine Setiawan and Mira Pijselman
</dd>
</dl>
<p><a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> <img style="height:22px!important;vertical-align:text-bottom;" src="https://realworlddatascience.net/ideas/posts/2023/07/03/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/ideas/posts/2023/07/03/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"> International licence</a>.</p>
<p>Thumbnail image by <a href="https://www.burg-halle.de/en/xlab">Alexa Steinbrück</a> / <a href="https://www.betterimagesofai.org">Better Images of AI</a> / Explainable AI / <a href="https://creativecommons.org/licenses/by/4.0/">Licenced by CC-BY 4.0</a>.</p>
</div>
<div class="g-col-12 g-col-md-6">
<dl>
<dt>How to cite</dt>
<dd>
Setiawan, Maxine and Mira Pijselman. 2023. “Trusted AI: translating AI ethics from theory into practice.” Real World Data Science, July 3, 2023. <a href="https://realworlddatascience.net/ideas/posts/2023/07/03/trusted-AI.html">URL</a>
</dd>
</dl>
</div>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>New York City Local Law 144.↩︎</p></li>
<li id="fn2"><p>Not statistical bias (usually known as bias-variance trade-off), which compares the training data and target value to approximate errors.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>AI ethics</category>
  <category>Principles</category>
  <category>Regulation</category>
  <guid>https://realworlddatascience.net/ideas/posts/2023/07/03/trusted-AI.html</guid>
  <pubDate>Sun, 02 Jul 2023 22:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/ideas/posts/2023/07/03/images/explainable-AI.png" medium="image" type="image/png" height="102" width="144"/>
</item>
</channel>
</rss>
