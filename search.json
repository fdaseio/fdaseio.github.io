[
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#site-content",
    "href": "ts-and-cs.html#site-content",
    "title": "Terms and conditions",
    "section": "Site content",
    "text": "Site content\nThis site and the “Real World Data Science” and “RWDS” brands and logos are copyright © The Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page. We make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged on this website so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish this site in its entirety."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nRWDS editors and contributors recommend external web links on the basis of their suitability and usefulness for our users. Selection and addition of links to our website is entirely a matter for RWDS and for RWDS alone.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders of any product, service, policy or opinion of the organisation or individual. RWDS, its editors, the RSS, or other partners and funders are not responsible for the content of external websites."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of RWDS is final and no correspondence will be entered into.\nIf you wish to report a concern, please email b.tarran@rss.org.uk."
  },
  {
    "objectID": "ts-and-cs.html#software-and-services",
    "href": "ts-and-cs.html#software-and-services",
    "title": "Terms and conditions",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for this site are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct.\nThis site is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nReal World Data Science is hosted by GitHub Pages.\nThis site uses Google Analytics 4 for web analytics reporting.\nUser comments and reaction functionality is provided by giscus, a comments system powered by GitHub Discussions. Use of this comment functionality is governed by the Contributor Covenant Code of Conduct."
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown policy",
    "text": "Notice and Takedown policy\nIf you are a rights holder and are concerned that you have found material on our site for which you have not given permission, or is not covered by a limitation or exception in national law, please contact us in writing stating the following:\n\nYour contact details.\nThe full bibliographic details of the material.\nThe exact and full url where you found the material.\nProof that you are the rights holder and a statement that, under penalty of perjury, you are the rights holder or are an authorised representative.\n\nContact details:\nNotice and Takedown,\nLicensing,\n12 Errol Street,\nLondon EC1Y 8LX\nweb@rss.org.uk\nUpon receipt of notification, the ‘Notice and Takedown’ procedure is then invoked as follows:\n\nWe will acknowledge receipt of your complaint by email or letter and will make an initial assessment of the validity and plausibility of the complaint.\nUpon receipt of a valid complaint the material will be temporarily removed from our website pending an agreed solution.\nWe will contact the contributor who deposited the material, if relevant. The contributor will be notified that the material is subject to a complaint, under what allegations, and will be encouraged to assuage the complaints concerned.\nThe complainant and the contributor will be encouraged to resolve the issue swiftly and amicably and to the satisfaction of both parties, with the following possible outcomes:\n\nThe material is replaced on our website unchanged.\nThe material is replaced on our website with changes.\nThe material is permanently removed from our website.\n\n\nIf the contributor and the complainant are unable to agree a solution, the material will remain unavailable through the website until a time when a resolution has been reached."
  },
  {
    "objectID": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "href": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "title": "Terms and conditions",
    "section": "Contributor Covenant Code of Conduct",
    "text": "Contributor Covenant Code of Conduct\n\nOur pledge\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\nOur standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\nEnforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\nScope\nThis Code of Conduct applies within all community spaces (encompassing this site, our GitHub repository, our social media channels, and any RWDS-organised online and offline events). It also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nNote that unless prior permission is agreed in writing with the editor of RWDS, only the editor and editorial board of RWDS may officially represent the community. Comment to the media must only be given by appointed representatives and must be approved by the RSS press office.\n\n\nEnforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at b.tarran@rss.org.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\nEnforcement guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\nAttribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "miembros/career-profiles/tamanna-haque/tamanna-haque.html",
    "href": "miembros/career-profiles/tamanna-haque/tamanna-haque.html",
    "title": "Antonio Elías Fernández",
    "section": "",
    "text": "Photo supplied by Tamanna Haque.\n\n\n\nDescubre otros miembros del grupo"
  },
  {
    "objectID": "join.html",
    "href": "join.html",
    "title": "Cómo unirse",
    "section": "",
    "text": "El grupo está abierto a cualquier persona con interés en el área, tanto del mundo académico como de la industria.\nPara inscribirte puedes seguir los pasos que se facilitan en la página web de la SEIO. Si tienes cualquier consulta no dudes en escribirnos."
  },
  {
    "objectID": "feeds.html",
    "href": "feeds.html",
    "title": "RSS feeds",
    "section": "",
    "text": "Latest content\nrealworlddatascience.net/latest-content.xml\n\n\nCase studies\nrealworlddatascience.net/case-studies/index.xml\n\n\nIdeas\nrealworlddatascience.net/ideas/index.xml\n\n\nCareers\nrealworlddatascience.net/careers/index.xml\n\n\nViewpoints\nrealworlddatascience.net/viewpoints/index.xml"
  },
  {
    "objectID": "eventos/posts/2024/road-to-reproducible-research.html",
    "href": "eventos/posts/2024/road-to-reproducible-research.html",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "",
    "text": "Reproducibility, or “the ability of a researcher to duplicate the results of a prior study using the same materials as the original investigator”, is critical for sharing and building upon scientific findings. Reproducibility not only verifies the correctness of processes leading to results but also serves as a prerequisite for assessing generalisability to other datasets or contexts. This we refer to as replicability, or “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”. Reproducibility, which is the focus of our work here, can be challenging – especially in the context of deep learning. This article, and associated material, aims to provide practical advice for overcoming these challenges.\nOur story begins with Davit Svanidze, a master’s degree student in economics at the London School of Economics (LSE). Davit’s efforts to make his bachelor’s thesis reproducible are what inspires this article, and we hope that readers will be able to learn from Davit’s experience and apply those learnings to their own work. Davit will demonstrate the use of Jupyter notebooks, GitHub, and other relevant tools to ensure reproducibility. He will walk us through code documentation, data management, and version control with Git. And, he will share best practices for collaboration, peer review, and dissemination of results.\nDavit’s story starts here, but there is much more for the interested reader to discover. At certain points in this article, we will direct readers to other resources, namely a Jupyter notebook and GitHub repository which contain all the instructions, data and code necessary to reproduce Davit’s research. Together, these components offer a comprehensive overview of the thought process and technical implementation required for reproducibility. While there is no one-size-fits-all approach, the principles remain consistent."
  },
  {
    "objectID": "eventos/posts/2024/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "href": "eventos/posts/2024/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Davit’s journey towards reproducibility",
    "text": "Davit’s journey towards reproducibility\n\nMore power, please\nThe focus of my bachelor’s thesis was to better understand the initial spread of Covid-19 in China using deep learning algorithms. I was keen to make my work reproducible, but not only for my own sake. The “reproducibility crisis” is a well-documented problem in science as a whole,1 2 3 4 with studies suggesting that around one-third of social science studies published between the years 2010 and 2015 in top journals like Nature and Science could not be reproduced.5 Results that cannot be reproduced are not necessarily “wrong”. But, if findings cannot be reproduced, we cannot be sure of their validity.\nFor my own research project, I gathered all data and started working on my computer. After I built the algorithms to train the data, my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long, and I needed a faster, more powerful solution to be able to submit my thesis in time. Fortunately, I could access the university server to train the algorithms. Once the training was complete, I could generate the results on my local computer, since producing maps and tables was not so demanding. However…\n\n\nBloody paths!\nIn switching between machines and computing environments, I soon encountered an issue with my code: the paths, or file directory locations, for the trained algorithms had been hardcoded! As I quickly discovered, hardcoding a path can lead to issues when the code is run in a different environment, as the path might not exist in the new environment.\nAs my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake – which would have been easily corrected if spotted earlier – resulted in incorrect outputs. Such errors could have enormous (negative) implications in a public health context, where evidence-based decisions have real impacts on human lives. It was at this point that I realised that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if they are not able to verify it?\nThe following dummy code demonstrates the hardcoding issue:\n```{python}\n# Hardcoded path\nfile_path = \"/user/notebooks/toydata.csv\"\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nIn the code above, a dummy file (toydata.csv) is used. The dummy file contains data on the prices of three different toys, but only the path of the file is relevant to this example. If the hardcoded file path – \"/user/notebooks/toydata.csv\" – exists on the machine being used, the code will run just fine. But, when run in a different environment without said path, the code will result in a \"File not found error\". Better code that uses relative paths can be written as:\n```{python}\n# Relative path\nimport os\n\nfile_path = os.path.join(os.getcwd(), \"toydata.csv\")\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nYou can see that this code has successfully imported data from the dataset toydata.csv and printed its two columns (toy and price) and three rows.\nThe following example is a simplified version of what happened when I wrote code to train several models, store the results and run a procedure to compare results with the predictive performance of a benchmark model:\n```{python}\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)\n```\n```{python}\n# Load the model result and compare with benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model\nresult = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nEverything looks fine at a glance. But, if you examine the code carefully, you may spot the problem. Initially, when I coded the procedure (training the model, saving and loading the results), I hardcoded the paths and had to change them for each tested model. First, I trained model2, a complex model, and tested it against the benchmark (70 &gt; 50 → accepted). I repeated the procedure for model1 (a simple model). Its result was identical to model2, therefore I kept model1 following the parsimony principle.\nHowever, for the code line loading the result for the current model (line 5, second cell), I forgot to amend the path and so mistakenly loaded the result of model2. As a consequence, I accepted a model which should have been rejected. These wrong results were then spread further in the code, including all charts and maps and the conclusions of my analysis.\nA small coding error like this can therefore be fatal to an analysis. Below is the corrected code:\n```{python}\nimport os\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details (INCLUDING PATHS) in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": os.path.join(os.getcwd(), \"results-of-model1.csv\")}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": os.path.join(os.getcwd(), \"results-of-model2.csv\")}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv(current_model[\"path\"], index=False)\n```\n```{python}\n# Get the model result and compare with the benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model WITH a VARIABLE PATH\nresult = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nHere, the paths are stored with other model details (line 7–8, first cell). Therefore, we can use them as variables when we need them (e.g., line 16, first cell, and line 5, second cell). Now, when the current model is set to model1 (line 11, first cell), everything is automatically adjusted. Also, if the path details need to be changed, we only need to change them once and everything else is automatically adjusted and updated. The code now correctly states that model1 performs worse than the benchmark and is therefore rejected and we should keep model2, which performs best.\nI managed to catch this error in time, but it often can be difficult to spot our own mistakes. That is why making code available to others is crucial. A code review by a second (or third) pair of eyes can save everyone a lot of time and avoid spreading incorrect results and conclusions.\n\n\nSolving compatibility chaos with Docker\nOne might think that it would be easy to copy code from one computer to another and run it without difficulties, but it turns out to be a real headache. Different operating systems on my local computer and the university server caused multiple compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, the server did not support the Python programming language – and all the deep learning algorithm packages that I needed – in the same way as my macOS computer did.\nAs a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed. This way, I could integrate them with different hardware and use the processing power of that hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. In fact, I found the Docker website very helpful, with lots of resources and tutorials available. Once Docker was installed, it was easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they could also make it easier for others to reproduce my work.\nBelow is an example of a Dockerfile which recreates an environment with Python 3.7 on Linux. It describes what, how, when and in which order operations should be carried out to generate the environment with all Python packages required to run the main Python script, main.py.\n\n\nAn example of a Dockerfile.\n\nIn this example, by downloading the project, including the Dockerfile, anyone can run main.py without installing packages or worrying about what OS was used for development or which Python version should be installed. You can view Docker as a great robot chef: show it a recipe (Dockerfile), provide the ingredients (project files), push the start button (to build the container) and wait to sample the results.\n\n\nWhy does nobody check your code?\nEven after implementing Docker, I still faced another challenge to reproducibility: making the verification process for my code easy enough that it could be done by anyone, without them needing a degree in computer science! Increasingly, there is an expectation for researchers to share their code so that results can be reproduced, but there are as yet no widely accepted or enforced standards on how to make code readable and reusable. However, if we are to embrace the concept of reproducibility, we must write and publish code under the assumption that someone, somewhere – boss, team member, journal reviewer, reader – will want to rerun our code. And, if we expect that someone will want to rerun our code (and hopefully check it), we should ensure that the code is readable and does not take too long to run.\nIf your code does take too long to run, some operations can often be accelerated – for example, by reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using PyTorch). Aim for a running time of a few minutes – or about as long as it takes to make a cup of tea or coffee. Of course, if data needs to be reduced to save computational time, the person rerunning your code won’t generate the same results as in your original analysis. This therefore will not lead to reproducibility, sensu stricto. However, as long as you state clearly what are the expected results from the reduced dataset, your peers can at least inspect your code and offer feedback, and this marks a step towards reproducibility.\nWe should also make sure our code is free from bugs – both the kind that might lead to errors in analysis and also those that stop the code running to completion. Bugs can occur for various reasons. For example, some code chunks written on a Windows machine may not properly execute on a macOS machine because the former uses \\ for file paths, while the latter uses /:\n```{python}\n# Path works on macOS/Linux\nwith open(\"../../all/notebooks/toydata.csv\", \"r\") as f:\n    print(f.read())\n\n# Path works only on Windows    \nwith open(r\"..\\..\\all\\notebooks\\toydata.csv\", \"r\") as f:\n   print(f.read())\n```\n\nHere, only the macOS/Linux version works, since the code this capture was taken from was implemented on a Linux server. There are alternatives, however. The code below works on macOS, Linux, and also Windows machines:\n```{python}\nfrom pathlib import Path\n\n# Path works on every OS: macOS/Linux/Windows\n# It will automatically replace the path to \"..\\..\\all\\notebooks\\toydata.csv\" when it runs on Windows\nwith open(Path(\"../../all/notebooks/toydata.csv\"), \"r\") as f:\n    print(f.read())\n```\n\nThe extra Python package, pathlib, is of course unnecessary if you build a Docker container for your project, as discussed in the previous section.\n\n\nJupyter, King of the Notebooks\nBy this stage in my project, I was feeling that I’d made good progress towards ensuring that my work would be reproducible. I’d expended a lot of effort to make my code readable, efficient, and also absent of bugs (or, at least, this is what I was hoping for). I’d also built a Docker container to allow others to replicate my computing environment and rerun the analysis. Still, I wanted to make sure there were no barriers that would prevent people – my supervisors, in particular – from being able to review the work I had done for my undergraduate thesis. What I wanted was a way to present a complete narrative of my project that was easy to understand and follow. For this, I turned to Jupyter Notebook.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nJupyter notebooks combine Markdown text, code, and visualisations. The notebook itself can sit within an online directory of folders and files that contain all the data and code related to a project, allowing readers to understand the processes behind the work and also access the raw resources. From the notebook I produced, readers can see exactly what I did, how I did it, and what my results were.\nWhile creating my notebook, I was able to experiment with my code and iterate quickly. Code cells within a document can be run interactively, which allowed me to try out different approaches to solving a problem and see the results almost in real time. I could also get feedback from others and try out new ideas without having to spend a lot of time writing and debugging code.\n\n\nVersion control with Git and GitHub\nMy Jupyter notebook and associated folders and files are all available via GitHub. Git is a version control system that allows you to keep track of changes to your code over time, while GitHub is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code and collaborate with others without the risk of losing any work. I really couldn’t afford to redo the entire year I spent on my dissertation!\nGit and GitHub are great for reproducibility. By sharing code via these platforms, others can access your work, verify it and reproduce your results without risking changing or, worse, destroying your work – whether partially or completely. These tools also make it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easy to keep track of the different versions of your code and to see how your work has evolved.\nThe following illustration shows the tracking of very simple changes in a Python file. The previous version of the code is shown on the left; the new version is shown on the right. Additions and deletions are highlighted in green and red, and with + and - symbols, respectively.\n\n\nA simple example of GitHub version tracking."
  },
  {
    "objectID": "eventos/posts/2024/road-to-reproducible-research.html#the-deep-learning-challenge",
    "href": "eventos/posts/2024/road-to-reproducible-research.html#the-deep-learning-challenge",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "The deep learning challenge",
    "text": "The deep learning challenge\nSo far, this article has dealt with barriers to reproducibility – and ways around them – that will apply to most, if not all, modern research projects. While I’d encourage any scientist to adopt these practices in their own work, it is important to stress that these alone cannot guarantee reproducibility. In cases where standard statistical procedures are used within statistical software packages, reproducibility is often achievable. However, in reality, even when following the same procedures, differences in outputs can occur, and identifying the reasons for this may be challenging. Cooking offers a simple analogy: subtle changes in room temperature or ingredient quality from one day to the next can impact the final product.\nOne of the challenges for research projects employing machine learning and deep learning algorithms is that outputs can be influenced by the randomness that is inherent in these approaches. Consider the four portraits below, generated by the Midjourney bot.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nEach portrait looks broadly similar at first glance. However, upon closer inspection, critical differences emerge. These differences arise because deep learning models rely on numerous interconnected layers to learn intricate patterns and representations. Slight random perturbations, such as initial parameter values or changes in data samples, can propagate through the network, leading to different decisions during the learning process. As a result, even seemingly negligible randomness can amplify and manifest as considerable differences in the final output, as with the distinct features of the portraits.\nRandomness is not necessarily a bad thing – it mitigates overfitting and helps predictions to be generalised. However, it does present an additional barrier to reproducibility. If you cannot get the same results using the same raw materials – data, code, packages and computing environment – then you might have good reasons to doubt the validity of the findings.\nThere are many elements of an analysis in which randomness may be present and lead to different results. For example, in a classification (where your dependent variable is binary, e.g., success/failure with 1 and 0) or a regression (where your dependent variable is continuous, e.g., temperature measurements of 10.1°C, 2.8°C, etc.), you might need to split your data into training and testing sets. The training set is used to estimate the model (hyper)parameters and the testing set is used to compute the performance of the model. The way the split is usually operationalised is as a random selection of rows of your data. So, in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may therefore lead to different values of the model (hyper)parameters and affect the predictive performance that is measured from the testing set. Also, differences in the testing set may lead to variations in the predictive performance scores, which in turn lead to potentially different interpretations and, ultimately, decisions if the results are used for that purpose.\nThis aspect of randomness in the training of models is relatively well known. But randomness may hide in other parts of code. One such example is illustrated below. Here, using Python, we set the seed number to 0 using np.random.seed(seed value). The random.seed() function from the package numpy (abbreviated np) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. A seed value is an initial input or starting point used by a pseudorandom number generator to generate a sequence of random numbers. It is often an integer or a timestamp. The number generator takes this seed value and uses it to produce a deterministic series of random numbers that appear to be random but can be recreated by using the same seed value. Without providing this seed value, the first execution of the function typically uses the current system time. The animation below generates two random arrays arr1 and arr2 using np.random.rand(3,2). Note that the values 3,2 indicate that we want random values for an array that has 3 rows and 2 columns.\n```{python}\nimport numpy as np\n\n#Set the seed number e.g. to 0\nnp.random.seed(0)\n# Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Set the seed number as before to get the same results\nnp.random.seed(0)\n# Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nIf you run the code yourself multiple times, the values of arr1 and arr2 should remain identical. If this is not the case, check that the seed value is set to 0 in lines 4 and 11. These identical results are possible because we set the seed value to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let’s look at what happens if we remove the line np.random.seed(0):\n```{python}\n#Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nHere, the values of arr1 and arr2 will be different each time we run the code since the seed value was not set and is therefore changing over time.\nThis short code demonstrates how randomness that can be controlled by the seed value may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the seed value will contribute to making your work reproducible. I also find it helpful to document the seed number I use in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see code chunk 9 in the Jupyter notebook) we set the seed value in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
  },
  {
    "objectID": "eventos/posts/2024/road-to-reproducible-research.html#conclusion",
    "href": "eventos/posts/2024/road-to-reproducible-research.html#conclusion",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Conclusion",
    "text": "Conclusion\nWe hope you have enjoyed learning more about our quest for reproducibility. We have explained why reproducibility matters and provided tips for how to achieve it – or, at least, work towards it. We have introduced a few important issues that you are likely to encounter on your own path to reproducibility. In sum, we have mentioned:\n\nThe importance of having relative instead of hard-coded paths in code.\nOperating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment.\nThe convenience of Jupyter notebooks for code editing – particularly useful for data science projects and work using deep learning because of the ability to include text and code in the same document and make the work accessible to everyone (so long as they have an internet connection).\nThe need for version control using, for example, Git and GitHub, which allows you to keep track of changes in your code and collaborate with others efficiently.\nThe importance of setting the seed values in random number generators.\n\nThe graphic below provides a visual overview of the different components of our study and shows how each component works with the others to support reproducibility.\n\nWe use (A) the version control system, Git, and its hosting service, GitHub, which enables a team to share code with peers, efficiently track and synchronise code changes between local and server machines, and reset the project to a working state in case something breaks. Docker containers (B) include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows).\nNow, it is over to you. Our Jupyter notebook provides a walkthrough of our research. Our GitHub repository has all the data, code and other files you need to reproduce our work, and this README file will help you get started.\nAnd with that, we wish you all the best on the road to reproducibility!\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nDavit Svanidze is a master’s degree student in economics at the London School of Economics (LSE). Andre Python is a young professor of statistics at Zhejiang University’s Center for Data Science. Christoph Weisser is a senior data scientist at BASF. Benjamin Säfken is professor of statistics at TU Clausthal. Thomas Kneib is professor of statistics and dean of research at the Faculty of Business and Economic Sciences at Goettingen University. Junfen Fu is professor of pediatrics, chief physician and director of the Endocrinology Department of Children’s Hospital, Zhejiang University, School of Medicine.\n\n\n\n\n\nAcknowledgement\n\nAndre Python has been funded by the National Natural Science Foundation of China (82273731), the National Key Research and Development Program of China (2021YFC2701905) and Zhejiang University global partnership fund (188170-11103).\n\n\n\n\n\nCopyright and licence\n\n© 2023 Davit Svanidze, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu.\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nSvanidze, Davit, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu. 2023. “The road to reproducible research: hazards to avoid and tools to get you there safely.” Real World Data Science, June 15, 2023. URL"
  },
  {
    "objectID": "eventos/posts/2024/road-to-reproducible-research.html#footnotes",
    "href": "eventos/posts/2024/road-to-reproducible-research.html#footnotes",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "References",
    "text": "References\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–1227.↩︎\nIoannidis, John P. A., Sander Greenland, Mark A. Hlatky, Muin J. Khoury, Malcolm R. Macleod, David Moher, Kenneth F. Schulz, and Robert Tibshirani. 2014. “Increasing Value and Reducing Waste in Research Design, Conduct, and Analysis.” The Lancet 383 (9912): 166–175.↩︎\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716.↩︎\nBaker, Monya. 2016. “Reproducibility Crisis?” Nature 533 (26): 353–366.↩︎\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Gideon Nave, Brian A. Nosek, Thomas Pfeiffer, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science between 2010 and 2015.” Nature Human Behaviour 2: 637–644.↩︎"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contacto",
    "section": "",
    "text": "coordinacionfdaseio@gmail.com"
  },
  {
    "objectID": "contact.html#email",
    "href": "contact.html#email",
    "title": "Contacto",
    "section": "",
    "text": "coordinacionfdaseio@gmail.com"
  },
  {
    "objectID": "contact.html#github",
    "href": "contact.html#github",
    "title": "Contacto",
    "section": "Github",
    "text": "Github\n@fda.seio"
  },
  {
    "objectID": "contact.html#researchgate",
    "href": "contact.html#researchgate",
    "title": "Contacto",
    "section": "ResearchGate",
    "text": "ResearchGate\nLab"
  },
  {
    "objectID": "about-rwds.html",
    "href": "about-rwds.html",
    "title": "Bienvenido a FDA-SEIO",
    "section": "",
    "text": "Es un conjunto de técnicas estadísticas desarrolladas con objeto de resolver problemas reales en los que los datos tratados son curvas que proceden de la observación de una variable aleatoria con valores en un espacio de dimensión infinita. El caso más conocido es el de los procesos estocásticos, cuyas realizaciones son funciones dependientes del tiempo."
  },
  {
    "objectID": "about-rwds.html#qué-es-el-análisis-de-datos-funcionales",
    "href": "about-rwds.html#qué-es-el-análisis-de-datos-funcionales",
    "title": "Bienvenido a FDA-SEIO",
    "section": "",
    "text": "Es un conjunto de técnicas estadísticas desarrolladas con objeto de resolver problemas reales en los que los datos tratados son curvas que proceden de la observación de una variable aleatoria con valores en un espacio de dimensión infinita. El caso más conocido es el de los procesos estocásticos, cuyas realizaciones son funciones dependientes del tiempo."
  },
  {
    "objectID": "about-rwds.html#historia-del-grupo",
    "href": "about-rwds.html#historia-del-grupo",
    "title": "Bienvenido a FDA-SEIO",
    "section": "Historia del grupo",
    "text": "Historia del grupo\nLa creación del grupo Análisis de Datos Funcionales de la SEIO fue aprobada en el Consejo Ejecutivo de la SEIO que tuvo lugar en Murcia en febrero de 2009 durante la celebración del XXXI Congreso Nacional de Estadística e Investigación Operativa. Dicha formación fue impulsada por la profesora Ana María Aguilera del Pino (Universidad de Granada) y respaldada por:\n\nAna María Aguilera del Pino (Universidad de Granada)\nJuan Antonio Cuesta Albertos (Universidad de Cantabria)\nAntonio Cuevas González (Universidad Autónoma de Madrid)\nPedro Delicado Useros (Universidad Politécnica de Cataluña)\nManuel Escabias Machuca (Universidad de Granada)\nManuel Febrero Bande (Universidad de Santiago de Compostela)\nPedro Galeano San Miguel (Universidad Carlos III)\nWenceslao González Mantenga (Universidad de Santiago de Compostela)\nFrancisco Antonio Ocaña Lara (Universidad de Granada)\nPaula Rodríguez Bouzas (Universidad de Granada)\nJuan Romo Urroz (Universidad Carlos III de Madrid)\nMariano José Valderrama Bonnet (Universidad de Granada)\n\nTras la consolidación del grupo, el profesor Pedro Delicado Useros (Universitat Politècnica de Catalunya) tomó el cargo de coordinador en el año 2013. En 2015, le sucedió la profesora Carmen Aguilera Morillo (Universidad Carlos III de Madrid). Bajo esta coordinación se creó el I International Workshop in Advances on Functional Data Analysis, dando lugar desde entonces y hasta la fecha a sucesivos encuentros bajo este formato en el que se fomenta no sólo el contacto con figuras internacionales expertas en la materia sino también aumentar la visibilidad de jóvenes promesas en la materia. En 2018 y hasta 2022, la coordinación estuvo a cargo de Paula Navarro Esteban, profesora de la Universidad de Cantabria. Desde 2022 hasta la actualidad la coordinación está a cargo de Antonio Elías (Universidad de Málaga) y Luis Alberto Rodríguez (Universidad de Gottigen)."
  },
  {
    "objectID": "about-rwds.html#lineas-de-trabajo",
    "href": "about-rwds.html#lineas-de-trabajo",
    "title": "Bienvenido a FDA-SEIO",
    "section": "Lineas de trabajo",
    "text": "Lineas de trabajo\n\nModelos de regresión lineal funcional\nModelos lineales generalizados funcionales\nRegresión PLS funcional\nInferencia y predicción en procesos Hilbertianos autoregresivos\nProcesos de Poisson con intensidad aleatoria\nImplementación de métodos computacionales en FDA\nAnálisis en componentes principales funcional\nAnálisis de correlaciones canónicas funcional\nClasificación de datos funcionales\nAnálisis no paramétrico de datos funcionales\nMedidas de profundidad funcional"
  },
  {
    "objectID": "eventos/posts/2025/workshop_madrid.html",
    "href": "eventos/posts/2025/workshop_madrid.html",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "",
    "text": "Reproducibility, or “the ability of a researcher to duplicate the results of a prior study using the same materials as the original investigator”, is critical for sharing and building upon scientific findings. Reproducibility not only verifies the correctness of processes leading to results but also serves as a prerequisite for assessing generalisability to other datasets or contexts. This we refer to as replicability, or “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”. Reproducibility, which is the focus of our work here, can be challenging – especially in the context of deep learning. This article, and associated material, aims to provide practical advice for overcoming these challenges.\nOur story begins with Davit Svanidze, a master’s degree student in economics at the London School of Economics (LSE). Davit’s efforts to make his bachelor’s thesis reproducible are what inspires this article, and we hope that readers will be able to learn from Davit’s experience and apply those learnings to their own work. Davit will demonstrate the use of Jupyter notebooks, GitHub, and other relevant tools to ensure reproducibility. He will walk us through code documentation, data management, and version control with Git. And, he will share best practices for collaboration, peer review, and dissemination of results.\nDavit’s story starts here, but there is much more for the interested reader to discover. At certain points in this article, we will direct readers to other resources, namely a Jupyter notebook and GitHub repository which contain all the instructions, data and code necessary to reproduce Davit’s research. Together, these components offer a comprehensive overview of the thought process and technical implementation required for reproducibility. While there is no one-size-fits-all approach, the principles remain consistent."
  },
  {
    "objectID": "eventos/posts/2025/workshop_madrid.html#davits-journey-towards-reproducibility",
    "href": "eventos/posts/2025/workshop_madrid.html#davits-journey-towards-reproducibility",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Davit’s journey towards reproducibility",
    "text": "Davit’s journey towards reproducibility\n\nMore power, please\nThe focus of my bachelor’s thesis was to better understand the initial spread of Covid-19 in China using deep learning algorithms. I was keen to make my work reproducible, but not only for my own sake. The “reproducibility crisis” is a well-documented problem in science as a whole,1 2 3 4 with studies suggesting that around one-third of social science studies published between the years 2010 and 2015 in top journals like Nature and Science could not be reproduced.5 Results that cannot be reproduced are not necessarily “wrong”. But, if findings cannot be reproduced, we cannot be sure of their validity.\nFor my own research project, I gathered all data and started working on my computer. After I built the algorithms to train the data, my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long, and I needed a faster, more powerful solution to be able to submit my thesis in time. Fortunately, I could access the university server to train the algorithms. Once the training was complete, I could generate the results on my local computer, since producing maps and tables was not so demanding. However…\n\n\nBloody paths!\nIn switching between machines and computing environments, I soon encountered an issue with my code: the paths, or file directory locations, for the trained algorithms had been hardcoded! As I quickly discovered, hardcoding a path can lead to issues when the code is run in a different environment, as the path might not exist in the new environment.\nAs my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake – which would have been easily corrected if spotted earlier – resulted in incorrect outputs. Such errors could have enormous (negative) implications in a public health context, where evidence-based decisions have real impacts on human lives. It was at this point that I realised that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if they are not able to verify it?\nThe following dummy code demonstrates the hardcoding issue:\n```{python}\n# Hardcoded path\nfile_path = \"/user/notebooks/toydata.csv\"\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nIn the code above, a dummy file (toydata.csv) is used. The dummy file contains data on the prices of three different toys, but only the path of the file is relevant to this example. If the hardcoded file path – \"/user/notebooks/toydata.csv\" – exists on the machine being used, the code will run just fine. But, when run in a different environment without said path, the code will result in a \"File not found error\". Better code that uses relative paths can be written as:\n```{python}\n# Relative path\nimport os\n\nfile_path = os.path.join(os.getcwd(), \"toydata.csv\")\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nYou can see that this code has successfully imported data from the dataset toydata.csv and printed its two columns (toy and price) and three rows.\nThe following example is a simplified version of what happened when I wrote code to train several models, store the results and run a procedure to compare results with the predictive performance of a benchmark model:\n```{python}\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)\n```\n```{python}\n# Load the model result and compare with benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model\nresult = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nEverything looks fine at a glance. But, if you examine the code carefully, you may spot the problem. Initially, when I coded the procedure (training the model, saving and loading the results), I hardcoded the paths and had to change them for each tested model. First, I trained model2, a complex model, and tested it against the benchmark (70 &gt; 50 → accepted). I repeated the procedure for model1 (a simple model). Its result was identical to model2, therefore I kept model1 following the parsimony principle.\nHowever, for the code line loading the result for the current model (line 5, second cell), I forgot to amend the path and so mistakenly loaded the result of model2. As a consequence, I accepted a model which should have been rejected. These wrong results were then spread further in the code, including all charts and maps and the conclusions of my analysis.\nA small coding error like this can therefore be fatal to an analysis. Below is the corrected code:\n```{python}\nimport os\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details (INCLUDING PATHS) in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": os.path.join(os.getcwd(), \"results-of-model1.csv\")}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": os.path.join(os.getcwd(), \"results-of-model2.csv\")}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv(current_model[\"path\"], index=False)\n```\n```{python}\n# Get the model result and compare with the benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model WITH a VARIABLE PATH\nresult = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nHere, the paths are stored with other model details (line 7–8, first cell). Therefore, we can use them as variables when we need them (e.g., line 16, first cell, and line 5, second cell). Now, when the current model is set to model1 (line 11, first cell), everything is automatically adjusted. Also, if the path details need to be changed, we only need to change them once and everything else is automatically adjusted and updated. The code now correctly states that model1 performs worse than the benchmark and is therefore rejected and we should keep model2, which performs best.\nI managed to catch this error in time, but it often can be difficult to spot our own mistakes. That is why making code available to others is crucial. A code review by a second (or third) pair of eyes can save everyone a lot of time and avoid spreading incorrect results and conclusions.\n\n\nSolving compatibility chaos with Docker\nOne might think that it would be easy to copy code from one computer to another and run it without difficulties, but it turns out to be a real headache. Different operating systems on my local computer and the university server caused multiple compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, the server did not support the Python programming language – and all the deep learning algorithm packages that I needed – in the same way as my macOS computer did.\nAs a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed. This way, I could integrate them with different hardware and use the processing power of that hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. In fact, I found the Docker website very helpful, with lots of resources and tutorials available. Once Docker was installed, it was easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they could also make it easier for others to reproduce my work.\nBelow is an example of a Dockerfile which recreates an environment with Python 3.7 on Linux. It describes what, how, when and in which order operations should be carried out to generate the environment with all Python packages required to run the main Python script, main.py.\n\n\nAn example of a Dockerfile.\n\nIn this example, by downloading the project, including the Dockerfile, anyone can run main.py without installing packages or worrying about what OS was used for development or which Python version should be installed. You can view Docker as a great robot chef: show it a recipe (Dockerfile), provide the ingredients (project files), push the start button (to build the container) and wait to sample the results.\n\n\nWhy does nobody check your code?\nEven after implementing Docker, I still faced another challenge to reproducibility: making the verification process for my code easy enough that it could be done by anyone, without them needing a degree in computer science! Increasingly, there is an expectation for researchers to share their code so that results can be reproduced, but there are as yet no widely accepted or enforced standards on how to make code readable and reusable. However, if we are to embrace the concept of reproducibility, we must write and publish code under the assumption that someone, somewhere – boss, team member, journal reviewer, reader – will want to rerun our code. And, if we expect that someone will want to rerun our code (and hopefully check it), we should ensure that the code is readable and does not take too long to run.\nIf your code does take too long to run, some operations can often be accelerated – for example, by reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using PyTorch). Aim for a running time of a few minutes – or about as long as it takes to make a cup of tea or coffee. Of course, if data needs to be reduced to save computational time, the person rerunning your code won’t generate the same results as in your original analysis. This therefore will not lead to reproducibility, sensu stricto. However, as long as you state clearly what are the expected results from the reduced dataset, your peers can at least inspect your code and offer feedback, and this marks a step towards reproducibility.\nWe should also make sure our code is free from bugs – both the kind that might lead to errors in analysis and also those that stop the code running to completion. Bugs can occur for various reasons. For example, some code chunks written on a Windows machine may not properly execute on a macOS machine because the former uses \\ for file paths, while the latter uses /:\n```{python}\n# Path works on macOS/Linux\nwith open(\"../../all/notebooks/toydata.csv\", \"r\") as f:\n    print(f.read())\n\n# Path works only on Windows    \nwith open(r\"..\\..\\all\\notebooks\\toydata.csv\", \"r\") as f:\n   print(f.read())\n```\n\nHere, only the macOS/Linux version works, since the code this capture was taken from was implemented on a Linux server. There are alternatives, however. The code below works on macOS, Linux, and also Windows machines:\n```{python}\nfrom pathlib import Path\n\n# Path works on every OS: macOS/Linux/Windows\n# It will automatically replace the path to \"..\\..\\all\\notebooks\\toydata.csv\" when it runs on Windows\nwith open(Path(\"../../all/notebooks/toydata.csv\"), \"r\") as f:\n    print(f.read())\n```\n\nThe extra Python package, pathlib, is of course unnecessary if you build a Docker container for your project, as discussed in the previous section.\n\n\nJupyter, King of the Notebooks\nBy this stage in my project, I was feeling that I’d made good progress towards ensuring that my work would be reproducible. I’d expended a lot of effort to make my code readable, efficient, and also absent of bugs (or, at least, this is what I was hoping for). I’d also built a Docker container to allow others to replicate my computing environment and rerun the analysis. Still, I wanted to make sure there were no barriers that would prevent people – my supervisors, in particular – from being able to review the work I had done for my undergraduate thesis. What I wanted was a way to present a complete narrative of my project that was easy to understand and follow. For this, I turned to Jupyter Notebook.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nJupyter notebooks combine Markdown text, code, and visualisations. The notebook itself can sit within an online directory of folders and files that contain all the data and code related to a project, allowing readers to understand the processes behind the work and also access the raw resources. From the notebook I produced, readers can see exactly what I did, how I did it, and what my results were.\nWhile creating my notebook, I was able to experiment with my code and iterate quickly. Code cells within a document can be run interactively, which allowed me to try out different approaches to solving a problem and see the results almost in real time. I could also get feedback from others and try out new ideas without having to spend a lot of time writing and debugging code.\n\n\nVersion control with Git and GitHub\nMy Jupyter notebook and associated folders and files are all available via GitHub. Git is a version control system that allows you to keep track of changes to your code over time, while GitHub is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code and collaborate with others without the risk of losing any work. I really couldn’t afford to redo the entire year I spent on my dissertation!\nGit and GitHub are great for reproducibility. By sharing code via these platforms, others can access your work, verify it and reproduce your results without risking changing or, worse, destroying your work – whether partially or completely. These tools also make it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easy to keep track of the different versions of your code and to see how your work has evolved.\nThe following illustration shows the tracking of very simple changes in a Python file. The previous version of the code is shown on the left; the new version is shown on the right. Additions and deletions are highlighted in green and red, and with + and - symbols, respectively.\n\n\nA simple example of GitHub version tracking."
  },
  {
    "objectID": "eventos/posts/2025/workshop_madrid.html#the-deep-learning-challenge",
    "href": "eventos/posts/2025/workshop_madrid.html#the-deep-learning-challenge",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "The deep learning challenge",
    "text": "The deep learning challenge\nSo far, this article has dealt with barriers to reproducibility – and ways around them – that will apply to most, if not all, modern research projects. While I’d encourage any scientist to adopt these practices in their own work, it is important to stress that these alone cannot guarantee reproducibility. In cases where standard statistical procedures are used within statistical software packages, reproducibility is often achievable. However, in reality, even when following the same procedures, differences in outputs can occur, and identifying the reasons for this may be challenging. Cooking offers a simple analogy: subtle changes in room temperature or ingredient quality from one day to the next can impact the final product.\nOne of the challenges for research projects employing machine learning and deep learning algorithms is that outputs can be influenced by the randomness that is inherent in these approaches. Consider the four portraits below, generated by the Midjourney bot.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nEach portrait looks broadly similar at first glance. However, upon closer inspection, critical differences emerge. These differences arise because deep learning models rely on numerous interconnected layers to learn intricate patterns and representations. Slight random perturbations, such as initial parameter values or changes in data samples, can propagate through the network, leading to different decisions during the learning process. As a result, even seemingly negligible randomness can amplify and manifest as considerable differences in the final output, as with the distinct features of the portraits.\nRandomness is not necessarily a bad thing – it mitigates overfitting and helps predictions to be generalised. However, it does present an additional barrier to reproducibility. If you cannot get the same results using the same raw materials – data, code, packages and computing environment – then you might have good reasons to doubt the validity of the findings.\nThere are many elements of an analysis in which randomness may be present and lead to different results. For example, in a classification (where your dependent variable is binary, e.g., success/failure with 1 and 0) or a regression (where your dependent variable is continuous, e.g., temperature measurements of 10.1°C, 2.8°C, etc.), you might need to split your data into training and testing sets. The training set is used to estimate the model (hyper)parameters and the testing set is used to compute the performance of the model. The way the split is usually operationalised is as a random selection of rows of your data. So, in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may therefore lead to different values of the model (hyper)parameters and affect the predictive performance that is measured from the testing set. Also, differences in the testing set may lead to variations in the predictive performance scores, which in turn lead to potentially different interpretations and, ultimately, decisions if the results are used for that purpose.\nThis aspect of randomness in the training of models is relatively well known. But randomness may hide in other parts of code. One such example is illustrated below. Here, using Python, we set the seed number to 0 using np.random.seed(seed value). The random.seed() function from the package numpy (abbreviated np) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. A seed value is an initial input or starting point used by a pseudorandom number generator to generate a sequence of random numbers. It is often an integer or a timestamp. The number generator takes this seed value and uses it to produce a deterministic series of random numbers that appear to be random but can be recreated by using the same seed value. Without providing this seed value, the first execution of the function typically uses the current system time. The animation below generates two random arrays arr1 and arr2 using np.random.rand(3,2). Note that the values 3,2 indicate that we want random values for an array that has 3 rows and 2 columns.\n```{python}\nimport numpy as np\n\n#Set the seed number e.g. to 0\nnp.random.seed(0)\n# Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Set the seed number as before to get the same results\nnp.random.seed(0)\n# Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nIf you run the code yourself multiple times, the values of arr1 and arr2 should remain identical. If this is not the case, check that the seed value is set to 0 in lines 4 and 11. These identical results are possible because we set the seed value to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let’s look at what happens if we remove the line np.random.seed(0):\n```{python}\n#Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nHere, the values of arr1 and arr2 will be different each time we run the code since the seed value was not set and is therefore changing over time.\nThis short code demonstrates how randomness that can be controlled by the seed value may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the seed value will contribute to making your work reproducible. I also find it helpful to document the seed number I use in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see code chunk 9 in the Jupyter notebook) we set the seed value in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
  },
  {
    "objectID": "eventos/posts/2025/workshop_madrid.html#conclusion",
    "href": "eventos/posts/2025/workshop_madrid.html#conclusion",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Conclusion",
    "text": "Conclusion\nWe hope you have enjoyed learning more about our quest for reproducibility. We have explained why reproducibility matters and provided tips for how to achieve it – or, at least, work towards it. We have introduced a few important issues that you are likely to encounter on your own path to reproducibility. In sum, we have mentioned:\n\nThe importance of having relative instead of hard-coded paths in code.\nOperating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment.\nThe convenience of Jupyter notebooks for code editing – particularly useful for data science projects and work using deep learning because of the ability to include text and code in the same document and make the work accessible to everyone (so long as they have an internet connection).\nThe need for version control using, for example, Git and GitHub, which allows you to keep track of changes in your code and collaborate with others efficiently.\nThe importance of setting the seed values in random number generators.\n\nThe graphic below provides a visual overview of the different components of our study and shows how each component works with the others to support reproducibility.\n\nWe use (A) the version control system, Git, and its hosting service, GitHub, which enables a team to share code with peers, efficiently track and synchronise code changes between local and server machines, and reset the project to a working state in case something breaks. Docker containers (B) include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows).\nNow, it is over to you. Our Jupyter notebook provides a walkthrough of our research. Our GitHub repository has all the data, code and other files you need to reproduce our work, and this README file will help you get started.\nAnd with that, we wish you all the best on the road to reproducibility!\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nDavit Svanidze is a master’s degree student in economics at the London School of Economics (LSE). Andre Python is a young professor of statistics at Zhejiang University’s Center for Data Science. Christoph Weisser is a senior data scientist at BASF. Benjamin Säfken is professor of statistics at TU Clausthal. Thomas Kneib is professor of statistics and dean of research at the Faculty of Business and Economic Sciences at Goettingen University. Junfen Fu is professor of pediatrics, chief physician and director of the Endocrinology Department of Children’s Hospital, Zhejiang University, School of Medicine.\n\n\n\n\n\nAcknowledgement\n\nAndre Python has been funded by the National Natural Science Foundation of China (82273731), the National Key Research and Development Program of China (2021YFC2701905) and Zhejiang University global partnership fund (188170-11103).\n\n\n\n\n\nCopyright and licence\n\n© 2023 Davit Svanidze, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu.\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nSvanidze, Davit, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu. 2023. “The road to reproducible research: hazards to avoid and tools to get you there safely.” Real World Data Science, June 15, 2023. URL"
  },
  {
    "objectID": "eventos/posts/2025/workshop_madrid.html#footnotes",
    "href": "eventos/posts/2025/workshop_madrid.html#footnotes",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "References",
    "text": "References\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–1227.↩︎\nIoannidis, John P. A., Sander Greenland, Mark A. Hlatky, Muin J. Khoury, Malcolm R. Macleod, David Moher, Kenneth F. Schulz, and Robert Tibshirani. 2014. “Increasing Value and Reducing Waste in Research Design, Conduct, and Analysis.” The Lancet 383 (9912): 166–175.↩︎\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716.↩︎\nBaker, Monya. 2016. “Reproducibility Crisis?” Nature 533 (26): 353–366.↩︎\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Gideon Nave, Brian A. Nosek, Thomas Pfeiffer, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science between 2010 and 2015.” Nature Human Behaviour 2: 637–644.↩︎"
  }
]