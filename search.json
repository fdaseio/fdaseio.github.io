[
  {
    "objectID": "case-studies/posts/2023/08/21/00-food-for-thought.html",
    "href": "case-studies/posts/2023/08/21/00-food-for-thought.html",
    "title": "The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy",
    "section": "",
    "text": "There’s a saying: “You are what you eat.” Its meaning is somewhat open to interpretation, as with many such sayings, but it is typically used to make the point that if you want to be well, you need to eat well. Nutrition scientists and dieticians spend their careers trying to figure out what “eating well” looks like – the foods the human body needs, in what quantities, and how best to consume them. Their research informs advice and guidance issued by health professionals and governments. Ultimately, though, the choice of what to eat falls to us – individuals and families – and our choices are often determined by our tastes, the availability of foodstuffs in our local stores, their price and affordability.\nSo, what exactly do we eat? Answers come from a variety of sources. In the United States, there are dietary recall studies such as the National Health and Nutrition Examination Survey, which asks a sample of respondents to report their food and beverage consumption over a set period of time. There are also organisations like IRI that collect point-of-sale data from retail stores on the actual food and drink being sold to consumers. By and large, this information comes from barcodes on product packaging being scanned at checkouts, so it is often referred to as “scanner data”.\nThis data – from dietary recall studies and retail scanners – is valuable: once we know what people are eating, we can check the nutritional content of those foods and build up a picture of what the diet of a typical individual or family looks like and how it compares to the diet recommended by doctors and policymakers. And, if we know what other foodstuffs are available, how much they cost, and the nutritional value of those items, we can work out how much families need to spend, and on what, in order to eat well and, hopefully, be well.\nFiguring all this out is where something called the Purchase to Plate Crosswalk (PPC) comes in. It’s a key tool for understanding the “healthfulness of retail food purchases” and it does this by linking IRI scanner data on what people buy with data on the nutritional content of those foods, as recorded in the US Department of Agriculture’s Food and Nutrient Database for Dietary Studies (FNDDS). But there’s a catch: scanner data is collected about hundreds of thousands of food products, whereas the FNDDS has nutritional profile information for only a few thousand items. Linking these two datasets therefore gives rise to a one-to-many matching problem – a problem that takes several hundred person-hours to resolve.\nWhat if machine learning can help? That question inspired a competition, the Food for Thought Challenge, organized by the Coleridge Initiative, a nonprofit organization working with governments to ensure that data are more effectively used for public decision-making. Researchers and data scientists were invited to use machine learning and natural language processing to more efficiently link data on supermarket products to nutrient databases.\nThis collection of articles tells the story of the Food for Thought Challenge. We begin by exploring the policy issues that drive the development of the PPC – the need to understand the national diet, developing healthy diet plans, and costing up those plans – and the issues posed by record linkage. Next, we learn about the nature of the challenge and the structure of the competition in more detail, and then the three winning teams walk us through their solutions. We end the collection with some closing thoughts on the value of competitions for addressing data scientific challenges in the public sector.\n\n\n\n\nFind more case studies\n\n\n\n\nPart 1: The Purchase to Plate Suite →\n\n\n\n\n\n\n\n\nAbout the authors\n\nBrian Tarran is editor of Real World Data Science, and head of data science platform at the Royal Statistical Society.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative, whose goal is to use data to transform the way governments access and use data for the social good through training programs, research projects and a secure data facility. She recently served on the Advisory Committee on Data for Evidence Building and the National AI Research Resources Task Force.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society and Julia Lane\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Melanie Lim on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian, and Julia Lane. 2023. “The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html",
    "title": "Food for Thought: Competition and challenge design",
    "section": "",
    "text": "Since 2014, the professional services firm Westat, Inc. has been developing the Purchase to Plate Crosswalk (PPC) for the United States Department of Agriculture (USDA) Economic Research Service (ERS). The PPC links the retail food transactions database from IRI’s InfoScan service and the USDA Food and Nutrient Database for Dietary Studies (FNDDS). However, the current linkage process uses only partly automated data matching, meaning it is resource intensive, time consuming, and requires manual review.\nWith sponsorship from ERS, Westat partnered with the Coleridge Initiative to host the Food for Thought competition to challenge researchers and data scientists to use machine learning and natural language processing to find accurate and efficient methods for creating the PPC. Figure 1 provides a visual overview of the challenge set by the competition.\nThe one-to-many matching task that is central to the competition throws up many challenges for researchers to wrestle with. Because IRI data contains food transactions collected from partnered retail establishments for over 350,000 items, the matchings need to be made based on limited data features, including categories, providers, and semantically inconsistent descriptions that consist of short phrases. Consider this hypothetical example: IRI product-related information about a (fictional) “Cheesy Hashbrowns Hamburger Helper, 5.5 Oz Box” needs to be linked to FNDDS nutrition-related information found under “Mixed dishes – meat, poultry, seafood: Mixed meat dishes”. Figure 2 demonstrates how the two databases are linked with each other to create the PPC. As can be seen, there is no common word that easily indicates that “Cheesy Hashbrowns Hamburger Helper…” should be matched with “Mixed dishes…”, and such cases exist in all IRI tables used for the challenge, from 2012 through 2018.\nAlso, because nutritionists or food scientists will always need to review the matching, regardless of the matching method used, it was important that our evaluation of proposed matching methods focused both on the accuracy of prediction models and also on metrics that would lead participants to develop models that facilitate qualified reviewers to reduce their workloads.\nOrganising the competition was also a challenge in its own right, for data privacy reasons. IRI scanner data contains sensitive information, such as store name, location, unit price, and weekly quantity sold for each item. This ruled out using existing online platforms like Kaggle, DrivenData or AIcrowd to host the competition, and instead required a private secure data enclave to ensure the safe use of sensitive and confidential data assets. The need for such an environment imposed capacity constraints on the competition, meaning only dozens of teams could be invited to take part, whereas on open platforms it is common to have thousands of teams competing and sharing ideas and code."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Competition structure",
    "text": "Competition structure\nThe competition ran over 10 months and consisted of three separate challenges: two interim, one final. Applications opened in September 2021, and the competition started in January 2022. Submission deadlines for the first and second interim challenges were in July and September 2022, respectively. For these rounds, participants submitted preliminary solutions for evaluation based solely on quantitative metrics, and two awards of $10,000 were given to the highest-scoring teams. The deadline for the final challenge was in October 2022. Here, solutions were evaluated by the scientific review board based on three judging criteria: quantitative metrics, transferability, and innovation. First, second, and third place winners received awards of $30,000, $1,500, and $1,000 respectively. Final presentations were given at the Food for Thought symposium in December 2022.\nThe competition was run entirely within the Coleridge Initiative’s Administrative Data Research Facility (ADRF), which was established by the United States Census Bureau to inform the decision-making of the Commission on Evidence-Based Policy under the Evidence Act. ADRF follows the Five Safes Framework: safe projects, safe people, safe data, safe settings, and safe outputs.\nIn keeping with this framework, participants were provided with ADRF login credentials after signing the relevant data use agreements during the onboarding process. All participants were required to agree to the ADRF terms of use, to complete security training, and to pass a security training assessment prior to accessing the challenge data. Participants’ access within ADRF was limited to the challenge environment and data only. There was no internet access, so Coleridge Initiative ensured that any packages requested by teams were available for use within the environment after passing security review. All codes and documentation were only allowed to be exported outside ADRF after export reviews from both Coleridge Initiative and USDA staff. At the end of each challenge, the teams submitted write-ups and supporting files by placing all the necessary submission files in their ADRF team folder. Detailed submission instructions are available via the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Metrics",
    "text": "Metrics\nSubmissions were evaluated by Coleridge Initiative and technical review and subject review boards based on the following criteria:\n\nQuantitative metrics were used to measure the predictive accuracy and runtime of the model.\n\nTransferability measured the quality of documentation and code, and the ability of individuals who are not involved in model development to replicate and implement the team’s approach.\n\nInnovation measured novelty and creativity of the model in addressing the linkage problem.\n\nTechnical review was overseen by faculty members from computer science and engineering departments of top US universities. Subject review was handled by subject matter experts from USDA and Westat.\nFrom a quantitative perspective, the most common way to evaluate machine learning competition submissions is to use model predictive accuracy. However, single metrics are typically incomplete descriptions of real-world tasks, and they can easily hide significant differences between models which simple predictive accuracy cannot capture. To select the most appropriate official challenge metrics, Coleridge Initiative reviewed the literature on the use of evaluation measures in both classification and ranking task machine learning competitions. Success at 5 (S@5) and Normalized Discounted Cumulative Gain at 5 (NDCG@5) scores were ultimately used as the quantitative metrics.\nThe metrics were applied as follows: models proposed by each team were tasked with outputting five potential FNDDS matches for each IRI code, with potential FNDDS matches ordered from most likely to least likely. S@5 and NDCG@5 scores are broadly similar – both measure whether a correct match is present in the five proposed matches that participants were asked to identify. However, S@5 does not take rank position into account and only considers whether the five proposed FNDDS matches contain the correct FNDDS response. NDCG@5 does take rank into account and also measures how highly the correct FNDDS response is ranked among the five proposed matches. Both measures range from 0 to 1 (or 0% to 100%). Models get a “full credit” for S@5 as long as they contain the correct FNDDS option. NDCG@5 penalizes models when the correct match is ranked lower on the list of 5 proposed matches."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Technical description",
    "text": "Technical description\n\nEnvironment setup\nColeridge Initiative solicited technical requirements from participants at the challenge application stage to prepare the ADRF environment as much as possible before the competition began. Each team was asked to share anticipated workspace specifications and software library requests in their application package. From this we identified, reviewed, and installed the requested Python and R packages, libraries, and library components (e.g., pre-trained models, training data) that were not yet available within ADRF.\nThe setup of graphics processing units (GPUs) was also a critical part of competition preparation. We created an environment with 16 gibibyte (GiB) of GPU memory for each team. Our technology team met with multiple teams several times to discuss computing environment configurations to ensure the GPU could work properly. None of these efforts was wasted: without GPU access, it would be impossible for teams to use state-of-the-art pre-trained models such as the Bidirectional Encoder Representations from Transformers (BERT, Devlin et al. 2018).\nWe completed the setup of new team workspaces, each customized to the individual team’s resource and library requirements, including GPU configuration. The isolation and customization of workspaces was vital because teams may request different versions of libraries that potentially have version conflict with other libraries. We ensured the configurations were all set before the challenge began because such data challenges are bursty in nature (Macavaney et al. 2021), and handling support requests in the private data enclave risked causing delays. We hoped to avoid receiving too many requests in the beginning phase of the competition in order to give participants a better experience, though we did of course provide participants with instructions on how to request additional libraries during the challenge period.\n\n\nSupporting materials\nIn addition to environment preparation, we made available a list of supporting documentation, including IRI, PPC, and FNDDS codebooks, technical reports, and related publications that could help teams understand the challenge datasets. The FNDDS codebook pooled information on variable availability, coding, and descriptions across dataset files and years. It also included internal Westat food category coding difficulty ratings and notes on created PPC codes and provided UPC code, EC code, and general dataset remarks and observations that may take time for analysts to discover on their own.\nWe developed a baseline model to demonstrate the challenge task and the expected outputs – both outside of ADRF using FNDDS and fictitious data in place of IRI data, and an analogous model using FNDDS and IRI data within the ADRF secure environment. Moreover, we provided the teams with an evaluation script to read in their submissions and evaluate them for predictive accuracy against the public test set using S@5 and NDCG@5 challenge metrics. Finally, we held multiple webinars during the course of the challenge to explain next steps, address participant questions, solicit feedback, and provide general support. Multiple teams also met with our technology team to clarify ADRF-related questions or troubleshoot technical issues.\n(Baseline model, toolkits, and evaluation script are available from the Real World Data Science GitHub repository.)\n\n\nData splitting\nTo mimic the real-world scenario, the competition used 2012–2016 IRI data as the training set, and the 2017–2018 IRI data as the test set, since the data change over time and USDA could provide the most recent data available. To make sure that models were generalizable and not just overfit to the test set, we split the test set into private and public test sets. In this way, we guaranteed that the models were evaluated on completely hidden data. In order to keep the similar distribution of the two sets, we first divided the data into five quintiles based on EC code frequencies and then randomly sampled 80% of records in each group without repetition for placement into the private test set. Later in the competition, because of the computation limit, we further shrank the private test set to 40% of its original size using the same data-splitting method.\n\n\nJudging\nIn the first two rounds, submissions were evaluated based on the quantitative metrics, as previously mentioned above. Coleridge Initiative was responsible for running the evaluation script, making sure not to re-train the model or modify the configs in any way, and only applying the model to predict the private test set. Prediction results were then compared against ground truth to get the private scores.\nThe final challenge was reviewed by the scientific review board on all three judging criteria. Submitted models were first evaluated by Coleridge Initiative in the same way as in the first two rounds. The runtime of models was also recorded as an assessment of model cost. The scientific review boards then assessed the models by the quality of documentation, the quality of code, and the ability to replicate and implement the team’s approach, and scored the models for innovation and creativity in addressing the linkage problem. Lastly, scores were summarized and the scientific review board discussed and decided the winners of the competition."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#results",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#results",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Results",
    "text": "Results\nThe next few articles in this collection walk readers through the solutions proposed by competition finalists. Figure 3 provides a brief summary.\n\n\n\n\n\n\nFigure 3: Top competitors and their solutions to the Food for Thought challenge."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was undoubtedly challenging for teams to work with highly secured data in a private data enclave for this data challenge. We solicited feedback from teams and summarized the issues that we experienced throughout the competitions, together with the solutions to resolve those issues. Below are our main lessons learned and we hope this summary can serve to inform future competitions.\n\nEnvironmental factors: The installation and setup of packages, libraries, and resources, as well as the configuration of GPUs, system dependencies, and workspace design were expected to take a long time as each team had their own needs. To accelerate the process, we requested a list of specific package and environment requirements from the teams in advance. However, due to the complexity of the system configuration required by the teams, environment setup took longer than expected. Thus, the challenge deadlines had to be postponed a few times to accommodate this.\nTime commitment: Twelve teams were selected to participate in the challenge, but only three teams remained in the final challenge. Other than one team that was disqualified for violating the ADRF terms of use agreement, eight dropped out because of other commitments and insufficient time to meaningfully participate. To ensure security, ADRF does not allow jobs to run in the backend, which also adds to the time commitment of teams. To encourage teams to participate in the final challenge, we gave out additional awards for second and third places.\nComputing resource limit: One issue encountered in evaluating submitted models was computing environment resource limits due to the secured nature of the data enclave. The original private test dataset is four times larger than the public test dataset, making it unfeasible to evaluate. To overcome this issue, given the fixed resource constraints, we decided to reduce the private test set to 40% of its original size. It would have been helpful, though, if the competition had set a model running time limit at the outset, so that participants could build simpler yet effective models.\nSupporting code: Although the initial baseline model we provided was extremely simple, we found this helped participants a lot in the initial phase – yet there is space to improve. To be specific, supporting codes should be constructed so that all relevant data tables are used and specify the main function to run the code, especially how the model should be tested. The teams only used the main table, which was the only table that was used in the baseline model, for training and did not touch the other supporting table. If we included the other table in the baseline model, it could help participants to have a better use of this data as well. In addition, a baseline model should be intuitive for the participants to follow, allowing evaluators to easily replace the public test set with the private test set without any programming modifications.\n\n\n\n\n\n← Part 1: Purchase to Plate\n\n\n\n\nPart 3: First place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nZheyuan Zhang and Uyen Le are research scientists at the Coleridge Initiative.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Zheyuan Zhang and Uyen Le\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nZhang, Zheyuan, and Uyen Le. 2023. “Food for Thought: Competition and challenge design.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "",
    "text": "The Auburn Big Data team from Auburn University consists of five members, including three assistant professors: Dr Wenying Li of the Department of Agricultural Economics and Rural Sociology, Dr Jingyi Zheng of the Department of Mathematics and Statistics, and Dr Shubhra Kanti Karmaker of the Department of Computer Science and Software Engineering. Additionally, the team comprises two PhD students, Naman Bansal and Alex Knipper, who are affiliated with Dr Karmaker’s big data lab at Auburn University.\nIt is estimated that our team has spent approximately 1,400 hours on this project."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nAt the start of this competition, we decided to test three general approaches, in the order listed:\n\nA heuristic approach, where we use only the data and a defined similarity metric to predict which FNDDS label a given IRI item should have.\nA simpler modeling approach, where we train a simple statistical classifier, like a random forest (Parmar, Katariya, and Patel 2019), logistic regression, etc., to predict the FNDDS label for a given IRI item. For this method, we opted to use a random forest as our statistical model, as it was a simpler model to use as a baseline, having shown decent performance in a wide range of classification tasks. As it turned out, this approach was quite robust and accurate, so we kept it as our main model for this approach.\nA large language modeling approach, where we train a model like BERT (Devlin et al. 2018) to map the descriptions for given IRI and FNDDS items to the FNDDS category the supplied IRI item belongs to."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our approach",
    "text": "Our approach\nAs we explored the data provided, we opted to use the given 2017–2018 PPC dataset as our primary dataset for both training and testing. To ensure a fair evaluation of the model, we randomly split the dataset into 60% training samples and 40% testing samples, making sure our training process never sees the testing dataset. For evaluating our models, we adopted the competition’s metrics: Success@5 and NDCG@5. After months of testing, our statistical classifier (approach #2) proved itself to be the model that both processes the data fastest and achieves the highest performance on our testing metrics.\nThis approach, at a high level, takes in the provided data (among other configuration parameters), formats the data in a computer-readable format – converting the IRI and FNDDS descriptions to a numerical representation with word embeddings (2018; Mikolov et al. 2013; Pennington, Socher, and Manning 2014) and then using that numerical representation to calculate the distances between each description – and then trains a classification model (random forest (2019)/neural network (Schmidhuber 2015)) that can predict an FNDDS label for a given IRI item.\nIn terms of data, our approach uses the FNDDS/IRI descriptions, combining them into a single “description” field, and the IRI item’s categorical items – department, aisle, category, product, brand, manufacturer, and parent company – to further discern between items.\nWhile most industrial methods require use of a graphics processing unit (graphics card, or GPU) to perform this kind of processing, our primary method only requires the computer’s internal processor (CPU) to function properly. With that in mind, to achieve the best possible performance on our test metrics, the most time-consuming operations are run in parallel. The time taken to train our primary model can likely be further improved if we parallelize these operations across a GPU, with the only downside being the imposition of a GPU requirement for systems aiming to run this method.\nIn addition to our primary method, our team has worked with alternate approaches on the GPU (using BERT (2018), neural networks (2015), etc.) to either: 1) speed up the time it takes to process and make inferences for the data, achieving similar performance on our test metrics, or 2) achieve higher performance, likely at a cost to the time it takes to process everything. Our reasoning behind doing so is that if a simple statistical model performs well, then a larger language model should be able to demonstrate a higher performance on our test metrics without much of an increase in training time. At the current time, these methods are still unable to match the performance/efficiency tradeoff of our primary method.\nAfter exploring alternate methods to no avail, our team then decided to focus again on our primary method, the random forest (2019), and a secondary method, feed-forward neural network mapping our input features (X) to the FNDDS labels (Y) (2015), to optimize their training hyperparameters for the dataset. Our aim in this is to see which of our already-implemented, easier-to-run downstream methods would better optimize the performance/efficiency tradeoff after having its training parameters optimized to the fullest. This has resulted in a marginal increase in training time (+20-30 minutes) and a roughly 5% increase in performance for our still-highest performing model, the random forest.\nOverall, our primary method – the random forest – gave us an approximate training time (including data pre-processing) of 4 hours 30 minutes for our ~38,000 IRI item training set, and an approximate inference time of 15 minutes on our testing set of ~15,000 IRI items. Furthermore, our method gave us a Success@5 score of .789 and an NDCG@5 score of .705 on our testing set.\n\nKey features\nHere is a list of the key features we utilize, along with what type of data we treat it as.\n\nFNDDS\n\nfood_code – identifier\nmain_food_description – text\nadditional_food_description – text\ningredient_description – text\n\nIRI\n\nupc – identifier\nupcdesc – text\ndept – categorical\naisle – categorical\ncategory – categorical\nproduct – categorical\nbrand – categorical\nmanufacturer – categorical\nparent – categorical\n\n\nThe intuition behind using these particular features is that the text-based descriptions provide the majority of the “meaning” of the item. By converting each description to a numerical representation (2013; 2014), we can then calculate the similarity between each “meaning” to determine which FNDDS label is most similar to the IRI item provided. However, that alone is not enough. The categorical features on the IRI item help to further enhance the model’s classifications using the logic and categories people use in places like grocery stores. For example, if given an item whose aisle was “fruit” and brand was “Dole”, the item could be reasonably expected to be something like “peaches” over something like “broccoli”.\n\n\nFeature selection\nAforementioned intuition aside, our feature selection was rather naive, in that we manually examined the data and removed any redundant text features before doing anything else. After that, we decided to use description fields as “text” data to comprise the main “meaning” of the item, represented numerically after converting the text using a word embedding (2013; 2014). We also decided to use the non-description fields (aisle, category, etc.) as “categorical” data that would be turned into its own numerical representation, allowing our model to more easily discern between items using similar systems to people.\n\n\nFeature transformations\nOur feature transformations are also relatively simple. First, we combine all description fields for each item to make one large description, and then use a word embedding method (like GloVe (2014) or BERT (2018)) to convert the description into a numerical representation, resulting in a 300-dimensional GloVe or 768-dimensional BERT vector of numbers for each description. Then, for each IRI item, we calculate the cosine and Euclidean distances from each FNDDS item, resulting in two vectors, both equal in length to the original FNDDS data (in this case, two vectors of length ~7,300). The intuition behind this is that while cosine and Euclidean distances can tell us similar things, providing both of these sets of distances to the model should allow it to pick up on a more nuanced set of relationships between the IRI and FNDDS items.\nFor categorical data, we take all unique values in each field and assign them an ID number. While that is often not the best practice for making a numerical representation out of categorical data (Potdar, Pardawala, and Pai 2017), it seemed to work for the downstream model.\nRegardless, the aforementioned feature transformations give us (ad hoc) ~14,900 features if we use GloVe and ~15,300 features if we use BERT. Both feature sets can then be sent to the downstream random forest/neural network to start classifying items.\nIt should be noted that processing the data is by far the most time-consuming part of our method. The data processing times for each embedding are as follows:\n\nGloVe: ~3 hours\nBERT: ~6 hours\n\nDue to BERT both taking so long to process data and performing lower than our GloVe embeddings on the classification task, we opt to use GloVe embeddings for our primary method. Our only theoretical explanation here is that since BERT is better at context-dependent tasks (Wang, Nulty, and Lillis 2021), it likely will expect something similar to well-structured sentences as input, which is not what the IRI/FNDDS descriptions are. Rather, GloVe – being a method that depends less on context (2013; 2014) – should excel better when the input text is not a well-formed sentence.\n\n\nTraining methods\nOnce the data has been processed, we collect the following data for each IRI item:\n\nUPC code\nDescription (converted to numerical representation)\nCategorical variables (converted to numerical representation)\nDistances to each FNDDS item\n\nOnce that has been collected for each IRI item, we can finally use our classification model. We initialize our model and begin the training process with the IRI data mentioned above and the target FNDDS labels for each one, so the model knows what the “correct” answer is for the given data. Once the model has trained on our training dataset, we save the model and it is ready for use.\nThis part of training takes much less time than preparing the data, since calculating the embeddings takes a lot more computation than a random forest model. The training times for each method are as follows:\n\nRandom Forest: ~1 hour 15 minutes\nNeural Network: ~25 minutes\n\nDespite the neural network taking far less time to train than the random forest, it still scores lower on the scoring metrics than the random forest, so we opt to continue using the random forest model as our primary method.\n\n\nGeneral approach to developing the model\nSince the linkage problem involves mapping tens of thousands of items to a smaller category set of a few thousand items, we decided to frame this problem as a multi-class classification problem (Aly 2005), where we then rank the top “k” most probable class mappings, as requested by the competition ruleset.\nMost of the usable data available to us is text data, so we need a method that can use that text-based information to accurately map classes based on the aforementioned text information. To best accomplish this, we opt to use word embedding techniques to calculate an average numerical representation for each text description (both IRI and FNDDS), so we can calculate distances between each description, giving our model a sense of how similar each description is.\n\n\nThe key “trick” to the model\nSince text descriptions hold the most information that can be used to link between an IRI item and an FNDDS item, finding a way to calculate the similarity between each description is paramount to making this method work.\nBoth distance calculation methods used in this work, cosine and Euclidean distance, are very similar in the type of information encoded, the only major difference being that cosine distance is implicitly normalized and Euclidean distance is not (Qian et al. 2004).\n\n\nNotable observations\nJust by building the ranking using the cosine similarities between each IRI item and all FNDDS items, we can achieve a Success@5 performance of 0.234 and an NDCG@5 performance of 0.312. The other features are provided and the random forest classifier is used to add some extra discriminative power to the model.\n\n\nData disclaimer\nOur current method only uses the data readily available from the 2017–2018 dataset, which we acknowledge is intended for testing. To remedy this, we further split this dataset into train/test sets and report results on our unseen test subset for our primary performance metrics. This gives a decent look into how the model will perform on unseen data.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our results",
    "text": "Our results\n\nApproximate training time\nOverall, our approximate training time for our primary method is 4 hours 30 minutes broken down (approximately) as follows:\n\nReading data from database: 30 seconds\nCalculating ~7,300 FNDDS description embeddings: 15 minutes 45 seconds\nCalculating ~38,000 IRI description embeddings and similarity scores: 2 hours 20 minutes 45 seconds\nFormatting calculated data for the random forest classifier: 35 minutes\nTraining the random forest classifier: 1 hour 15 minutes\n\n\n\nApproximate inference time\nOur approximate inference time for our primary method is 15 minutes to make inferences for ~15,000 IRI items.\n\n\nS@5 & NDCG@5 performance\nThis is how our best-performing model (GloVe + random forest) performs at the current time on the testing set:\n\nNDCG@5: 0.705\nSuccess@5: 0.789\n\nWhen we evaluate that same model on the full PPC dataset we were provided (~38,000 items), we get the following scores:\n\nNDCG@5: 0.879\nSuccess@5: 0.916\n\n(Note: The full PPC dataset contains approximately 15,000 items that we used to train the model, so these scores are not as representative of our method’s performance as the previous scores.)"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nAs mentioned previously, we only used the given 2017–2018 PPC dataset as our primary dataset for both training and testing. Going forward, we would like to include datasets from previous years as well, which we believe would further increase our model performance. Additionally, the datasets generated from this research have the potential to inform and support additional studies from a variety of perspectives, including nutrition, consumer research, and public health. Further research utilizing these datasets has the potential to make significant contributions to our understanding of consumer behavior and the role of food and nutrient consumption in overall health and well-being."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was interesting that the random forest model performed better than the vanilla neural network model. This shows that a simple solution can work better, depending on the application. This observation is in line with the well-established principle in machine learning that the choice of model should be guided by the nature of the problem and the characteristics of the data. In this case, the random forest model, being a simpler and more interpretable model, was better suited to the problem at hand and was able to outperform the more complex neural network model. These results underscore the importance of careful model selection and the need to consider both the complexity of the model and the specific requirements of the problem when choosing an algorithm for a particular application.\n\n\n\n\n← Part 2: Competition design\n\n\n\n\nPart 4: Second place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAlex Knipper and Naman Bansal are PhD students, and Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker are assistant professors at Auburn University.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by nrd on Unsplash.\n\n\n\nHow to cite\n\nKnipper, Alex, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker. 2023. “Food for Thought: First place winners – Auburn Big Data.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "",
    "text": "Disclaimer\n\n\n\nThe findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA or US Government determination or policy. This research was supported by the US Department of Agriculture’s Economic Research Service and Center for Nutrition, Policy and Promotion. Findings should not be attributed to Circana (formerly IRI).\nAbout 600,000 deaths per year in the United States are related to chronic diseases that are linked to poor dietary choices. Many other individuals suffer from diet-related health conditions, which may limit their ability to work, learn, and be physically active (US Department of Agriculture and US Department of Health and Human Services 2020). In recognition of the link between diet and health, in 1974 the Senate Select Committee on Nutrition and Human Needs, originally formed to eliminate hunger, expanded its focus to improving eating habits, nutrition policy and the national diet. Since 1980, the Dietary Guidelines for Americans have been released every five years by the US Departments of Agriculture (USDA) and Health and Human Services (DHHS). The guidelines present “advice on what to eat and drink to meet nutrient needs, promote health, and prevent disease”.\nBecause there can be economic and social barriers to maintaining a healthy diet, USDA promotes Food and Nutrition Security so that everyone has consistent and equitable access to healthy, safe, and affordable foods that promote optimal health and well-being. A set of data tools called the Purchase to Plate Suite (PPS) supports these goals by enabling the update of the Thrifty Food Plan (TFP), which estimates how much a budget-conscious family of four needs to spend on groceries to ensure a healthy diet. The TFP market basket – consisting of the specific amounts of various food categories required by the plan – forms the basis of the maximum allotment for the Supplemental Nutrition Assistance Program (SNAP, formerly known as the “Food Stamps” program), which provided financial support towards the cost of groceries for over 41 million individuals in almost 22 million households in fiscal year 2022.\nThe 2018 Farm Act (Agriculture Improvement Act of 2018) requires that USDA reevaluate the TFP every five years using current food composition, consumption patterns, dietary guidance, and food prices, and using approved scientific methods. USDA’s Economic Research Service (ERS) was charged with estimating the current food prices using retail food scanner data (Levin et al. 2018; Muth et al. 2016) and utilized the PPS for this task. The most recent TFP update was released in August 2021 and the revised cost of the market basket was the first non-inflation adjustment increase in benefits for SNAP in over 40 years (US Department of Agriculture 2021).\nThe PPS combines datasets to enhance research related to the economics of food and nutrition. There are four primary components of the suite:\nThe PPC allows researchers to measure the healthfulness of store purchases. On average US consumers acquire about 75% of their calories from retail stores, and there are a number of studies linking the availability of foods at home to the healthfulness of the overall diet (e.g., Gattshall et al. 2008; Hanson et al. 2005). Thus, understanding the healthfulness of store purchases allows us to understand differences in consumers who purchase healthy versus less healthy foods, and may contribute to better policies that promote healthier food purchases. While healthier diets are linked to a lower risk of disease outcomes (Reedy et al. 2014), other factors such as health care access may also be contributors (Cleary, Liu, and Carlson 2022). The PPC also forms the basis of the price tool, PPPT – which allows researchers to estimate custom prices for dietary recall studies – and a new ERS data product, the PP-NAP. The national average prices from PP-NAP are used in reevaluating the TFP. By using the PP-NAP with 24-hour dietary recall information from surveys such as What We Eat in America (WWEIA) – the dietary component of the nationally representative National Health and Nutrition Examination Survey(NHANES)1 – researchers can examine the relationship between the cost of food, dietary intake, and chronic diseases linked to poor diets. The price estimates also allow researchers to develop cost-effective healthy diets such as MyPlate Kitchen. The final component of the Purchase to Plate Suite, the ingredient tool (PPIT), breaks dietary recall-reported foods back into purchasable ingredients, based on US retail food purchases. The PPIT is also used in the revaluation of the TFP, and by researchers who want to look at the relationship between reported ingestion of grocery items, cost and disease outcomes using WWEIA/NHANES. More information on the development of the PPC is available in two papers by Carlson et al. (2019, 2022).\nThe Food for Thought competition aimed to support the development of the PPC – and thus policy-oriented research – by linking retail food scanner data to the USDA nutrition data used to analyze NHANES dietary recall data, specifically the Food and Nutrient Database for Dietary Studies (FNDDS) (2018, 2020). In particular, the competition set out to use artificial intelligence (AI) to reduce human resources in creating the links for the PPC, while still maintaining the high-quality standards required for reevaluating the TFP and for data published by ERS (which is one of 13 Principle Statistical Agencies in the United States Federal Government)."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Methods used to date",
    "text": "Methods used to date\nOn the surface, the linking process may appear simple: both the FNDDS and retail food scanner data are databases of food. But the scanner data are produced for market research, and the FNDDS for dietary studies. The scanner data include about 350,000 items with sales each year, while the FNDDS has only 10,000–15,000 items. Scanner data relates to specific products, while FNDDS items are often more general. Both datasets have different hierarchical structures – the FNDDS hierarchy is based around major food groups: dairy; meat, poultry and seafood; eggs; nuts and legumes; grains; fruits; vegetables; fats and oils; and sugars, sweets, and beverages. Items fall into the groups regardless of preparation method or form. That is, broccoli prepared from frozen and from fresh both appear in the vegetable group, and for some fruits and vegetables, the fresh, frozen, canned and dried form are the same FNDDS item. Vegetable-based mixed dishes, such as broccoli and carrot stir-fry or soup, are also classified in the vegetable group. On the other hand, the scanner data classifies foods by grocery aisle. That is, the fresh and frozen broccoli are classified in different areas: produce and frozen vegetables. Similarly, when sold as a prepared food, the broccoli and carrot stir-fry may be found in the frozen entries, as a kit in either the frozen or produce section, refrigerated foods, or all of these.\nTo allow researchers to import the FNDDS nutrient data into the scanner data, a one-to-many match between FNDDS and scanner data items was needed. The food descriptions in the scanner data include brand names and package sizes and are written as a consumer would pronounce them – e.g., fresh and crisp broccoli florets, ready-cut, 10 oz – versus a more general FNDDS description such as “Broccoli, raw”. (Also linked to the “Broccoli, raw” code would be broccoli sold with stems attached, broccoli spears, and any other way raw broccoli is sold.) In the scanner data, the Universal Product Code (UPC) and the European Article Number (EAN) can link items between tables within the scanner data, as well as between datasets of grocery items, such as the USDA Global Branded Foods Product Database, a component of USDA’s Food Data Central. However, these codes are not related to the FNDDS codes, or any other column within the FNDDS. In other words, before development of the PPC, there were no established linking identifiers.\nFigure 1 shows the process USDA uses to develop matches between scanner data and FNDDS.\n\n\n\n\n\n\nFigure 1: Process currently used to create the matches between the USDA Food and Nutrient Database for Dietary Studies (FNDDS) and the retail scanner data (labelled “IRI” for the IRI InfoScan and Consumer Network) product dictionaries. Source: Author provided.\n\nWe start the linking process by categorizing the scanner data items into homogeneous groups to make the first round of automated matching more efficient. To save time, we use the second lowest hierarchical category in the scanner data which generally divides items within a grocery aisle into homogenous groups such as produce, canned beans, baking mixes, and bread. Once the linking categories for scanner data are established, we select appropriate items from the FNDDS. Since the FNDDS is highly structured, this selection is usually straightforward.\nOur next step is to use semantic matching to create a search table that aligns similar terms within the IRI product dictionary and FNDDS. This first requires that we extract attributes from the FNDDS descriptions into fields similar to those in the scanner data product dictionary. The FNDDS descriptions are found across multiple columns because they are added as the need arises to provide examples of brand names or alternative descriptions of foods which help code the foods WWEIA participants report eating. We manually create matching tables that link terms used in FNDDS to those used in the scanner data, organized by the fields defined in the restructured FNDDS. We then use this table as the basis of a probabilistic matching process. For example, when linking the produce group, “fresh” in the scanner data would be aligned with “raw” and “prepared from fresh” and NOT “prepared from frozen” in the FNDDS, and “broccoli florets” would also be aligned with “raw” and “broccoli”. Since the FNDDS is designed to code the foods individuals report eating, many of the foods in the FNDDS are already prepared and result in descriptions such as “broccoli, steamed, prepared from fresh” or “broccoli, boiled, prepared from frozen”.\nOnce the linking table is established, the probabilistic match process returns the single best possible match for each item in the scanner data. For example, a match between fresh broccoli florets and frozen broccoli would have a lower probability score than “broccoli, raw”. Because these matches form the basis of major USDA policies, we cannot accept an error rate of more than 5 percent, and lower is preferred. To reach that goal, nutritionists review every match to make sure the probabilistic match did not return a match between cauliflower florets and fresh broccoli, say, or that a broccoli and carrot stir-fry is not matched to a dish with broccoli, carrots, and chicken. The correct matches, such as the one between fresh broccoli florets and raw broccoli, are set aside while the items with an incorrect match, such as cauliflower florets and the broccoli and carrot stir-fry, are used to revise the search table. Revisions might include adding (NOT chicken) to the broccoli and carrot stir-fry dish. Mixed dishes — such as the broccoli and carrot stir-fry — pose particular challenges because there are a wide variety of similar products available in the grocery store. After a few rounds of revising the search table and running the probabilistic match process, it is more efficient to use a manual match, established by one nutritionist and reviewed by another, after which the match is assumed to be correct.\nThe process improved with each new wave of FNDDS and IRI data. Our first creation of the PPC linked the FNDDS 2011/12 to the 2013 IRI retail scanner data. Subsequent waves started with the previous search table and resulting matches were reviewed by nutritionists. We also used more fields in the IRI product dictionary to create the homogeneous linking groups and made modifications to these groups with each wave. During each wave we experimented with the number of rounds of probabilistic matching that was the most cost effective. For some linking groups it took less human time to manually match from the start, while for other groups it was more efficient to do multiple rounds of improvements to the search table. Starting with the most recent wave (matching FNDDS 2017/18 to the 2017 and 2018 retail scanner data), we assumed previous matches appearing in the newer data were correct. Although this assumption was good for most matches, a review demonstrated the need to review previous matches prior to removing the item from the list of scanner data items needing FNDDS matches. In the future we intend to explore methods developed by the participants of the Food for Thought competition."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Linking challenges",
    "text": "Linking challenges\nAn ongoing challenge to the linking problem is that both the scanner data and the FNDDS undergo substantive changes each year, meaning that both the previous matches and search tables need to be reviewed and revised with each new effort, as tables that work with one cycle of FNDDS and scanner data will need revisions to use with the next cycle. Changes to the scanner data that impact our current method include dropped and added items, data corrections, and revisions to the categories that form the basis of the homogeneous linking groups. In addition, there are errors such as incorrect food descriptions, conflicting package size information, and changes in the item description from year to year. Since the FNDDS is designed to support dietary recall studies, revisions reflect both changes to available foods and the level of detail respondents can provide. These revisions result in dropped/added food codes, changes to food descriptions that impact which scanner data items match to the FNDDS items, and revisions to recipes used in the nutrient coding which impacts the number of retail ingredients available in the FNDDS.\nOf the four parts of the PPS, establishing the matches is the most time-consuming task and constitutes at least 60 percent of the total budget. In the most recent round, we had 168 categories and each one went through 2-3 automated matching rounds; after each round, nutritionists spent an average of two hours reviewing the matches. This adds up to somewhere between 670 and 1,000 hours of review time. After the automated review, manual matching requires an additional 300 hours. Reducing the amount of time required to establish matches and link the FNDDS and retail scanner datasets may lead to significant time savings, resulting in faster data availability. That, in turn, could allow more timely policy-based research, and the mandated revision of the Thrifty Food Plan can continue with the most recent food price data.\n\n\n\n\n← Introduction\n\n\n\n\nPart 2: Competition design →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAndrea Carlson is an agricultural economist in the Food Markets Branch of the Food Economics Division in USDA’s Economic Research Service. She is the project lead for the Purchase to Plate Suite, which allows users to import USDA nutrient and food composition data into retail food scanner data acquired by USDA and estimate individual food prices for dietary intake data.\n\n\nThea Palmer Zimmerman is a senior study director and research nutritionist at Westat.\n\n\n\n\n\nImage credit\n\nThumbnail photo by Kenny Eliason on Unsplash.\n\n\n\n\n\nHow to cite\n\nCarlson, Andrea, and Thea Palmer Zimmerman. 2023. “Food for Thought: The importance of the Purchase to Plate Suite.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe research presented in this compendium supports the Purchase to Plate Suite of data products. Carlson has been privileged to both develop and lead this project over the course of her career, but it is not a solo project. Many thanks to the Linkages Team from USDA’s Economic Research Service (Christopher Lowe, Mark Denbaly Elina Page, and Catherine Cullinane Thomas) the Center for Nutrition Policy and Promotion (Kristin Koegel, Kevin Kuczynski, Kevin Meyers Mathieu, TusaRebecca Pannucci), and our contractor Westat, Inc. (Thea Palmer Zimmerman, Carina E. Tornow, Amber Brown McFadden, Caitlin Carter, Viji Narayanaswamy, Lindsay McDougal, Elisha Lubar, Lynnea Brumby, Raquel Brown, and Maria Tamburri). Many others have supported this project over the years."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#footnotes",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#footnotes",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNHANES is a multi-module continuous survey conducted by the Centers for Disease Control and Prevention. In addition to the WWEIA, NHANES includes a four-hour complete medical exam including a health history, and a blood and urine analysis.↩︎"
  },
  {
    "objectID": "case-studies/posts/2023/11/22/splink.html",
    "href": "case-studies/posts/2023/11/22/splink.html",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "",
    "text": "In 2019, the data linking team at the Ministry of Justice was challenged to develop a new data linking methodology to produce new, higher quality linked datasets from the justice system.\nThe ultimate goal was to share new linked datasets with academic researchers, as part of the ADR UK-funded Data First programme. These datasets – which include data from prisons, probation, and the criminal and family courts – are now available, and researchers can apply for secure access.\nThe linking methodology is widely applicable and has been published as a free and open source software package called Splink. The software applies statistical best practice to accurately and quickly link and deduplicate large datasets. The software has now been downloaded over 7 million times, and has been used widely in government, academia and the private sector."
  },
  {
    "objectID": "case-studies/posts/2023/11/22/splink.html#the-problem",
    "href": "case-studies/posts/2023/11/22/splink.html#the-problem",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "The problem",
    "text": "The problem\nData duplication is a ubiquitous problem affecting data quality. Organisations often have multiple records that refer to the same entity but no unique identifier that ties these entities together. Data entry errors and other issues mean that variations usually exist, so the records belonging to a single entity aren’t necessarily identical.\nFor example, in a company, customer data may have been entered multiple times in multiple different databases, with different spellings of names, different addresses, and other typos. The inability to identify which records belong to each customer presents a data quality problem at all stages of data analysis – from basic questions such as counting the number of unique customers, through to advanced statistical analysis.\nWith the growing size of datasets held by many organisations, any solution must be able to work on very large datasets of tens of millions of records or more."
  },
  {
    "objectID": "case-studies/posts/2023/11/22/splink.html#approach",
    "href": "case-studies/posts/2023/11/22/splink.html#approach",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Approach",
    "text": "Approach\nIn collaboration with academic experts, the team started with desk research into data linking theory and practice, and a review of existing open source software implementations.\nOne of the most common theoretical approaches described in the literature is the Fellegi-Sunter model. This statistical model has a long history of application for high profile, important record linking tasks such as in the US Census Bureau and the UK Office for National Statistics (ONS).\nThe model takes pairwise comparisons of records as an input, and outputs a match score between 0 and 1, which (loosely) can be interpreted as the probability of the two records being a match. Since the record comparison can be either two records from the same dataset, or records from different datasets, this is applicable to both deduplication and linkage problems.\nAn important benefit of the model is explainability. The model uses a number of parameters, each of which has an intuitive explanation that can be understood by a non-technical audience. The relative simplicity of the model also means it is easier to understand and explain how biases in linkage may occur, such as varying levels of accuracy for different ethnic groups.\n\nExample\nConsider the following simple record comparison. Are these records a match?\n\n\n\nFigure 1: Colour coded comparison of two records.\n\n\nThe parameters of the model are known as partial match weights, which capture the strength of the evidence in favour or against these records being a match.\nThey can be represented in a chart as follows, in which the highlighted bars correspond to the above example record comparison:\n\n\n\nFigure 2: Chart showing partial match weights of model.\n\n\nWe can see, for example, that the first name (Robin vs Robyn) is not an exact match, but they have a Jaro-Winkler similarity of above 0.9. As a result, the model ‘activates’ the corresponding partial match weight (in orange). This lends some evidence in favour of a match, but the partial match weight is not as strong as it would have been for an exact match.\nSimilarly we can see that the non-match on gender leads to the activation (in purple) of a strong negative partial match weight.\nThe activated partial match weight can then be represented in a waterfall chart as follows, which shows how the final match score is calculated:\n\n\n\nFigure 3: Waterfall chart showing how partial match weights combine to calculate the final prediction.\n\n\nThe parameter estimates in these charts all have intuitive explanations:\n\nThe partial match weight on first name is positive, but relatively weak. This makes sense, because the first names are a fuzzy match, not an exact match, so this provides only moderate evidence in favour of the record being a match.\nThe match weight for the exact match on postcode is stronger than the equivalent weight for surname. This is because the cardinality of the postcode field in the underlying data is higher than the cardinality for surname, so matches on postcode are less likely to occur by chance than matches on surname.\nThe negative match weight for the mismatch on gender is relatively strong. This reflects the fact that, in this dataset, it’s uncommon for the ‘gender’ field to match amongst truly matching records.\n\nThe final result is that the model predicts these records are a match, but with only 94% probability: it’s not sure. Most examples would be less ambiguous than this one, and would have a match probability very close to either 0 or 1.\nFor further details of the theory behind the Fellegi-Sunter model, and a deep dive into the intuitive explanations of the model, I have have developed a series of interactive tutorials."
  },
  {
    "objectID": "case-studies/posts/2023/11/22/splink.html#implementation",
    "href": "case-studies/posts/2023/11/22/splink.html#implementation",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Implementation",
    "text": "Implementation\nThrough our desk research and open source software review, an existing software package called fastLink was identified which implements the Fellegi-Sunter model, but unfortunately the software is not able to handle very large datasets of more than a few hundred thousand records.\nInspired by the popularity of fastLink, the team quickly realised that the methodology it was developing was generally applicable and could be valuable to a wide range of users if published as a software package.\nAs we spoke to colleagues across government and beyond, we found record linkage and deduplication problems are pervasive, and crop up in many different guises, meaning that any software needed to be very general and flexible.\nThe result is Splink – which is a Python package that implements the Fellegi-Sunter model, and enables parameters to be estimated using the Expectation Maximisation algorithm.\nThe package is free to use, and open source. It is accompanied by detailed documentation, including a tutorial and a set of examples.\nSplink makes no assumptions about the type of entity being linked, so it is very flexible. We are aware of its use to match data on a variety of entity types including persons, companies, financial transactions and court cases.\nThe package closely follows the statistical approach described in fastLink. In particular it implements the same mathematical model and likelihood functions described in the fastLink paper (see pages 354 to 357), with a comprehensive suite of tests to ensure correctness of the implementation.\nIn addition, Splink introduces a number of innovations:\n\nAble to work at massive scale – with proven examples of its use on over 100 million records.\nExtremely fast – capable of linking 1 million records on a laptop in around a minute.\nComprehensive graphical output showing parameter estimates and iteration history make it easier to understand the model and diagnose statistical issues.\nA waterfall chart which can be generated for any record pair, which explains how the estimated match probability is derived.\nSupport for deduplication, linking, and a combination of both, including support for deduplicating and linking multiple datasets.\nGreater customisability of record comparisons, including the ability to specify custom, user defined comparison functions.\nTerm frequency adjustments on any number of columns.\nIt’s possible to save a model once it’s been estimated – enabling a model to be estimated, quality assured, and then reused as new data becomes available.\nA companion website provides a complete description of the various configuration options, and examples of how to achieve different linking objectives."
  },
  {
    "objectID": "case-studies/posts/2023/11/22/splink.html#using-splink",
    "href": "case-studies/posts/2023/11/22/splink.html#using-splink",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Using Splink",
    "text": "Using Splink\nFull documentation and a tutorial are available for Splink, but the following snippet gives a simple example of Splink in action:\nfrom splink.datasets import splink_datasets\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.duckdb.comparison_library import (\n    exact_match,\n    jaro_winkler_at_thresholds,\n    levenshtein_at_thresholds,\n)\nfrom splink.duckdb.linker import DuckDBLinker\n\ndf = splink_datasets.fake_1000\n\n# Specify a data linkage model\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n      block_on(\"first_name\"),\n      block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        jaro_winkler_at_thresholds(\"first_name\", 2),\n        jaro_winkler_at_thresholds(\"surname\"),\n        levenshtein_at_thresholds(\"dob\"),\n        exact_match(\"city\", term_frequency_adjustments=True),\n        exact_match(\"email\"),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\n\n# Estimate model parameters\n\n# Direct estimation using random sampling can be used for the u probabilities\nlinker.estimate_u_using_random_sampling(target_rows=1e6)\n\n# Expectation maximisation is used to train the m values\nbr_training = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n\nbr_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n\n# Use the model to compute pairwise match scores\npairwise_predictions = linker.predict()\n\n# Cluster the match scores into groups to produce a synthetic unique person id\nclusters = linker.cluster_pairwise_predictions_at_threshold(\n  pairwise_predictions, 0.95\n)\nclusters.as_pandas_dataframe(limit=5)\nThe example shows the flexibility of Splink, and how various types of configuration can be used:\n\nHow should different data fields be compared? In this example, the Jaro-Winkler distance is used for names, whereas Levenshtein is used for date of birth since Jaro-Winkler is not appropriate for numeric data.\nWhat blocking rules should be used? Blocking rules are the primary determinants of how fast Splink will run, but there is a trade off between speed and accuracy. In this case, the input data is small, so the blocking rules are loose.\nHow should the model parameters be estimated? In this case, the user has no labels for supervised training, and so uses the unsupervised Expectation Maximisation approach.\nIs clustering needed? In this case, each person may potentially have many duplicates, so clustering is used. This creates an estimated (synthetic) unique identifier for each entity (person) in the input dataset."
  },
  {
    "objectID": "case-studies/posts/2023/11/22/splink.html#outcomes",
    "href": "case-studies/posts/2023/11/22/splink.html#outcomes",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Outcomes",
    "text": "Outcomes\nSplink has been used to link some of the largest datasets held by the Ministry of Justice as part of the Data First programme, and researchers are now able to apply for secure access to these datasets. Research using this data won the ONS Linked Administrative Data Award at the 2022 Research Excellence Awards.\nMore widely, the demand for Splink has been higher than we expected – with over 7 million downloads. It has been used in other government departments including the Office for National Statistics and internationally, the private sector, and published academic research from top international universities.\nSplink has also had external contributions from over 30 people, including staff at the Australian Bureau of Statistics, DataBricks, other government departments, academics, and various private sector consultancies.\n\n\n\n\n\n\nEditor’s note: For more on data linkage, check out our interview with Helen Miller-Bakewell of the UK Office for Statistics Regulation, discussing the OSR report, Data Sharing and Linkage for the Public Good.\n\n\n\n\nFind more case studies\n\n\n\n\n\nAbout the author\n\nRobin Linacre is an economist, data scientist and data engineer based at the UK Ministry of Justice. He is the lead author of Splink.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Robin Linacre\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Possessed Photography on Unsplash.\n\n\n\nHow to cite\n\nLinacre, Robin. 2023. “Deduplicating and linking large datasets using Splink.” Real World Data Science, November 22, 2023. URL"
  },
  {
    "objectID": "careers/posts/2023/05/11/chatgpt-data-science-pt1.html",
    "href": "careers/posts/2023/05/11/chatgpt-data-science-pt1.html",
    "title": "How is ChatGPT changing data science?",
    "section": "",
    "text": "For many people, it starts with a question. Something simple, something they already know the answer to. A test, in other words, to see what these AI-powered chatbots are all about. But spend any amount of time with ChatGPT and other such tools and you’ll quickly start to wonder what else they might do, and how useful they might be in your day-to-day working life.\nData scientists certainly have been thinking along these lines, and to find out more about current use cases, proofs of concepts and potential applications, Real World Data Science got together with members of the Royal Statistical Society’s Data Science and AI Section (DS&AI) for a group discussion.\nOur interviewees, in order of appearance, are:\n\nPiers Stobbs, VP science at Deliveroo, and DS&AI committee member.\nDetlef Nauck, head of AI and data science research at BT, and editorial board member, Real World Data Science.\nAdam Davison, head of data science at the Advertising Standards Authority, and DS&AI committee member.\nTrevor Duguid Farrant, senior principal statistician at Mondelez International, and DS&AI committee member.\nGiles Pavey, global director for data science at Unilever, and DS&AI vice-chair.\nMartin Goodson, CEO and chief scientist at Evolution AI, and DS&AI committee member.\n\nThe first part of our discussion focuses on how large language models are becoming part of the data science toolkit, and what this new development means for data science teams and skillsets. Stay tuned for part two, which we’ll be publishing soon!\n(UPDATE: Part two is now published: “Large language models: Do we need to understand the maths, or simply recognise the limitations?”)\n\n\n\n\n\nAs data scientists, how has ChatGPT – and other tools built on large language models (LLMs) – changed your working lives?\nPiers Stobbs: Up to about a year ago, although I was really impressed with the developments in deep learning and the improvements in computer vision and natural language models, it felt in line with general improvements in machine learning. And then, probably about six months ago, with things like DALL·E and ChatGPT, it felt like something changed – properly ground-breaking capabilities. And I still can’t quite get my head around the fact that you can basically have a model that tries to predict the next token, and it comes up with outputs that really feel quite sensible and human-like – if prone to hallucination.\nThe way I think about it is, this feels like a brand-new capability that we’ve just not really had before. It’s almost like an interface with unstructured information. Historically, you sort of have to turn text into something, and then turn something back into text, if you want to have this interface with humans. Now, we’ve got this really quite elegant way of plugging the gaps, which feels full of opportunities.\nI’m having great fun playing around with the code co-pilots – GitHub’s Copilot is amazing and, productivity wise, is helping me a lot. I am now a much faster coder because there’s all those Stack Exchange lookups that I don’t have to do anymore. Again, from a personal productivity perspective, I’m using [ChatGPT] for initial drafts of documents and other things. And then I use it almost for validating things. For instance, I had a random discussion the other night with ChatGPT about logistic algorithms. It’s not going to solve problems for you, but I asked it to give some pointers of things I could be thinking about – some of which I had, some of which I hadn’t. So, it’s almost like a brainstorming helper, somehow.\nBut probably the thing I’m most excited about is the knowledge sharing side of it – plugging it into, or on top of, private information, and surfacing all that knowledge that is locked away in documents and intranet pages.\n\n\n\n\n\n\nPiers Stobbs\n\n\n\n\n\nThis feels like a brand-new capability – an interface with unstructured information. Historically, you have to turn text into something, and then turn something back into text, if you want to have this interface with humans. Now, we’ve got this elegant way of plugging the gaps.\n\n\n\nDetlef Nauck: We’re looking into running proof of concepts to see whether LLMs do bring value. Software engineering is the most obvious one, and easiest to set up and run. And then we want to look at making use of internal documents – so, either summarization or creation of internal documents in appropriate language. The latter use cases are trickier to evaluate. We want to know whether the outputs produced are any good. With software engineering you can track GitHub statistics, for example. But if you give ChatGPT to somebody to write marketing material, or to get information out of a document, how do you know that the results are good? We need to get our head around metrics for evaluation.\nAdam Davison: I’ve been using it for basically anything where I don’t remember the API very well or it’s a bit confusing. Pandas is the key, right? We all use pandas, but you don’t really remember how to do some complicated thing with apply(), say, so you just ask GPT-4 to give you the answer, and it saves you that hassle. Also, I read some insightful tweet that said these chat systems are really good for things where generating the solution is hard, but verifying it is easy. And I think that’s true for some of these things. You know, you get a short piece of Python code, you can basically look at that and you can tell if it’s right.\nIn data science, you’re a bit of a jack-of-all-trades. You need to do little bits of everything, but you’re not a specialist in anything. And so, I think for software development, it’s been really helpful. For example, right now, I’m doing a bit of frontend development in a project to visualise something. I’m never going to be a professional frontend developer, but GPT-4 can help deal with the oddities of JavaScript much more easily than it would be for me to trawl through Stack Overflow posts.\nBut the thing that we’re using it for, practically, is natural language processing (NLP) and classification. We have this particular problem at the Advertising Standards Authority (ASA) where we are running lots of different models that are completely unconnected to each other because every project is a different topic. So, one week we’re looking at, “Do these gambling ads appeal to young people?” and then the week after it’s, “Are these cryptocurrency ads being clear about the risks involved?” It’s very disparate, we don’t have a lot of time to iterate on models, and we don’t have huge amounts of training data. Ten years ago, when you were doing sentiment classification, you were on Mechanical Turk getting 10,000 examples, and even then it didn’t work very well after these really complicated models. Now, you’ve got a couple of hundred examples and with the embeddings [from LLMs] you can get to a pretty decent classifier quite quickly. We’re also starting to experiment with using OpenAI’s fine-tuning tools, and the performance that we’ve seen from that is very impressive, to the extent that it’s making us rethink whether we bother doing anything else in some of our classifiers.\n\n\n\n\n\n\nAdam Davison\n\n\n\n\n\nFive years ago, if you had a sophisticated problem involving text or images, you’d need a big research team with a big budget to tackle it. But increasingly we find, like many other people, that you can take models off the shelf and repurpose them for quite diverse tasks.\n\n\n\nTrevor Duguid Farrant: My organisation is not as far forward as the rest of you. I’ve introduced it to the leadership team, and the digital services team – what was IT – are looking to make a decision on whether we can use it or not. I think there’ll be so much pressure they’ll have to use it, but there’s still a feeling of discomfort with it, whereas I think it’s really good and have started using it. Everyone else on the call seems to have started using it. So, can organisations like the Royal Statistical Society help companies to embrace this and start using it, and then everyone can benefit from it?\nGiles Pavey: I wish I could be with Piers and Adam – actually using it – but my life has been taken over as the guy who goes and explains it to the business. Unilever is a massive business, and we are concerned about privacy, confidentiality and trustworthiness. We’ve now built an initial GPT instance on Azure and fed it with some of our own documents, and a lot of my time has been working with legal to convince ourselves that that’s okay. Now we are really trying to work out just how we manage the amazing demand for proofs of concepts and use cases – and what we’re just about to uncover, I think, is the unknown but potentially massive expense of running it.\nIn pure proofs of concepts, departments that have large knowledge banks are using it: research and development, and marketing, for example. And one of the big technical things that we’re working on – and, because of the size that we are, we’re doing a lot of work with OpenAI and Microsoft on this – is how to stop the models from hallucinating.\nHave your experiences with ChatGPT and other tools changed your thinking about the skillsets required of data scientists and data science teams?\nAD: A little bit. As someone at a small organisation, I think it’s quite exciting because, five years ago, maybe you were in a world where if you had a sophisticated problem involving text or images, you’d need a big research team with a big budget to tackle it. But increasingly we find, like many other people, that you can take models off the shelf and repurpose them for quite diverse tasks. So, I think it’s becoming increasingly viable to have a small team of people who are implementers, who aren’t necessarily backed up by a big research organisation, doing increasingly sophisticated stuff.\nI don’t think it does away with the sort of things that we always bang on about in the Data Science and AI Section, like the need for an understanding of statistics and how the underlying systems really work, because I think you still need to understand what you’re doing with LLMs, as with any other machine learning technique. But, if I had to guess, what we’re going to be seeing now is that for a lot of problems, you’re going to have more of a division – so, you’re either in one of a small number of very large labs doing research on very cutting-edge big models, or you can be an implementer who is taking things off the shelf and applying them. And maybe that space in between is going to get a little bit squeezed – that would be my guess, but obviously it’s very unpredictable.\n\n\n\n\n\n\nGiles Pavey\n\n\n\n\n\nWe’ve built an initial GPT instance on Azure. Now we are really trying to work out just how we manage the amazing demand for proofs of concepts and use cases – and what we’re just about to uncover, I think, is the unknown but potentially massive expense of running it.\n\n\n\nPS: That’s exactly my view. When I first started hiring data scientists, a long time ago, you basically had to write stuff from scratch, and you needed PhDs – people who really understood, at a deep level, how the maths all works. But I think there’s been a steady progression towards valuing software engineering skills, and I think, in some ways, this is another step along that path. If I think now about implementing a chatbot over your own knowledge base, it’s basically like plugging APIs together with some Python. Adam’s point is still hugely important, though, because I think we still need the background knowledge about what’s actually going on – OK, I’m creating embeddings here, and that’s allowing this search to work so I can surface the right docs – that whole process, which an average software engineer is maybe not going to know. But I think it’s definitely blurring the lines.\nMartin Goodson: It’s just as important now to understand how to evaluate performance. The difference is, it used to be that you were trying to figure out whether it’s 80% accurate, or 85%. Now, it’s like 99.9%. But you still need to figure it out. You still need to understand what the failure modes are, what caused it; how is it actually working, and is it doing what you need it to do for the product? Is it actually satisfying our needs as users or as customers of the products.\nDN: I think in the future, the skills we will need are people who can run and build these models. Giles made the point about how expensive it is to run these things. Right now, you have two options: subscribe to an API, and then you are limited in how you can modify these models; or build your own – take an open-source LLM and modify it. But then you need people who know how to build a high-performance computing environment and operate this efficiently. You need to know how to actually train the models, how to curate the data, how to set the model parameters. And I always think there’s too much alchemy still going on in this field, right? It’s not proper science. People build these things and then are surprised at what they can do; they didn’t know such things would be possible. A lot of these capabilities only emerge when you make the models really, really big and, essentially, you also have no control over them – you can’t stop them hallucinating. So, these are the kinds of issues we need to get under control if we want to get any value out of them.\nPrompt engineering is another one – you really need to understand how these models work and how to prompt them. If you want to give them to, say, a marketer to generate copywriting, they may not have the right ideas of how to prompt the machine. So, I could see roles developing out of data science that understand how to influence these models and make them do what we want them to do.\nMG: The other angle to this is junior engineers. Now, the bar for being a useful junior engineer is that you’re better than GitHub Copilot. Why do you need a really junior person if you can just use a large language model to be the junior developer?\nDN: I’m not thinking about the data science person who needs to write some code for a project here, but if you have a large software team in an organisation that produces production code, they will become more efficient by using these tools. But still, with all this overhead of testing and putting it all together, there will be a lot of manual work that needs to be done. But the teams will get more efficient and junior people will get up to speed quicker. That’s probably another advantage.\n\n\n\n\n\n\nTrevor Duguid Farrant\n\n\n\n\n\nCan the Royal Statistical Society help non-tech companies embrace large language models, extolling their virtues and dispelling the myths?\n\n\n\nPS: I think Detlef’s point about understanding is an interesting one. It definitely feels like there’s been this sort of continuum from, you know, “OK, it’s a linear regression, we know what’s going on” to complex models to ensemble models where, again, you’re combining these things you can individually understand. Even with big ImageNet architectures, billions of parameters, at least conceptually you can understand how these work and build out tools where you can understand the layers. To me, what’s different now is you’ve got this reinforcement learning layer on top, or diffusion layer, or some other additional approach – this combination of really complicated things. I honestly don’t know where to start with trying to understand why a specific output is generated, and I think that is a proper concern. That’s definitely an area of research, because I think we need to understand this.\nGP: I think there’s also a question in large companies of just who owns these things. Up until this point, everybody’s been happy that AI is the realm of data science. And, suddenly, generative AI looks like it might be the realm of the IT team – that it’s a service that you get off the shelf. It’s going to be interesting to see how that plays out. I really liked the point that Martin was making about being able to tell what the systems are actually doing, what they are supposed to do and how to check them, because if you don’t have a background in that area, you might just assume they work. Now, nobody knows exactly how these things work – not even the people who build them. But having a background in how you test things, for potential causes for things not working, is actually going to be incredibly powerful or useful.\nTDF: Will experts like us actually be able to check it because of the speed that new versions are coming out and the developments that are happening? Is it going to take us six months to check that GPT-3.5 works? Well, too late, a month later GPT-4’s out! I just think that pace is going to keep accelerating.\n\n\n\n\n\n\nWant to hear more from the RSS Data Science and AI Section? Sign up for its newsletter at rssdsaisection.substack.com.\n\n\n\n\n\n\n\nBack to Careers\n\n\n\n\nRead part two →\n\n\n\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photos are not covered by this licence. Portrait photos are supplied by interviewees and used with permission. ChatGPT homescreen photo by Levart_Photographer on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How is ChatGPT changing data science?” Real World Data Science, May 11, 2023. URL"
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html",
    "title": "The politics of performance measurement",
    "section": "",
    "text": "At the beginning of 2016, the Criminal Justice Division (CJD) of the Texas Governor’s Office received news all government agencies dread: budgets were to be cut. CJD oversaw a grant program that funded specialty courts throughout the state, however it was now being told that the program’s budget of $10.6m would be reduced 20% to $8.5m by 2018.\nHow should these cuts be distributed among grant holders? CJD had no meaningful performance data on which to base its decisions, and I would know: I was hired by the agency just a few months before to analyze grant performance. Still, decisions needed to be made. We had to come up with a plan of action, and the clock was ticking…\nThis is a story of making opportunity out of crisis, of the interaction between not just theory of change and technical implementation, but the “political” process of negotiating these changes with stakeholders in a manner that led to better decisions. Through careful outreach and continuous communication, we developed a data collection and performance assessment process that enabled us to allocate budget cuts in a manner widely accepted.\nThe story ends on a bittersweet note. But, along the way, there are lessons to be learned about how to find common ground, manage expectations, forge productive working partnerships, and sustain a data science project longer term."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-1-consider-your-options",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-1-consider-your-options",
    "title": "The politics of performance measurement",
    "section": "Step 1: Consider your options",
    "text": "Step 1: Consider your options\nTexas had over 150 specialty courts in 2016, providing a program of specialized services – usually drug treatment – to offenders as an alternative to incarceration. About half of the state’s specialty courts received CJD grant funds (and about half of grantees received 100% of their program budget from our grants). Funding cuts of the size we needed to make would not go over well with them. Any changes to the program would have to run a gauntlet of decision-makers including advisory boards, interest groups, and professional associations, most with contacts in the legislature.\nComplicating this situation further, CJD didn’t even make the final funding decisions. We administered the grants, but the merit review process fell to the Specialty Courts Advisory Council, an appointed group of specialty courts staff and related experts who annually scored the grant applications we received. We needed to get them onboard.\nThe way our Executive Director saw it, we had three options to implement the cut in a way that could get us buy-in from stakeholders and the Advisory Council:\n\nCut across the board. The Advisory Council would employ the same scoring method as the previous year but reduce each grant amount by 20%.\n\nThis option would leave long-running grantees scrambling to make up for this shortfall by reducing services, laying off staff, or spending more of their limited local funds. Worse, it would punish all grantees equally – our most successful programs would be arbitrarily defunded.\n\nFewer grants. Grants were scored based on the quality of their application and all grants that passed a certain threshold got funded. The Advisory Council would employ the same scoring method as the previous year but instead of funding the top $10.6m worth of grants, they would fund the top $8.5m worth.\n\nThis seemed a less bad option than cutting across the board, but we would still run into the problem of arbitrarily defunding successful programs. Grants near the bottom of the Advisory Council’s cutoff that got funded the previous year would be denied renewal only because the goalposts had moved.\n\nTargeted funding. The Advisory Council would incorporate performance data and statewide strategic plan alignment into their scoring method and make cuts accordingly.\n\nAt the time, the Advisory Council did not take performance into consideration when scoring grant applications. They agreed in theory that a grant requesting its tenth annual renewal should perhaps at some point be assessed on its outcomes, but they had never seen CJD commit to a rigorous performance assessment process before. We administered the grants, not them, so without our commitment to develop a performance assessment process, and their trust in that commitment, this would not be a viable option.\nAfter due consideration, option 3 emerged as the favorite of our Executive Director. On the face of it, this seemed the most “objective” approach to take. We would let the data decide who gets funded and who doesn’t, rather than cutting arbitrarily. But that would be a fallacious argument. Data does not decide. It might inform our decisions, but it would be up to us to choose the structure of the performance measurement process: what aspects to focus on, what data to collect, what benchmarks to set – all of which would later help determine funding decisions. And in any funding decision, politics inevitably plays a role.\nPolitics is, in its broadest sense, the negotiated decision-making between groups with opposing interests. And in developing our performance measurement process we would encounter a variety of interests – from the Advisory Council down to the grantees themselves. Success would require us to acknowledge stakeholder perspectives and address or manage them appropriately. Planning decisions made in the early phases of a project as a result of political processes directly influence the type and scope of analysis a data scientist will eventually be able to perform, so it behooves the data scientist to participate in these processes!"
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-2-engage-stakeholders-and-define-performance",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-2-engage-stakeholders-and-define-performance",
    "title": "The politics of performance measurement",
    "section": "Step 2: Engage stakeholders and define performance",
    "text": "Step 2: Engage stakeholders and define performance\nHaving settled on our preferred option, our Executive Director convened a strategy session with the Advisory Council to discuss how to proceed as part of a broader strategic plan. The session began by achieving consensus on high-level goals such as “fund strategically”, “focus on success”, “build capacity”, etc. The session also helped the Advisory Council and CJD alike to clarify our conception of how we ought to fit into the specialty courts field going forward. CJD would develop its performance assessment system to help the Advisory Council target funding, but that would come as part of a larger plan that included capacity building, training and technical assistance, helping courts obtain non-CJD sources of funding, and steering grantees toward established best practice.\nWe left the meeting with a very basic plan that looked good on paper. Our Executive Director set to work persuading our external stakeholders of the wisdom of this new strategic direction. Meanwhile, I had to build a performance assessment process that people could trust.\nCJD had no formally designated standards to measure performance data against. However, drug courts have been around for decades and there existed a large body of research supporting the program model.1 Offering supervised drug treatment instead of incarceration had been repeatedly shown to cost less money and lower recidivism rates. I performed a literature review and spoke with numerous subject matter experts to get started on defining program-specific performance metrics.\nI was conscious that imposing metrics without any feedback or input from affected parties would all but guarantee bad-faith engagement, especially if these metrics are tied to funding. A problem inherent to any performance measurement is that once something gets measured as a performance outcome, it warps the very processes it is intended to measure. This phenomenon happens so frequently that the phrase “Campbell’s Law” was coined to describe it in 1979.2 Think of standardized testing at schools: once the government ties test performance to school funding it creates powerful incentives for schools to improve test scores at any cost. Even in the absence of outright cheating, struggling schools feel massive pressure to adjust their curriculum, to the point where they teach test score optimization strategies more than math, language, history, and science.\nI consistently heard from specialty court scholars and practitioners alike that arrest recidivism would be the ideal outcome measure. On paper, recidivism was a direct expression of long-term program success and could also be used as an outcome variable for classification modeling. And, in practice, a court could do little to affect recidivism by way of manipulation. Courts do not make arrests – police make arrests. Once a specialty court participant finished a program, the court itself no longer intervened in their lives. If a participant got arrested within 1-3 years of completion, the program had no say in the matter.\nThis, however, presented an implementation problem: one-year recidivism data would, by definition, take a year past the point of implementation to collect, i.e., not soon enough to inform our cuts. And while recidivism was the best measure of success, it could not be the only measure. Recidivism was, after all, a stochastic process not within the court’s control – a crime wave or other systemic factors could move recidivism up and make it look like a successful court had actually failed. We would have to use something else as well.\nThe National Association of Drug Court Professionals (NADCP) publishes a book of best practice standards, and our stakeholders identified a court’s adherence to these standards as another strong performance assessment standard. These criteria, unlike recidivism, were directly under the program’s control. Does your program have the recommended staff? Does your program drug test participants frequently enough to guarantee sobriety? Does your program meet with participants regularly enough? Do you offer a continuum of services instead of a “one-size-fits-all” approach?\nIn addition to being much easier to measure than recidivism, best practice adherence also resists Campbell’s Law by avoiding outcome measurement. In our school metaphor, this would be like measuring school performance based on student-to-teacher ratio, variety of course offerings, attendance rates, and teacher qualifications. Far from perfect, but measuring a variety of elements that predict success and taking them as a whole represents a vast improvement over a single, easily-gamed outcome measure.\nBut to operationalize these standards, we would have to have good data."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-3-update-processes-and-collect-data",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-3-update-processes-and-collect-data",
    "title": "The politics of performance measurement",
    "section": "Step 3: Update processes and collect data",
    "text": "Step 3: Update processes and collect data\nWe inherited a longstanding process in which grantees had to fill out a form every six months asking them to report performance data. This is a screenshot of what that form looked like:\n\n\n\n\n\nNo additional definitions or instructions were provided, leaving grantees with many questions: Does the request for “annual data” mean as of fiscal year or calendar year? What counts as a person being “assessed for eligibility”? And so on. Grantees did not know the answers, and neither did we. And these were the more straightforward measures. The form went on for 10 pages, most of which asked grantees to report extensively on information they had already provided as part their application.\nThis disaster of an assessment process did have a silver lining. When we announced we were throwing out these forms entirely we faced almost no pushback from grantees.\nWe knew from the start that our new assessment process would need to collect individual-level participant data instead of aggregated measures. Even with clear definitions, 75 grants would mean 75 different aggregations at work. Asking the grantees to report their individual-level participant data in a consistent format and doing the aggregations ourselves meant a single aggregation at work.\nBut we needed to establish trust with grantees before making this request. Strictly speaking, we could mandate the reporting of this data. However, if that angered enough of our grantees, they or their contacts might take it up with our bosses at the Governor’s Office, and our bosses could cancel any plan we came up with if they thought it was not worth the fuss. So, from day one we communicated clearly to all grantees that we would maintain total transparency when it came to definitions and calculations. Before we used any calculated metric to assess performance we would send it to the grantees themselves to review for accuracy.\nTo avoid the vagueness and inscrutability that characterized the old reporting process, every piece of data we asked for in the new process had a clear written definition and specific reason for being asked. These reasons usually amounted to some combination of best practices, Advisory Council recommendations, and grantee suggestions.\nImplementing the new process was far from easy, however. We faced numerous administrative and technical barriers. Texas courts at this time did not share a common case management system, so we couldn’t just get a data export from everybody. Meanwhile, the Governor’s Office banned all of its divisions from all usage of the cloud. This forced us to build a more labor-intensive reporting process, in which courts would obtain blank Excel templates with required data fields. Courts had either to fill out these templates by hand or export their case management data and reconfigure it to template specifications. Then, courts submitted their data for review and we sent back any bad formatting.\nWe collected preliminary data at the six-month mark and made another adjustment based on these results, which we would not count toward performance measurement. A majority of courts had some kind of data error in this first case. Specific definitions of data fields had to be written and rewritten using grantee feedback over the course of the year, leading to significant changes between the six-month reports and the year-end reports.\nImportantly, we had developed reporting requirements iteratively with participation from grantees and the Advisory Council from the start. By mid-2017 we had so successfully achieved buy-in that only one grantee court’s judge refused to give us data (the court’s grant manager later sent it to us)."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-4-analyze-and-report-findings",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#step-4-analyze-and-report-findings",
    "title": "The politics of performance measurement",
    "section": "Step 4: Analyze and report findings",
    "text": "Step 4: Analyze and report findings\nIn the course of this process, we established the benchmarks in Table 1 based on best practices and justification for funding. Because this was our initial rollout, we set the specific values low to function more as minimum standards than targets.\n\nTable 1: Specialty court best practices translated into quantitative measures.\n\n\n\n\n\n\n\n\n\n\nBenchmark\nBest practice\nRationale\n\n\n\n\n1. Number of participants\n10+\nCJD decision: programs should be of sufficient size to justify a grant\n\n\n2. Number of graduates\n5+\nCJD decision: programs should be of sufficient size to justify a grant\n\n\n3. Graduation rate\n20%-90%\nCJD decision: 0% and 100% success rates are both red flags\n\n\n4. Average amount of time graduates spent in program (in months)\n12-24\nNADCP best practice recommended program lengths of 1-2 years\n\n\n5. Percent of graduates employed, seeking education, or supported through family, partner, SSI, etc.\n100%\nNADCP best practice recommended against releasing participants without financial support, which all but guarantees relapse or rearrest.\n\n\n6. Percent of participants with “low-risk” assessment score\n0%\nNADCP best practice recommended moderate- or high-risk participants. Research had shown that low-risk participants get little benefit.\n\n\n7. Average sessions per participant per month\n1+\nNADCP best practice recommended sessions be held at least monthly.\n\n\n\n\nGrantee performance data for each benchmark would be generated from the individual level data that courts sent us. Crucially, we sent our aggregations back to grantees for confirmation prior to using them in any kind of evaluation, alongside the program-wide average and the best practice values for comparison (example in the table below). If something didn’t look right, they had the chance to let us know before we took their numbers as final.\n\nTable 2: Specialty court best practices compared with program-wide averages and grantee reported values.\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\nBest practice\nProgram-wide average\nGrantee reported values\n\n\n1. Number of participants\n10+\n89\n96\n\n\n2. Number of graduates\n5+\n25\n27\n\n\n3. Graduation rate\n20%-90%\n71%\n56%\n\n\n4. Average amount of time graduates spent in program (in months)\n12-24\n17\n14\n\n\n5. Percent of graduates employed, seeking education, or supported through family, partner, SSI, etc.\n100%\n95%\n100%\n\n\n6. Percent of participants with “low-risk” assessment score\n0%\n18%\n2%\n\n\n7. Average sessions per participant per month\n1+\n2\n3.7\n\n\n\n\nIn the end, we found seven grants that we could unequivocally recommend be cut. Two of the seven had effectively never gotten off the ground, and served almost no participants the entire year. The other five served mostly low-risk participants, the type of people that research had shown do not benefit from specialty court programs. Some of these grantees were inevitably disappointed at the decision, but we had so actively worked within the field to develop and justify our processes that they understood why the decision had been made."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#factors-for-success",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#factors-for-success",
    "title": "The politics of performance measurement",
    "section": "Factors for success",
    "text": "Factors for success\nIn the span of one year, CJD went from collecting a large volume of useless data to a specific, targeted collection of data informed by best practices. The new collection process had high grantee compliance and stakeholder buy-in.\nThe following factors proved essential to getting to a place where we had useful, reliable data upon which to base future data science efforts:\n\n\nDiscontent with status quo\n\nThe Advisory Council wanted CJD to play a more active support role in the field. Meanwhile, everyone disliked the existing performance assessment process. As a result, most of the challenges we faced along the way related to implementation rather than defending the status quo on its merits.\n\n\n\nA catalyst for change\n\nDespite existing discontent, it took a funding shortfall to kickstart the process of change. It would have been unlikely for us to be able to create this system a priori.\n\n\n\nContinuous, high-quality communication\n\nWe could impose rules and requirements all day long, but without good faith engagement from the grantees we could never collect the quality of data we needed. Note that “continuous communication” does not mean “tell them everything you do at every point”. People become overwhelmed by torrents of information.\n\n\n\nHumility and flexibility\n\nHad we begun this process assuming we had all of the answers, we would have been dead in the water. Continuous outreach and willingness to take criticism and suggestions shaped the process as it progressed, ultimately producing a better end-product than we could have devised on our own.\n\n\n\nAn established program model\n\nDrug courts have been around for decades, with a vast body of supporting research and a community of practitioners and scholars we could speak to. That meant we could focus on implementation and execution instead of determining if the model worked or not.\n\n\n\nStrong leadership support\n\nFrom the very beginning, we could not have accomplished what we did without the full support and advocacy of our Executive Director."
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#coda-why-knowledge-transfer-is-vital",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#coda-why-knowledge-transfer-is-vital",
    "title": "The politics of performance measurement",
    "section": "Coda: Why knowledge transfer is vital",
    "text": "Coda: Why knowledge transfer is vital\nI wish I could write a follow-up article about how we started using classification modeling to identify the most successful programs and to promote better approaches and practices; about how we iterated the process through multiple funding cycles, tuning and perfecting it to better meet stakeholder needs. But I cannot.\nThe performance assessment system we built had some major weaknesses from the outset. It was labor intensive, not required by law, produced no immediate benefit to the agency itself, and was so new it had yet to be entrenched in agency practice. In other words, no institutional incentives worked in its favor. Only the continual push of our Executive Director and myself kept this new performance assessment system going, and once we left the agency, it foundered.\nStill, the experience taught me much. I learned first and foremost that programs do not sustain themselves. Most of our attention had been focused on building up the best process we could. Only a minimal effort had been spent on institutionalizing and sustaining it. We had written documentation but no fundamental changes in policy or rule. We had undertaken groundbreaking efforts and built relationships, but had not planned for any meaningful knowledge transfer to other staff. While we had intended to eventually do these things, fate took us away before we could get them in place.\nFor any kind of change to last, sustainability must be built in from the start. In the moment, these actions can seem low-priority. Policy and rule changes can be arduous and time-consuming. Knowledge transfer from one stably employed staff to another feels redundant and wasteful. But without embedding sustainability, no success will outlast the individual people pushing for it.\n\nBack to Careers\n\n\n\n\n\nAbout the author\n\nNoah Wright is a data scientist with the Texas Juvenile Justice Department. He is interested in the applications of data science to public policy in the context of real-world constraints, and the ethics thereof (ethics being highly relevant in his line of work). He can be reached on LinkedIn.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Noah Wright\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nWright, Noah. 2023. “The politics of performance measurement.” Real World Data Science, April 18, 2023. URL"
  },
  {
    "objectID": "careers/posts/2023/04/18/politics-of-performance-measurement.html#footnotes",
    "href": "careers/posts/2023/04/18/politics-of-performance-measurement.html#footnotes",
    "title": "The politics of performance measurement",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome newer types of courts (Commercial Sexual Exploitation, Mental Health, Veterans) had a much more limited body of research and had to be accommodated separately. For the sake of keeping this narrative coherent I’m focusing on drug courts, which were the majority of our programs.↩︎\nRodamar, Jeffery. 2018. “There ought to be a law! Campbell versus Goodhart.” Significance 15 (6): 9. https://doi.org/10.1111/j.1740-9713.2018.01205.x↩︎"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/03/28/tamanna-haque.html",
    "href": "careers/career-profiles/posts/2023/03/28/tamanna-haque.html",
    "title": "‘Data science challenges you to keep learning – there’ll always be new advances in the field’",
    "section": "",
    "text": "Hi, Tamanna. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nI’m Tamanna Haque. I’ve been working at Jaguar Land Rover for nearly four years, recently promoted to lead data scientist working within product engineering. It’s coming up to eight years that I’ve been working in the field, and my areas of interest are the use of machine learning to provide the best products and experiences for my customers and stakeholders.\nWhat does your job involve?\nMy role involves using the connected car and AI to make our products and customer experiences better, whilst leading within our wide data science team too. The data science team in Manchester, UK, originated with myself and one of my teammates – it’s since grown to nearly 40 (cross-sites and countries) and developed into a high-performing, advanced data science team.\nWhat makes us stand out is the nature of our work – we mostly use vehicle data (of participating customers), which is different to a lot of other commercial businesses or teams who’ll focus more on transactional or web data. The data we use lends itself to some pretty interesting projects, and a general futuristic feel here.\nI’m particularly interested and active in enabling a more electric and modern luxury future from the use of vehicle data.\nWhat does “data science” mean to you?\nThe realisation of value! Whether that is added revenue, saved costs or improved growth, I’m led by what data science can do for the business and its customers. The use of data science can open up many exciting, value-adding opportunities.\n\n\n\n\n\n\nPhoto supplied by Tamanna Haque, courtesy of Jaguar Land Rover. Used with permission.\n\n\n\n\n\nThere are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly.\n\n\n\nWhat do you think is your most important skill as a data scientist?\nI’ve always presented myself as a technically astute data scientist, even when entering leadership. But my niche is my ever-growing commercial awareness and passion about our products, customers and business. These aren’t new qualities, but they now align with my professional interests, as well as personal (I’ve been a fan of the Jaguar brand since childhood)!\nHow did you get into data science?\nI did a maths degree at the University of Manchester, where I specialised in statistics. I didn’t do any post-graduate education and this was fine for me.\nAfter graduating, I joined a digital fashion retailer (with a financial services proposition) as an analyst initially. I learned a lot about real-life data and analytics itself, whilst developing a rounded understanding about the business and how to deal with stakeholders cross-functionally. I must have served a few hundred at least(!) and left most of the ‘fancy’ stuff I learned at university aside, whilst getting to grips with so many aspects of commercial analytics. A great way for me to set solid foundations for what followed, and I personally feel this gives me a lens that others who dive straight into data science don’t have.\nI was soon attracted to data science because it tapped into what I learned at university and challenges you to keep learning; there’ll always be things to learn, and new advances in the field.\nWhat, or who, first inspired you to become a data scientist?\nI have a twin sister, we’ve always been together throughout education. Even before we graduated together, she secured her first role as an analyst. This opened my eyes to data, and data science followed for us both!\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nI had a few people tell me I couldn’t do data science, possibly because I didn’t fit the typical data scientist stereotype in several ways. I think attitudes in the field have changed over time though and on a personal level, it’s motivated me to give it everything, and I can’t regret that.\nAnd what are the challenges that you face now, as a working data scientist?\nI need to manage my diary well to ensure effectiveness and work-life balance. I’m overseeing people, other projects, doing public speaking and trying to remain hands on. I sometimes block out chunks of time in my diary – I need some meeting-free time to produce quality technical work. I try to finish on time and enjoy a very busy social life with my family and friends. A flexible attitude to how we work helps to keep me happy and energised whilst I’m delivering from various angles.\nThinking back to your earlier roles in data science, how do they compare to your current role?\nMy current role is very different to my previous roles. I’m continually learning and adapting how I can be a good leader, providing support to a breadth of colleagues (in and outside the team) whilst delivering myself. I’m actively involved in setting and refining our team’s strategy and I’m enjoying leading projects which either deliver high financial impact or help set the path in terms of new tech and/or machine learning capability. There is much more responsibility but it’s easy to stay energised when working on cars and for a business I’ve long admired.\nWhat was the most important thing you learned in your first year on the job?\nI should have had more confidence in myself, but this grew – as I adjusted to the new environment I became much more assertive. My domain knowledge and data science expertise combined help to build my self-confidence, credibility and reputation.\nWhat have been your career highlights so far?\nI’m most proud of my recent promotion from senior to lead data scientist. Also it was exciting for my family and I when I gained an offer to join Jaguar Land Rover.\nHave there been any mistakes or regrets along the way?\nNo, what’s meant to be will be!\nHow do you think your role will evolve over the rest of your career?\nMy progression has been relatively rapid, and I hope I’ve got many, many years ahead of me in my career. It’s hard to say how my role will evolve, I have a blend of responsibilities in my role which combined provide great fulfilment for me at the moment.\nIf you were starting out in data science now, what would you put at the top of your reading/study list?\nA good understanding of analytics and the domain you’re in are my recommended prerequisites to doing data science.\nAnalytics is an important part of the data science lifecycle, being able to get the data yourself and communicate results with influence, for example, are just a few aspects of analytics which underpin successful data science projects.\nAlso, without awareness of the business and industry you’re working in, you can become very dependent on others. Data science itself can be quite challenging, so it’s great to have a solid foundation before starting out.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nWith the level of continuous learning required to just simply keep up, it can be more of a lifestyle and not a job, so this is something to consider!\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nI still expect to see a skills gap in the field. There are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly, understanding samples versus populations, statistical testing, which type of regularisation to use in a neural network, et cetera.\nI also think there’s a challenge of questionable data science products reaching high levels of popularity and usage amongst the public… Some recent developments in this space have been extremely intelligent but raise ethical concerns. Just because something can be done with AI doesn’t mean it should, and my preferences are towards AI being ethical and (ideally) explainable.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Data science challenges you to keep learning – there’ll always be new advances in the field.’” Real World Data Science, March 28, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/06/28/albert-lee.html",
    "href": "careers/career-profiles/posts/2023/06/28/albert-lee.html",
    "title": "‘Living my identity takes courage. It is the same courage necessary to start a new business’",
    "section": "",
    "text": "This week, in celebration of Pride, Real World Data Science is collaborating with the JEDI Outreach Group of the American Statistical Association (ASA) and the ASA LGBTQ+ Advocacy Committee to highlight the achievements of statisticians and data scientists from across the LGBTQ+ spectrum.\nMembers of the committee nominated two individuals to be featured as part of our career profile series, and so we’re pleased to bring you interviews with Albert Lee (below) and Claire Morton.\nRead on to discover more about Albert’s data science career (so far).\n\n\n\nHi, Albert. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nMy name is Albert Lee. I’m the founding partner at Summit Consulting, a quantitative and financial consulting firm in Washington, DC. Summit delivers data-driven solutions to help make government effective and society just. I started Summit in 2003, and we recently celebrated our 20th anniversary.\nI received my PhD in economics from UCLA in 1999. My professional practice is focused on econometrics – an academic specialty that blends economic theory with statistical practices – and statistical sampling.\nWhat does your job involve?\nA large portion of my time is spent running Summit and making decisions about management, personnel, and business development. That said, I am still pretty active in technical topics. I am a testifying expert in econometrics and statistical sampling. Recently, I have been leading a team of data scientists who are reformulating the edit and imputation algorithms for the US Department of Agriculture’s National Agriculture Statistical Service, which collects survey data from US agriculture sectors.\nWhat do you think is your most important skill as a data scientist?\nExplaining technical concepts is a big part of my job, and it requires the ability to consume the technical literature and know the concepts well enough that I can explain them to a lay audience (such as lawyers, judges, and program staff).\nHow has your gender and/or sexual identity factored into your career?\nMy gender and identity have given me important perspective as a data scientist and an entrepreneur. Living my identity takes courage. It is the same courage necessary to start a new business. From a young age, my identity has conditioned me to be comfortable with differences.\nMy identity has also taught me to see similarity among differences. Empathy is essential in client services and especially in quantitative consulting, where some of my clients feel disempowered by the complex subject matter.\n\n\n\n\n\n\nAlbert Lee\n\n\n\n\n\nThe data science field is moving very fast. Every day brings a new algorithm, software program, and hardware innovation. Since data science is a multidisciplinary field, keeping up with it has been challenging.\n\n\n\nHow did you get into data science?\nAlthough I studied mathematics and economics as an undergraduate student and economics as a graduate student, my academic training was very theoretical. I didn’t work with data and computers extensively until my first job outside of academia in the early 2000s. Little did I know that it was the advent of the “big data” revolution.\nAt Summit we serve mostly federal agencies, who are sitting on decades of administrative data – information they collected as part of their mission but not of research quality. These agencies want to use their administrative data to automate their routine tasks (like predicting which loans will default first) and evaluate program efficacy (determining whether a training program reached its goals). Extracting and analyzing administrative data has been a big part of my career.\nWhen I founded Summit, data science was not a recognized discipline. But as the datasets get larger, decisions about hardware setup, software programs, estimation algorithms, and data virtualization have become increasingly intertwined and interdependent. This really was my first taste of data science as we know it today.\nWhat, or who, first inspired you to become a data scientist?\nThere are too many people to mention by name. I owe a lot of my career to my first two managers at KPMG, Alan Salzberg and Rick Holt. They taught me how to code and reason quantitatively. And Rob Gould at UCLA has patiently converted a theorist to an empiricist. Once a convert, now a zealot.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nI am an immigrant and a first-generation college graduate. My journey was full of unknowns. Figuring out my academic and professional career has taken a lot of exploration. In this regard, the same exploration that guided my identity also guided my academic and professional journey.\nAnd what are the challenges that you face now that you are working in data science?\nThe data science field is moving very fast. Every day brings a new algorithm, software program, and hardware innovation. Since data science is a multidisciplinary field, keeping up with it has been challenging. As I progress along my professional journey, striking the right balance between management, hands-on practice, and learning has been difficult as well.\nWhat was your first job in data science, and how does it compare to your current role?\nAs an entrepreneur, I was given a lot of professional freedom to actualize my career. To a large extent, I have the career that I envisioned. To me, data science lives in the intersection of methods, software, and hardware. I have spent a large part of my career in this intersection.\nOf course there are many things that were not part of the original vision, such as running a 100-person organization. My approach has always been intention with openness. By this metric, my current role is not far off from my original vision.\nWhat was the most important thing you learned in your first year on the job?\nThe ability and the love of learning constantly, regardless of the topic.\nWhat have been your career highlights so far?\nThe biggest highlight was that on June 15, 2023, Summit celebrated its 20th anniversary! Reformulating the National Agricultural Statistics Service’s edit and imputation systems is also a big deal. And being a testifying expert in some of the most consequential legal cases in the United States was a highlight as well.\nWhat three things are at the top of your current reading/study list?\nIn recent years, I have been binge-reading Stoic philosophy. I have read most books by Ryan Holiday. His most recent book was Ego Is the Enemy. In between the Stoics, you will find me reading Buddhist meditation literature, including Thich Nhat Hanh’s The Heart of the Buddha’s Teaching. David McCullough’s Truman is also by my bedside.\nWhat advice would you give for anyone wanting to be a data scientist?\nBe open and multidisciplinary. Many good ideas in statistics come from other fields, such as economics, medicine, sociology, and education. Computer science enables computational statistics. Having the openness to these topics is key.\nWhat new ideas or developments in the field are you personally most excited about or intrigued by?\nMachine learning has transformed statistics both as a consumer and a contributor. It consumes statistics in that it requires cutting-edge statistical techniques and algorithms for its estimation. Machine learning has important applications in many of the statistical sciences.\nAnd what do you think will be the main challenges facing the profession over the next few years?\nThe proper use of statistics or statistical ethics is an important societal challenge. Machine learning is becoming increasingly sophisticated, and its applications are more broad and pervasive. Machine learning algorithms are making more and more decisions in society, including mortgage loan approvals, residential home prices, and which prisoners receive parole. These are important and weighty decisions. How do we know that these decisions are unbiased and fair?\n\n\n\n\n\n\nAbout the ASA Pride Scholarship\n\n\n\nThe ASA Pride Scholarship was established to raise awareness for and support the success of LGBTQ+ statisticians and data scientists and allies. The scholarship will celebrate their diverse backgrounds and showcase the invaluable skills and perspectives these individuals bring to the ASA, statistics, and data science.\nApply or nominate someone for the ASA Pride Scholarship.\n\n\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Albert Lee is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Living my identity takes courage. It is the same courage necessary to start a new business.’” Real World Data Science, June 28, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/04/24/sami-rahman.html",
    "href": "careers/career-profiles/posts/2023/04/24/sami-rahman.html",
    "title": "‘I always thought someone like me couldn’t work in data, let alone data science’",
    "section": "",
    "text": "Hi, Sami. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nHello! I’m Sami Rahman, a passionate head of data engineering and data platform at Penguin Random House, the book publisher that has enriched lives through literature. I started my career in data science five years ago and I’ve evolved into a data generalist with expertise in machine learning, data infrastructure, and data strategy.\nWhat does your job involve?\nMy role is about harnessing the power of data to drive extraordinary outcomes. Leading a skilled team, we empower our company to leverage data and cutting-edge technologies for informed decisions and automation. I help shape our organisation’s capabilities in data science, analytics, machine learning, data management, and strategy.\nWhat does “data science” mean to you?\nData science, to me, is a captivating fusion of modern data technologies and computational statistics that tackles business challenges, crafts intelligent automation, and generates insightful revelations.\nWhat do you think is your most important skill as a data scientist?\nActive listening is key. A data scientist must be surgical and precise in developing models, analysis, and tools that reinforce the company’s bottom line and operations. Data science exists to create value using data.\n\n\n\n\n\n\nPhoto supplied by Sami Rahman, used with permission.\n\n\n\n\n\nAs I’ve transitioned into management, maintaining my coding prowess is an ongoing challenge. I stay sharp by doing data science and infrastructure development for fun, leveraging tools like ChatGPT and AirOps where I’m rusty.\n\n\n\nHow did you get into data science?\nI began with a psychology degree, which led to working as business psychologist where I discovered psychometric data analysis. After a master’s in countering organised crime and terrorism and a few short jobs in counter terrorism/intelligence, I decided that it wasn’t for me. I embraced my love for statistics and research, I dove into data science, learning Python through online platforms, and secured my first data scientist role at a WPP agency called Essence.\nWhat, or who, first inspired you to become a data scientist?\nI always thought someone like me couldn’t work in data, let alone data science. Dr Suzy Moat’s fascinating talk on machine learning’s application to human behaviour and psychology showed me that a psychologist could make a significant impact in this field, inspiring my aspiration to try to have a data science career.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nBreaking into data science without a typical background in maths/computer science/physics was daunting. Building a Kaggle portfolio and coding models for fun prepared me for interviews. Another challenge was learning to harmonise my “data brain” and “business brain” to solve problems efficiently. Understanding how data solutions impact business problems will always propel you forward. \nAnd what are the challenges that you face now, as a working data scientist?\nAs I’ve transitioned into management, maintaining my coding prowess is an ongoing challenge. I stay sharp by doing data science and infrastructure development for fun, leveraging tools like ChatGPT and AirOps where I’m rusty. I’m currently building my own cloud data platform and running a lot of image neural networks on it.\nWhat was your first job in data science, and how does it compare to your current role?\nAs an analytics executive at WPP agency Essence, I tackled data science, cloud engineering, and analytics problems for clients. They were a lot more singular and tactical in nature. Now, as head of data engineering and data platform at Penguin Random House, I focus on shaping data and technology strategy to align with the company’s broader vision.\nWhat was the most important thing you learned in your first year on the job?\nTo always consider the bigger picture: how your work integrates with the organisation/client’s objectives, delivers value, and aligns with the aspirations of other stakeholders. Actionable insights and value is the most important thing.\nWhat have been your career highlights so far?\nTwo shining moments include being the first of three of HSBC UK fraud data science leaders, where each of our departments tackled a different type of crime and protected our customers, and developing data strategies and capabilities for analytics, science, and business intelligence at Penguin Random House.\nHave there been any mistakes or regrets along the way?\nI regret not delving deeper into natural language processing (NLP) or spatial data science, which are now more accessible and growing fields within data science. I reckon the NLP methodologies would’ve been extremely useful seeing as I’m at a publishing company now!\nHow do you think your role will evolve over the rest of your career?\nAs data technologies become more accessible, I anticipate data roles will transform. I envision a future where data professionals focus on general AI, quantum machine learning, and multi-dimensional data analytics as traditional specialisms become democratised.\nIf you were starting out in data science now, what three things would you put at the top of your reading/study list?\nI’d recommend Skin in the Game by Nassim Nicholas Taleb, Calling Bullshit: The Art of Scepticism in a Data-Driven World by Jevin West and Carl Bergstrom,  and Artificial Intelligence: How Machine Learning Will Shape the Next Decade by Matthew Burgess.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nSuccess in data science hinges on understanding how it can transform organisations and engaging with business stakeholders. My advice: never stop listening to the business – the stakeholders are your biggest allies. I would also try to find your niche that sets you apart from everyone else. Mine when I first started in the field was my expertise on computational psychology and behavioural machine learning. \nWhat new ideas or developments in the field of data science are you personally most excited about or intrigued by?\nTransfer learning excites me most, as numerous large technology companies now offer pre-trained models based on billions/trillions of parameters. This will revolutionise industries worldwide, as it will be easier to build more performant models even if a company has less data.\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nThe challenge lies in staying relevant amidst the democratisation of data science. Through large language models, low-code, and transfer learning, advanced data science methods will become easier for non-specialists to do and use. Innovation and keeping up with modern data technologies will be crucial.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Sami Rahman is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I always thought someone like me couldn’t work in data, let alone data science.’” Real World Data Science, April 24, 2023. URL"
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "",
    "text": "A little over a month ago, governments, technology firms, multilateral organisations, and academic and civil society groups came together at Bletchley Park – home of Britain’s World War II code breakers – to discuss the safety and risks of artificial intelligence.\nOne output from that event was a declaration, signed by countries in attendance, of their resolve to “work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all.”\nWe also heard from UK prime minister Rishi Sunak of plans for an AI Safety Institute, to be based in the UK, which will “carefully test new types of frontier AI before and after they are released to address the potentially harmful capabilities of AI models, including exploring all the risks, from social harms like bias and misinformation, to the most unlikely but extreme risk, such as humanity losing control of AI completely.”\nBut at a panel debate at the Royal Statistical Society (RSS) the day before the Bletchley Park gathering, data scientists, statisticians, and machine learning experts questioned whether such an institute would be sufficient to meet the challenges posed by AI; whether data inputs – compared to AI model outputs – are getting the attention they deserve; and whether the summit was overly focused on AI doomerism and neglecting more immediate risks and harms. There were also calls for AI developers to be more driven to solve real-world problems, rather than just pursuing AI for AI’s sake.\nThe RSS event was chaired by Andrew Garrett, the Society’s president, and formed part of the national AI Fringe programme of activities. The panel featured:\nWhat follows are some edited highlights and key takeaways from the discussion."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "AI safety, and AI risks",
    "text": "AI safety, and AI risks\nAndrew Garrett: For those who were listening to the commentary last week, the PM [prime minister] made a very interesting speech. Rishi Sunak announced the creation of the world’s first AI Safety Institute in the UK, to examine, evaluate and test new types of AI. He also stated that he pushed hard to agree the first ever international statement about the risks of AI because, in his view, there wasn’t a shared understanding of the risks that we face. He used the example of the IPCC, the Intergovernmental Panel on Climate Change, to establish a truly global panel to publish a “state of AI science” report. And he also announced an investment in raw computing power, so around a billion pounds in a supercomputer, and £2.5 billion in quantum computers, making them available for researchers and businesses as well as government.\nThe RSS provided two responses this year to prominent [AI policy] reviews. The first was in June on the AI white paper, and the second was on the House of Lords Select Committee inquiry into large language models back in September. How do they relate to what the PM said? There’s some good news here, and maybe not quite so good news.\nFirst, the RSS had requested investments in AI evaluation and a risk-based approach. And you could argue, by stating that there will be a safety institute, that that certainly ticks one of the boxes. We also recommended investment in open source, in computing power, and in data access. In terms of computing power, that was certainly in the [PM’s] speech. We spoke about strengthening leadership, and in particular including practitioners in the [AI safety] debate. A lot of academics and maybe a lot of the big tech companies have been involved in the debate, but we want to get practitioners – those close to the coalface – involved in the debate. I’m not sure we’ve seen too much of that. We recommended that strategic direction was provided, because it’s such a fast-moving area, and the fact that the Bletchley Park Summit is happening tomorrow, I think, is good for that. And we also recommended that data science capability was built amongst the regulators. I don’t think there was any mention of that.\nThat’s the context [for the RSS event today]. What I’m going to do now is ask each of the panellists to give an introductory statement around the AI summit, focusing on the safety aspects. What do they see as the biggest risk? And how would they mitigate or manage this risk?\nDetlef Nauck: I work at BT and run the AI and data science research programme. We’ve been looking at the safety, reliability, and responsibility of AI for quite a number of years already. Five years ago, we put up a responsible AI framework in the company, and this is now very much tied into our data governance and risk management frameworks.\nLooking at the AI summit, they’re focusing on what they call “frontier models,” and they’re missing a trick here because I don’t think we need to worry about all-powerful AI; we need to worry about inadequate AI that is being used in the wrong context. For me, AI is programming with data, and that means I need to know what sort of data has been used to build the model, and I need AI vendors to be upfront about it and to tell me: What is the data that they have used to build it, how have they built it, or if they’ve tested for bias? And there are no protocols around this. So, therefore, I’m very much in favour of AI evaluation. But I don’t want to wait for an institute for AI evaluation. I want the academic research that needs to be done around this, which hasn’t been done. I want everybody who builds AI systems to take this responsibility and document properly what they’re doing.\n\n\n\n\n\n\n\n\n\n\n\nI hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\n\n\n\nMihaela van der Schaar: I am an AI researcher building AI and machine learning technology. Before talking about the risks, I also would like to say that I see tremendous potential for good. Many of these machine learning AI models can transform for the better areas that I find extremely important – healthcare and education. That being said, there are substantial risks, and we need to be very careful about that. First, if not designed well, AI can be both unsafe as well as biased, and that could lead to tremendous impact, especially in medicine and education. I completely agree with all the points that the Royal Statistical Society has made not only about open source but also about data access. This AI technology cannot be built unless you have access to high quality data, and what I see a lot happening, especially in industry, is people have data sources that they’ll keep private, build second-rate or third-rate technology on them, and then turn that into commercialised products that are sold to us for a lot of money. If data is made widely available, the best as well as the safest AI can be produced, rather than monopolised.\nAnother area of risk that I’m especially worried about is human marginalisation. I hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned as an AI researcher about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\nMartin Goodson: The AI Safety Summit is starting tomorrow. But, unfortunately, I think the government are focusing on the wrong risks. There are lots of risks to do with AI, and if you look at the scoping document for the summit, it says that what they’re interested in is misuse risk and the risk of loss of control. Misuse risk is that bad actors will gain access to information that they shouldn’t have and build chemical weapons and things like that. And the loss of control risk is that we will have this super intelligence which is going to take over and we should see, as is actually mentioned, the risk of the extinction of the human race, which I think is a bit overblown.\nBoth of these risks – the misuse risk and the loss of control risk – are potential risks. But we don’t really know how likely they are. We don’t even know whether they’re possible. But there are lots of risks that we do know are possible, like loss of jobs, and reductions in salary, particularly of white-collar jobs – that seems inevitable. There’s another risk, which is really important, which is the risk of monopolistic control by the small number of very powerful AI companies. These are the risks which are not just likely but are actually happening now – people are losing their jobs right now because of AI – and in terms of monopolistic control, OpenAI is the only company that has anything like a large language model as powerful as GPT-4. Even the mighty Google can’t really compete. This is a huge risk, I think, because we have no control over pricing: they could raise the prices if they wanted to; they could constrain access; they could only give access to certain people that they want to give access to. We don’t have any control over these systems.\nMark Levene: I work in NPL as a principal scientist in the data science department. I’m also emeritus professor in Birkbeck, University of London. I have a long-standing expertise in machine learning and focus in NPL on trustworthy AI and uncertainty quantification. I believe that measurement is a key component in locking-in AI safety. Trustworthy AI and safe AI both have similar goals but different emphases. We strive to demonstrate the trustworthiness of an AI system so that we can have confidence in the technology making what we perceive as responsible decisions. Safe AI puts the emphasis on the prevention of harmful consequences. The risk [of AI] is significant, and it could potentially be catastrophic if we think of nuclear power plants, or weapons, and so on. I think one of the problems here is, who is actually going to take responsibility? This is a big issue, and not necessarily an issue for the scientist to decide. Also, who is accountable? For instance, the developers of large language models: are they the ones that are accountable? Or is it the people who deploy the large language models and are fine-tuning them for their use cases?\nThe other thing I want to emphasise is the socio-technical characteristics [of the AI problem]. We need to get an interdisciplinary team of people to actually try and tackle these issues."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Do we need an AI Safety Institute?",
    "text": "Do we need an AI Safety Institute?\nAndrew Garrett: Do we need to have an AI Safety Institute, as Rishi Sunak has said? And if we don’t need one, why not?\nDetlef Nauck: I’m more in favour of encouraging academic research in the field and funding the kind of research projects that can look into how to build AI safely, [and] how to evaluate what it does. One of the key features of this technology is it has not come out of academic research; it has been built by large tech companies. And so, I think we have to do a bit of catch up in scientific research and in understanding how are we building these models, what can they do, and how do we control them?\nMihaela van der Schaar: This technology has a life of its own now, and we are using it for all sorts of things that maybe initially was not even intended. So, shall we create an AI [safety] institute? We can, but we need to realise first that testing AI and showing that it’s safe in all sorts of ways is complicated. I would dare say that doing that well is a big research challenge by itself. I don’t think just one institute will solve it. And I feel the industry needs to bear some of the responsibility. I was very impressed by Professor [Geoffrey] Hinton, who came to Cambridge and said, “I think that some of these companies should invest as much money in making safe AI as developing AI.” I resonated quite a lot with that.\nAlso, let’s not forget, many academic researchers have two hats nowadays: they are professors, and they are working for big tech [companies] for a lot of money. So, if we take this academic, we put them in this AI tech safety institute, we have potential for corruption. I’m not saying that this will happen. But one needs to be very aware, and there needs to be a very big separation between who develops [AI technology] and who tests it. And finally, we need to realise that we may require an enormous amount of computation to be able to validate and test correctly, and very few academic or governmental organisations may have [that].\n\n\n\n\nI think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\n\n\n\n\n\n\n\n\n\n\nMartin Goodson: Can I disagree with this idea of an evaluation institute? I think it’s a really, really bad idea, for two reasons. The first is an argument about fairness. If you look at drug regulation, who pays for clinical trials? It’s not the government. It’s the pharmaceutical companies. They spend billions on clinical trials. So, why do we want to do this testing for free for the big tech companies? We’re just doing product development for them. It’s insane! They should be paying to show that their products are safe.\nThe other reason is, I think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. I think it’s pathetic. We were one of the main leaders of the Human Genome Project, and we really pushed it – the Wellcome Trust and scientists in the UK pushed the Human Genome Project because we didn’t want companies to have monopolistic control over the human genome. People were idealistic, there was a moral purpose. But now, we’re so reduced that all we can do is test some APIs that have been produced by Silicon Valley companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\nMark Levene: Personally, I don’t see any problem in having an AI institute for safety or any other AI institutes. I think what’s important in terms of taxpayers’ money is that whatever institute or forum is invested in, it’s inclusive. One thing that the government should do is, we should have a panel of experts, and this panel should be interdisciplinary. And what this panel can do is it can advise government of the state of play in AI, and advise the regulators. And this panel doesn’t have to be static, it doesn’t have to be the same people all the time.\nAndrew Garrett: To evaluate something, whichever way you chose to do it, you need to have an inventory of those systems. So, with the current proposal, how would this AI Safety Institute have an inventory of what anyone was doing? How would it even work in practice?\nMartin Goodson: Unless we voluntarily go to them and say, “Can you test out our stuff?” then they wouldn’t. That’s the third reason why it’s a terrible idea. You’d need a licencing regime, like for drugs. You’d need to licence AI systems. But teenagers in their bedrooms are creating AI systems, so that’s impossible."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Let’s do reality-centric AI!",
    "text": "Let’s do reality-centric AI!\nAndrew Garrett: What are your thoughts about Rishi Sunak wanting the UK to be an AI powerhouse?\nMartin Goodson: It’s not going to be a powerhouse. This stuff about us being world leading in AI, it’s just a fiction. It’s a fairy tale. There are no real supercomputers in the UK. There are moves to build something, like you mentioned in your introduction, Andrew. But what are they going do with it? If they’re just going to build a supercomputer and carry on doing the same kinds of stuff that they’ve been doing for years, they’re not going to get anywhere. There needs to be a big project with an aim. You can build as many computers as you want. But if you haven’t got a plan for what to do with them, what’s the point?\nMihaela van der Schaar: I really would agree with that. What about solving some real problem: trying to solve cancer; trying to solve our crisis in healthcare, where we don’t have enough infrastructure and doctors to take care of us? What about solving the climate change problem, or even traffic control, or preventing the next financial crisis? I wrote a little bit about that, and I call it “let’s do reality-centric AI.” Let’s have some goal that’s human empowering, take a problem that we have – energy, climate, cancer, Alzheimer’s, better education for children, and more diverse education for children – and let us solve these big challenges, and in the process we will build AI that’s hopefully more human empowering, rather than just saying, “Oh, we are going to solve everything if we have general AI.” Right now, I hear too much about AI for the sake of AI. I’m not sure, despite all the technology we build, that we have advanced in solving some real-world problems that are important for humanity – and imminently important.\nMartin Goodson: So, healthcare– I tried to make an appointment with my GP last week, and they couldn’t get me an appointment for four weeks. In the US you have this United States Medical Licencing Examination, and in order to practice medicine you need to pass all three components, you need to pass them by about 60%. They are really hard tests. GPT-4 for gets over 80% in all three of those. So, it’s perfectly plausible, I think, that an AI could do at least some of the role of the GP. But, you’re right, there is no mission to do that, there is no ambition to do that.\nMihaela van der Schaar: Forget about replacing the doctors with ChatGPT, which I’m less sure is such a good idea. But, building AI to do the planning of healthcare, to say, “[Patient A], based on what we have found out about you, you’re not as high risk, maybe you can come in four weeks. But [patient B], you need to come tomorrow, because something is worrisome.”\nMartin Goodson: We can get into the details, but I think we are agreeing that a big mission to solve real problems would be a step forward, rather than worrying about these risks of superintelligences taking over everything, which is what the government is doing right now."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Managing misinformation",
    "text": "Managing misinformation\nAndrew Garrett: We have some important elections coming up in 2024 and 2025. We haven’t talked much about misinformation, and then disinformation. So, I’m interested to get your views here. How much is that a problem?\nDetlef Nauck: There’s a problem in figuring out when it happens, and that’s something we need to get our heads around. One thing that we’re looking at is, how do we make communication safe from bad actors? How do you know that you’re talking to the person you see on the camera and it’s not a deep fake? Detection mechanisms don’t really work, and they can be circumvented. So, it seems like what we need is new standards for communication systems, like watermarks and encryption built into devices. A camera should be able to say, “I’ve produced this picture, and I have watermarked it and it’s encrypted to a certain level,” and if you don’t see that, you can’t trust that what you see comes from a genuine camera, and it’s not artificially created. It’s more difficult around text and language – you can’t really watermark text.\nMark Levene: Misinformation is not just a derivative of AI. It’s a derivative of social networks and lots of other things.\nMihaela van der Schaar: I would agree that this is not only a problem with AI. We need to emphasise the role of education, and lifelong education. This is key to being able to comprehend, to judge for ourselves, to be trained to judge for ourselves. And maybe we need to teach different methods – from young kids to adults that are already working – to really exercise our own judgement. And that brings me to this AI for human empowerment. Can we build AI that is training us to become smarter, to become more able, more capable, more thoughtful, in addition to providing sources of information that are reliable and trustworthy?\nAndrew Garrett: So, empower people to be able to evaluate AI themselves?\nMihaela van der Schaar: Yes, but not only AI – all information that is given to us.\nMartin Goodson: On misinformation, I think this is really an important topic, because large language models are extremely persuasive. I asked ChatGPT a puzzle question, and it calculated all of this stuff and gave me paragraphs of explanations, and the answer was [wrong]. But it was so convincing I was almost convinced that it was right. The problem is, these things have been trained on the internet and the internet is full of marketing – it’s trillions of words of extremely persuasive writing. So, these things are really persuasive, and when you put that into a political debate or an election campaign, that’s when it becomes really, really dangerous. And that is extremely worrying and needs to be regulated.\n\n\n\n\n\n\n\n\n\n\n\nAt the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, ‘How did this information come about? Where did it come from?’\n\n\n\nMark Levene: You need ways to detect it. Even that is a big challenge. I don’t know if it’s impossible, because, if there’s regulation, for example, there should be traceability of data. So, at the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, “How did this information come about? Where did it come from?” But I agree that if you just look at an image or some text, and you don’t know where it came from, it’s easy to believe. Humans are easily fooled, because we’re just the product of what we know and what we’re used to, and if we see something that we recognise, we don’t question it."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#audience-qa",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#audience-qa",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Audience Q&A",
    "text": "Audience Q&A\n\nHow can we help organisations to deploy AI in a responsible way?\nDetlef Nauck: Help for the industry to deploy AI reliably and responsibly is something that’s missing, and for that, trust in AI is one of the things that needs to be built up. And you can only build up trust in AI if you know what these things are doing and they’re properly documented and tested. So that’s the kind of infrastructure, if you like, that’s missing. It’s not all big foundation models. It’s about, how do you actually use this stuff in practice? And 90% of that will be small, purpose-built AI models. That’s an area where the government can help. How do you empower smaller companies that don’t have the background of how AI works and how it can be used, how can they be supported in knowing what they can buy and what they can use and how they can use it?\nMark Levene: One example from healthcare which comes to mind: when you do a test, let’s say, a blood test, you don’t just get one number, you should get an interval, because there’s uncertainty. What current [AI] models do is they give you one answer, right? In fact, there’s a lot of uncertainty in the answer. One thing that can build trust is to make transparent the uncertainty that the AI outputs.\n\n\nHow can data scientists and statisticians help us understand how to use AI properly?\nMartin Goodson: One big thing, I think, is in culture. In machine learning – academic research and in industry – there isn’t a very scientific culture. There isn’t really an emphasis on observation and experimentation. We hire loads of people coming out of an MSc or a PhD in machine learning, and they don’t know anything, really, about doing an experiment or selection bias or how data can trip you up. All they think about is, you get a benchmark set of data and you measure the accuracy of your algorithm on that. And so there isn’t this culture of scientific experimentation and observation, which is what statistics is all about, really.\nMihaela van der Schaar: I agree with you, this is where we are now. But we are trying to change it. As a matter of fact, at the next big AI conference, NeurIPS, we plan to do a tutorial to teach people exactly this and bring some of these problems to the forefront, because trying really to understand errors in data, biases, confounders, misrepresentation – this is the biggest problem AI has today. We shouldn’t just build yet another, let’s say, classifier. We should spend time to improve the ability of these machine learning models to deal with all sorts of data.\n\n\nDo we honestly believe yet another institute, and yet more regulation, is the answer to what we’re grappling with here?\nDetlef Nauck: I think we all agree, another institute is not going to cut it. One of the main problems is regulators are not trained on AI, so it’s the wrong people looking into it. This is where some serious upskilling is required.\n\n\nAre we wrong to downplay the existential or catastrophic risks of AI?\nMartin Goodson: If I was an AI, a superintelligent AI, the easiest path for me to cause the extinction of the human race would be to spread misinformation about climate change, right? So, let’s focus on misinformation, because that’s an immediate danger to our way of life. Why are we focusing on science fiction? Let’s focus on reality.\n\n\nAI tech has advanced, but evaluation metrics haven’t moved forward. Why?\nMihaela van der Schaar: First, the AI community that I’m part of innovates at a very fast pace, and they don’t reward metrics. I am a big fan of metrics, and I can tell you, I can publish much faster a method in these top conferences then I can publish a metric. Number two, we often have in AI very stupid benchmarks, where we test everything on one dataset, and these datasets may be very wrong. On a more positive note, this is an enormous opportunity for machine learners and statisticians to work together and advance this very important field of metrics, of test sets, of data generating processes.\nMartin Goodson: The big problem with metrics right now is contamination, because most of the academic metrics and benchmark sets that we’re talking about, they’re published on the internet, and these systems are trained on the internet. I’ve already said that I don’t think this [evaluation] institute should exist. But if it did exist, there’s one thing that they could do, which is important, and that would be to create benchmark datasets that they do not publish. But obviously, you may decide, also, that the traditional idea of having a training set and a test set just doesn’t make any sense anymore. And there are loads of issues with data contamination, and data leakage between the training sets and the test sets."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Closing thoughts: What would you say to the AI Safety Summit?",
    "text": "Closing thoughts: What would you say to the AI Safety Summit?\nAndrew Garrett: If you were at the AI Safety Summit and you could make one point very succinctly, what would it be?\nMartin Goodson: You’re focusing on the wrong things.\nMark Levene: What’s important is to have an interdisciplinary team that will advise the government, rather than to build these institutes, and that this team should be independent and a team which will change over time, and it needs to be inclusive.\nMihaela van der Schaar: AI safety is complex, and we need to realise that people need to have the right expertise to be able to really understand the risks. And there is risk, as I mentioned before, of potential collusion, where people are both building the AI and saying it’s safe, and we need to separate these two worlds.\nDetlef Nauck: Focus on the data, not the models. That’s what’s important to build AI.\n\nDiscover more Viewpoints\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\nImages by Wes Cockx & Google DeepMind / Better Images of AI / AI large language models / Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” Real World Data Science, December 6, 2023. URL"
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "",
    "text": "Doomsaying is an old occupation. Artificial intelligence (AI) is a complex subject. It’s easy to fear what you don’t understand. These three truths go some way towards explaining the oversimplification and dramatisation plaguing discussions about AI.\nLast week, outlets around the world were plastered with news of yet another open letter claiming AI poses an existential threat to humankind. This letter, published through the nonprofit Center for AI Safety, has been signed by industry figureheads including Geoffrey Hinton and the chief executives of Google DeepMind, Open AI and Anthropic.\nHowever, I’d argue a healthy dose of scepticism is warranted when considering the AI doomsayer narrative. Upon close inspection, we see there are commercial incentives to manufacture fear in the AI space.\nAnd as a researcher of artificial general intelligence (AGI), it seems to me the framing of AI as an existential threat has more in common with 17th-century philosophy than computer science."
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Was ChatGPT a ‘breakthrough’?",
    "text": "Was ChatGPT a ‘breakthrough’?\nWhen ChatGPT was released late last year, people were delighted, entertained and horrified.\nBut ChatGPT isn’t a research breakthrough as much as it is a product. The technology it is based on is several years old. An early version of its underlying model, GPT-3, was released in 2020 with many of the same capabilities. It just wasn’t easily accessible online for everyone to play with.\nBack in 2020 and 2021, I and many others wrote papers discussing the capabilities and shortcomings of GPT-3 and similar models – and the world carried on as always. Forward to today, and ChatGPT has had an incredible impact on society. What changed?\nIn March, Microsoft researchers published a paper claiming GPT-4 showed “sparks of artificial general intelligence”. AGI is the subject of a variety of competing definitions, but for the sake of simplicity can be understood as AI with human-level intelligence.\nSome immediately interpreted the Microsoft research as saying GPT-4 is an AGI. By the definitions of AGI I’m familiar with, this is certainly not true. Nonetheless, it added to the hype and furore, and it was hard not to get caught up in the panic. Scientists are no more immune to group think than anyone else.\nThe same day that paper was submitted, The Future of Life Institute published an open letter calling for a six-month pause on training AI models more powerful than GPT-4, to allow everyone to take stock and plan ahead. Some of the AI luminaries who signed it expressed concern that AGI poses an existential threat to humans, and that ChatGPT is too close to AGI for comfort.\nSoon after, prominent AI safety researcher Eliezer Yudkowsky – who has been commenting on the dangers of superintelligent AI since well before 2020 – took things a step further. He claimed we were on a path to building a “superhumanly smart AI”, in which case “the obvious thing that would happen” is “literally everyone on Earth will die”. He even suggested countries need to be willing to risk nuclear war to enforce compliance with AI regulation across borders."
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "I don’t consider AI an imminent existential threat",
    "text": "I don’t consider AI an imminent existential threat\nOne aspect of AI safety research is to address potential dangers AGI might present. It’s a difficult topic to study because there is little agreement on what intelligence is and how it functions, let alone what a superintelligence might entail. As such, researchers must rely as much on speculation and philosophical argument as on evidence and mathematical proof.\nThere are two reasons I’m not concerned by ChatGPT and its byproducts.\nFirst, it isn’t even close to the sort of artificial superintelligence that might conceivably pose a threat to humankind. The models underpinning it are slow learners that require immense volumes of data to construct anything akin to the versatile concepts humans can concoct from only a few examples. In this sense, it is not “intelligent”.\nSecond, many of the more catastrophic AGI scenarios depend on premises I find implausible. For instance, there seems to be a prevailing (but unspoken) assumption that sufficient intelligence amounts to limitless real-world power. If this was true, more scientists would be billionaires.\nMoreover, cognition as we understand it in humans takes place as part of a physical environment (which includes our bodies), and this environment imposes limitations. The concept of AI as a “software mind” unconstrained by hardware has more in common with 17th-century dualism (the idea that the mind and body are separable) than with contemporary theories of the mind existing as part of the physical world."
  },
  {
    "objectID": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "href": "viewpoints/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Why the sudden concern?",
    "text": "Why the sudden concern?\nStill, doomsaying is old hat, and the events of the last few years probably haven’t helped – but there may be more to this story than meets the eye.\nAmong the prominent figures calling for AI regulation, many work for or have ties to incumbent AI companies. This technology is useful, and there is money and power at stake – so fearmongering presents an opportunity.\nAlmost everything involved in building ChatGPT has been published in research anyone can access. OpenAI’s competitors can (and have) replicated the process, and it won’t be long before free and open-source alternatives flood the market.\nThis point was made clearly in a memo purportedly leaked from Google entitled “We have no moat, and neither does OpenAI”. A moat is jargon for a way to secure your business against competitors.\nYann LeCun, who leads AI research at Meta, says these models should be open since they will become public infrastructure. He and many others are unconvinced by the AGI doom narrative.\n\n\n\nA NYT article on the debate around whether LLM base models should be closed or open.Meta argues for openness, starting with the release of LLaMA (for non-commercial use), while OpenAI and Google want to keep things closed and proprietary.They argue that openness can be…\n\n— Yann LeCun (@ylecun) May 18, 2023\n\n\n\nNotably, Meta wasn’t invited when US President Joe Biden recently met with the leadership of Google DeepMind and OpenAI. That’s despite the fact that Meta is almost certainly a leader in AI research; it produced PyTorch, the machine-learning framework OpenAI used to make GPT-3.\nAt the White House meetings, OpenAI chief executive Sam Altman suggested the US government should issue licences to those who are trusted to responsibly train AI models. Licences, as Stability AI chief executive Emad Mostaque puts it, “are a kinda moat”.\nCompanies such as Google, OpenAI and Microsoft have everything to lose by allowing small, independent competitors to flourish. Bringing in licensing and regulation would help cement their position as market leaders and hamstring competition before it can emerge.\nWhile regulation is appropriate in some circumstances, regulations that are rushed through will favour incumbents and suffocate small, free and open-source competition.\n\n\n\nThink Google or Microsoft are encouraging legislation for your safety? But of course! These are honorable companies.You might think they'd like less competition too though. Maybe a monopoly? Maybe legal red tape preventing free and open source alternatives? Perhaps other… https://t.co/Z7vSpMyuHg\n\n— Michael Timothy Bennett (@MiTiBennett) May 5, 2023\n\n\n\n\n\n\n\nDiscover more Viewpoints\n\n\n\n\n\nAbout the author\n\nMichael Timothy Bennett is a PhD student in the School of Computing, Australian National University.\n\n\n\n\n\nCopyright and licence\n\nThis article is republished from The Conversation under a Creative Commons license. Read the original article.\n\n\nThumbnail image by Alan Warburton / © BBC / Better Images of AI / Social Media / Licenced by CC-BY 4.0."
  },
  {
    "objectID": "viewpoints/posts/2023/09/18/pseudo-data-science.html",
    "href": "viewpoints/posts/2023/09/18/pseudo-data-science.html",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "",
    "text": "A typical article on data science hails new data sources, new tools, and new visualisations, and thereby supports the case for the value of data science.\nBut this article takes a different angle: it talks about potential pitfalls that can face data scientists. It is based on our work as the Office for Statistics Regulation (OSR), the UK’s regulator for official statistics. We see lots of great work done by statisticians in government. But we also see some of the challenges they face – and data scientists are also likely to encounter the same challenges.\nThe problems arise from the fact that neither statisticians nor data scientists do their work in isolation. The work usually takes places within organisations – businesses, government bodies, think tanks, academic institutions – and as a result, the statisticians and/or data scientists are not the only players who get to influence how data science is presented and used.\nWhat are the pitfalls we see in our work as regulator?"
  },
  {
    "objectID": "viewpoints/posts/2023/09/18/pseudo-data-science.html#pseudo-data-science",
    "href": "viewpoints/posts/2023/09/18/pseudo-data-science.html#pseudo-data-science",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Pseudo data science",
    "text": "Pseudo data science\nThe first type of pitfall is pseudo data science.\nPseudo data science is a term we use to describe attempts to pass off crude work as being more data science-y than it really is. That reflects a sense in public life that data science is new, innovative, somehow the Future. In this context, people who are not data scientists can be tempted to dress themselves up in the clothes of data science to enhance their credibility. This dressing up is usually well-intentioned – communications professionals who want to illuminate and explain complex issues in an engaging way.\nThe trouble is, it can sometimes backfire. In our work at OSR, we have over the last year seen several examples where organisations have sought to publish visualisations that look like they are the product of in-depth data analysis – when in fact they have been drawn by communications staff using graphic design packages. Examples include inflation, nurses pay, and comparisons of UK economic performance with other countries. To be fair, whenever we have pointed out issues like this, organisations have responded well, putting in place new procedures to ensure that analysts sign off on this kind of visualisations. Nevertheless, we suspect that the temptations to indulge in pseudo data science will remain strong – and we may need to intervene on similar cases in future."
  },
  {
    "objectID": "viewpoints/posts/2023/09/18/pseudo-data-science.html#unintelligent-transparency",
    "href": "viewpoints/posts/2023/09/18/pseudo-data-science.html#unintelligent-transparency",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Unintelligent transparency",
    "text": "Unintelligent transparency\nThe second pitfall is a failure of intelligent transparency.\nThere is a raw form of transparency – quoting a single number (a naked number we call it); or dumping data out into the public domain with no explanation. This is not intelligent transparency. The latter involves being clear where data come from, what their source is, and making underlying data available so that others can understand and verify the statements that are being made. Raw transparency and naked numbers treat an audience with little respect; intelligent transparency helps the audience understand and appreciate what sits behind high level claims.\nData science outputs can sometimes seem to communications teams easy to cherry pick for the most attractive number. Again, like pseudo data science, this reflects largely good intentions – to communicate complex things through ideas. But it becomes easy for a single, unsupported number to be used and reused until it loses most of its meaning. We call this weaponization of data, and it is the antithesis of intelligent transparency. And there is a lot of it about – for example the way in which the former Prime Minister of the UK talked repeatedly about employment; or claims about Scotland’s capacity for renewable energy. These examples indicate the pathology of weaponization that can impact data science outputs. They also act as a reminder that data scientists can counter weaponization of their own outputs by delivering engaging and insightful communication."
  },
  {
    "objectID": "viewpoints/posts/2023/09/18/pseudo-data-science.html#context-collapse",
    "href": "viewpoints/posts/2023/09/18/pseudo-data-science.html#context-collapse",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Context collapse",
    "text": "Context collapse\nThe third type of pitfall surrounds context collapse.\nThis idea comes from the work of the philosopher Lucy McDonald (who in turn has built on the ideas of danah boyd). What is context collapse? Imagine a swimming pool – with neat divisions of the pool into different lanes. All is clearly labelled – fast, medium, slow – for lane swimmers, who are in turn separated from the splash area for families and the deep end for divers. Removing the lanes, and thus taking away any signposting, increases the likelihood for things to go wrong. The fast swimmers doing front crawl clash with the slower breaststroke swimmers; both are constantly having to avoid the families with young children; and all need to watch for the periodic big splashes created by the divers. This is the online communication environment, in which formerly private and casual statements can go viral; in which a brief statement in a media environment can be picked up on and circulated many times; and in which some bad actors (the divers) may wish to disrupt deliberately the debate by breaking all the rules.\nHow can this affect data science? It happens when individual bits of data are taken from their context, and used in service of a different, and bigger, argument. A good example is data on Covid vaccinations. Here, UK organisations like the Office for National Statistics and the UK Health Security Agency published comprehensive data in good faith about vaccinations and their impact. Some of the underlying data, however, was taken out of the broader context and used in isolation to support criticisms of vaccines – criticisms that the wider evidence base did not support.\nThe challenge then became how the organisations should respond. At an organisational level, they did not wish to withdraw the data – because that would reduce transparency. Instead they sought to both caveat their data more clearly; and directly rebut the more egregious misuses of the data. In a sense, then, what began as an individual analytical output became part of a broader organisational judgement on positioning in the face of misinformation.\nIt is fair to say that, against this third pitfall, there is not yet a clear consensus on how to address it. Practice is emerging all the time and we at OSR continue to support producers of data as they grapple with it.\nThere are other potential pitfalls to using data science. But what unites these three – pseudo data science; unintelligent transparency; and context collapse – is that they relate to situations where data science rubs up against broader organisational dynamics, around communications, presentation and organisational strategy.\nAnd the meta-message is this: for data scientists to thrive in organisations, they need to be good at more than data science. They need to be skilled at working alongside and influencing colleagues from other functions. Only through this form of data leadership can the pitfalls be dealt with effectively.\n\n\n\n\n\n\nThis article is based on a presentation at the Data Science for Health Equity group in May 2023.\n\n\n\n\nDiscover more Viewpoints\n\n\n\n\n\nAbout the author\n\nEd Humpherson is head of the Office for Statistics Regulation, which provides independent regulation of all official statistics in the UK. The aim of OSR is to enhance public confidence in the trustworthiness, quality and value of statistics produced by government.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Ed Humpherson\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nHumpherson, Ed. 2023. “‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading.” Real World Data Science, September 18, 2023. URL"
  },
  {
    "objectID": "rwds-partners.html",
    "href": "rwds-partners.html",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk"
  },
  {
    "objectID": "rwds-partners.html#publisher",
    "href": "rwds-partners.html#publisher",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk"
  },
  {
    "objectID": "rwds-partners.html#partners",
    "href": "rwds-partners.html#partners",
    "title": "Our partners and funders",
    "section": "Partners",
    "text": "Partners\n\n\n\n\n\nThe American Statistical Association is the world’s largest community of statisticians, the “Big Tent for Statistics.” It is the second-oldest, continuously operating professional association in the US. Since it was founded in Boston in 1839, the ASA has supported excellence in the development, application, and dissemination of statistical science through meetings, member services, education, publications, advocacy, and accreditation.\nOur members serve in industry, government, and academia in more than 90 countries, advancing research and promoting sound statistical practice to inform public policy and improve human welfare.\nTo support the work of the ASA, become a member today.\nEmail: asainfo@amstat.org"
  },
  {
    "objectID": "rwds-partners.html#funders",
    "href": "rwds-partners.html#funders",
    "title": "Our partners and funders",
    "section": "Funders",
    "text": "Funders\n\n\n\n\n\nReal World Data Science was supported by startup funding from The Alan Turing Institute.\nThe Alan Turing Institute, headquartered in the British Library, London, was created as the UK’s national institute for data science in 2015. In 2017, as a result of a government recommendation, artificial intelligence was added to its remit. \nThe Institute is named in honour of Alan Turing (23 June 1912 – 7 June 1954), whose pioneering work in theoretical and applied mathematics, engineering and computing are considered to be the key disciplines comprising the fields of data science and artificial intelligence.\nTo find out more about The Alan Turing Institute, its strategy and programme of work, visit turing.ac.uk."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: b.tarran@rss.org.uk\nGitHub: @realworlddatascience\nTwitter: @rwdatasci\nLinkedIn: RSS Real World Data Science\nMastodon: @rwdatasci"
  },
  {
    "objectID": "contact.html#editorial",
    "href": "contact.html#editorial",
    "title": "Contact us",
    "section": "",
    "text": "Email: b.tarran@rss.org.uk\nGitHub: @realworlddatascience\nTwitter: @rwdatasci\nLinkedIn: RSS Real World Data Science\nMastodon: @rwdatasci"
  },
  {
    "objectID": "contact.html#advertising-and-commercial",
    "href": "contact.html#advertising-and-commercial",
    "title": "Contact us",
    "section": "Advertising and commercial",
    "text": "Advertising and commercial\nEmail: advertising@rss.org.uk"
  },
  {
    "objectID": "contributor-docs/training-guides.html",
    "href": "contributor-docs/training-guides.html",
    "title": "Training guides",
    "section": "",
    "text": "In data science, there’s no one-size-fits-all solution to every problem and challenge. So, part of the job of the data scientist is to rapidly learn about different sub-domains, tools and techniques, and put those learnings into practice.\nBut it can be time-consuming to figure out what you need to learn and in what order, and to identify the best resources for doing so. This is where our Training guides come in. Each will set out a learning pathway for data scientists to follow, with recommendations of textbooks, videos, practical exercises and other teaching material to use every step of the way."
  },
  {
    "objectID": "contributor-docs/training-guides.html#structure",
    "href": "contributor-docs/training-guides.html#structure",
    "title": "Training guides",
    "section": "Structure",
    "text": "Structure\nContributors should think about their training guides as being short online courses that are constructed from existing high-quality material. You do not need to create your own “course” content. Rather, you should focus on recommending texts and other material for users to follow in a logical ordered way, so that they may build up and secure their knowledge of a particular topic.\nGuides should feature a mix of content types – not only text, but audio and video – and they should provide ample opportunities for users to practice what they are learning.\nA brief and extremely simplified example of a guide is as follows:\n\nStep 1: Watch this introductory video on Topic X.\nStep 2: Now you are familiar with the basics of Topic X, you will want to read Chapter 2 of Textbook Y, which delves into more of the mathematical underpinnings.\nStep 3: Let’s try Topic X ourselves. This GitHub repository has code for you to run it in Python. Copy the code and give it a go.\nStep 4: You should now be ready to apply Topic X to a simple data challenge. Check out this Kaggle page and practice what you have learned so far.\nStep 5: We’re now moving from the “beginner” level to “intermediate”, and Training Course Z gives a thorough overview of what you need to know for the next stage of your learning journey.\n… etc."
  },
  {
    "objectID": "contributor-docs/training-guides.html#advice-and-recommendations",
    "href": "contributor-docs/training-guides.html#advice-and-recommendations",
    "title": "Training guides",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nBe mindful of different learning styles. Some people prefer to read, others prefer to watch or listen. So, wherever possible, for each stage of your training guide, try to provide a mix of resources that meet the same learning objectives.\nConsider barriers to entry. Data scientists in large organisations may have access to training budgets or mechanisms to apply for training funds. But that isn’t the case for all data scientists, meaning that paid-for materials and training courses might not be accessible to everyone. Recommend them sparingly, and if there are ways to access the material at reduced rates do let users know. However, you must not link to illicit copies of material – e.g., unauthorised PDF reproductions of textbooks.\nIf there are resource gaps, please tell us. While planning out your training guide, you may well struggle to find the perfect piece of content to recommend at a particular stage of your learning journey. If that is the case, do get in touch with us. One of the goals of Real World Data Science is to identify and plug these sorts of gaps, so that all in the data science community can benefit. We’ll sketch out a commission and take it out to our network of contacts. Or perhaps it’ll be something you want to create for the site!"
  },
  {
    "objectID": "contributor-docs/recommender.html",
    "href": "contributor-docs/recommender.html",
    "title": "Recommenders",
    "section": "",
    "text": "Too much content, not enough time. That about sums up the problem facing the data science community. So, our Recommenders are here to help. Contributors are invited to submit lists (or Feeds) of high-quality sources on all manner of topics – from foundational ideas in data science and cutting-edge techniques, to opinion and thought-leadership on the future of the profession. Reviews of new books and other material are also welcome."
  },
  {
    "objectID": "contributor-docs/recommender.html#article-types-and-structures",
    "href": "contributor-docs/recommender.html#article-types-and-structures",
    "title": "Recommenders",
    "section": "Article types and structures",
    "text": "Article types and structures\n\nFeeds\nFeeds can be constructed around different topics and audiences. For example, you might want to recommend to all data scientists the “10 best blogs on machine learning” or “5 data visualisation experts to follow on Twitter”. Or you might have a list of sources specifically targeted at data science educators (e.g., “the best books on teaching coding”) or data science leaders (“5 insightful case studies on building data science teams”).\nWhatever you choose to focus on, the following outline provides a basic guide for structuring your feed:\n\n\nOverview\n\nA brief introduction to your list, its main focus, who you are writing it for, and why. You should also say something about yourself and your background, too. This will give additional context to the recommendations you are making.\n\n\n\nList of sources\n\nAs well as naming your sources and telling people how to find them, you should also explain why you are recommending them, how they helped you in your career or studies, or other reasons why you find them to be of value. Sample quotes or small excerpts from the sources themselves might also be worth including.\n\n\n\nStart a dialogue\n\nConclude with a call for readers to share recommendations of their own, either in the article comments or on social media. Contributors are welcome to update their lists any time with new sources, including those suggested by site users.\n\n\n\n\n\nReviews\nUnlike the feeds described above, each submitted review should focus only on a single publication. It must be an honest summary of the reviewer’s thoughts, feelings and impressions, covering what they liked and didn’t like, the perceived strengths and weaknesses of the publication, and whether it is likely to be of interest and value to its intended audience. Reviews should of course provide an overview of the publication in question but must avoid dry, itemised descriptions of the publication’s constituent parts (e.g., listing out the chapters in a book).\nAll reviews should list the following information (if relevant):\n\nTitle of publication\nAuthor(s)\nDate of publication\nEdition or format used for review\nPublisher\nLength\nPrice\nWebsite address or other source of further information"
  },
  {
    "objectID": "contributor-docs/recommender.html#advice-and-recommendations",
    "href": "contributor-docs/recommender.html#advice-and-recommendations",
    "title": "Recommenders",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nKeep lists to a sensible size. Feeds are meant to help data scientists to prioritise who and what to follow based on their interests and career stage – and it is much easier to keep tabs on 5 sources than it is 50, or even 15! So, the fewer the better.\nKeep your recommendations up to date. In this era of digital publishing, things can and do change overnight. So, if one of your recommended bloggers stops blogging, or the author of one of your favourite books makes a major update to the text, do be sure to let us – and your audience – know. We want to keep feeds and reviews current and useful.\nOf course you are brilliant, but… Please do not recommend or review your own publications or those in which you have a pecuniary or similar interest."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html",
    "href": "contributor-docs/call-for-contributions.html",
    "title": "Call for contributions",
    "section": "",
    "text": "Real World Data Science aims to inform, inspire and strengthen the data science community by showcasing real-world examples of data science practice and bringing together data scientists to share knowledge.\nWe cannot succeed in these aims without the support and contributions of the data science community, so thank you for taking the time to review this open call for contributions."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "href": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "title": "Call for contributions",
    "section": "What are we looking for?",
    "text": "What are we looking for?\nBelow is a list of our core content areas. We welcome submissions in any of these areas. Each content area is linked to its own set of notes for contributors.\n\nCase studies\nExplainers\nExercises\nDatasets\nTraining guides\nRecommenders\nDataScienceBites\n\nSubmissions can focus on any and all topics and application areas. We want our content to reflect the breadth and depth of real-world data science."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#our-target-audience",
    "href": "contributor-docs/call-for-contributions.html#our-target-audience",
    "title": "Call for contributions",
    "section": "Our target audience",
    "text": "Our target audience\nReal World Data Science is for all who work in data science – whether they are students, teachers, practitioners or leaders. Submissions do not have to appeal to all data scientists, however. Contributors should think carefully about who they are trying to reach, and craft their submissions accordingly."
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "href": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "title": "Call for contributions",
    "section": "What can submissions include?",
    "text": "What can submissions include?\nWe encourage contributors to experiment with and include different media formats in their submissions – text, images, audio and video. And as our site is built on Quarto – the new open-source publishing system developed by Posit – submissions to Real World Data Science can also include code cells, equations, figures, interactive data displays, and other elements to enrich the user experience.\nIf you haven’t used Quarto before, check out this fantastic tutorial from the developers. You can also explore some of the range of Quarto features that we use in this GitHub template repository, created by Finn-Ole Höner. It’s an excellent resource to help Real World Data Science contributors get started!"
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#how-to-submit",
    "href": "contributor-docs/call-for-contributions.html#how-to-submit",
    "title": "Call for contributions",
    "section": "How to submit",
    "text": "How to submit\nOnce you’ve reviewed our notes for contributors and settled on a content area, theme and audience, please review our contributor guidelines for details on the submission process."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html",
    "href": "contributor-docs/contributor-guidelines.html",
    "title": "Contributor guidelines",
    "section": "",
    "text": "Thank you for your interest in contributing to Real World Data Science. This page will walk you through the process of preparing and submitting your idea. If you haven’t done so already, please review our call for contributions before continuing."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "href": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "title": "Contributor guidelines",
    "section": "Site functionality and ethos",
    "text": "Site functionality and ethos\nReal World Data Science is built on Quarto, the new open-source publishing system developed by Posit. The site has been designed from the ground up as a platform for data scientists, created by data scientists. Here’s what this means in practice:\n\nContributors can use data science software and tools to create content – e.g. Visual Studio Code, RStudio, Jupyter Lab; Python, R, Observable, and Shiny – allowing for the full integration of text, code, figures, equations, and other elements.\nReview and editing are transparent and collaborative, again making use of tools data scientists are familiar with – e.g. GitHub, Google Docs – for sharing and revising documents prior to publication.\nContent can be both engaging and interactive. Many data scientists learn by doing, so code can be made available as R Markdown or Jupyter Notebook files to be reused and experimented with offline. Or, the same documents can be used online through tools like Google Colab and Binder. Where appropriate, the use of interactive displays and Shiny apps is encouraged, allowing for data visualisations to be interrogated and regenerated on the fly.\nSite users are contributors too. Through annotation and commenting functionality, site users can interact and converse with authors and other members of the Real World Data Science community. And with all source files hosted on GitHub, users of our site can raise issues, or fork and propose improvements – leading to a true exchange of knowledge."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "href": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "title": "Contributor guidelines",
    "section": "The submission process",
    "text": "The submission process\n\nContact site editor Brian Tarran to discuss your proposed submission.\nWrite up a short content brief containing the following:\n\nTitle of submission\nAuthor name(s) and affiliation(s)\nTheme/topic area\nTarget audience\nSynopsis or sell line, summarising the story and its importance/value (250 words max.)\nKey audience takeaways\nFormats and features (e.g., text, audio, video; code blocks, interactive data visualisations, etc.)\nAccessibility considerations\nTarget length/word count\nFirst draft to be submitted by…\n\nThe RWDS_post_template repository on GitHub contains a Quarto document (content-brief.qmd) that can be used to produce a content brief in the style and format of a Real World Data Science article.\nOnce a content brief is finalised and approved, content is to be prepared in the agreed format and with reference to our style guide. For simple text-based articles, we recommend using Google Docs or Microsoft Word; for submissions that incorporate technical or multimedia content, such as code, equations or interactive graphics, we recommend the Quarto (.qmd) file format. Use the RWDS_post_template repository to create your draft article in Quarto using the correct style and formatting. A sample article in the repo (report.qmd) contains code examples for the Quarto features used by Real World Data Science. Documents can be submitted in Jupyter notebook (.ipynb) and R Markdown (.Rmd) formats but will require conversion before publishing.\nDraft submissions should be sent via email to the editor. Alternatively, contributors can commit their drafts to their own GitHub accounts using the RWDS_post_template repository and add site editor Brian Tarran as a collaborator."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "href": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "title": "Contributor guidelines",
    "section": "Copyright and content licencing",
    "text": "Copyright and content licencing\nContributors retain copyright of their work, but agree to publish their work under a Creative Commons licence. Contributors are free to choose the licence that best suits their content. The chosen licence should be indicated on the draft submission."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-review-process",
    "href": "contributor-docs/contributor-guidelines.html#the-review-process",
    "title": "Contributor guidelines",
    "section": "The review process",
    "text": "The review process\nDraft submissions will be shared for review with members of the Real World Data Science Editorial Board. Comments and edits to documents will be made via Google Docs/MS Word/GitHub, allowing for (a) version control, (b) open dialogue between reviewers and contributors, and (c) a transparent and well-documented review process.\nOnce revisions are complete and content is accepted for publication, authors will be provided with HTML files to preview published content. Following sign-off by author and editor, HTML files will be made live."
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#post-publication",
    "href": "contributor-docs/contributor-guidelines.html#post-publication",
    "title": "Contributor guidelines",
    "section": "Post-publication",
    "text": "Post-publication\nContributors and editors will work together to promote content via social media platforms – Twitter, LinkedIn, blogs – and in other channels as appropriate – e.g., in response to related questions on Quora or Stack Overflow.\nContributors are encouraged to monitor their content regularly for user comments and discussions. Engaging in discussions with users – whether through the Real World Data Science platform or via social media and other channels – is an effective way of developing an audience: it builds profile for the contributor and their content, and encourages other users to find and interact with content."
  },
  {
    "objectID": "contributor-docs/style-guide.html",
    "href": "contributor-docs/style-guide.html",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to Real World Data Science are creating content for their colleagues and peers and should “speak” to them as such."
  },
  {
    "objectID": "contributor-docs/style-guide.html#tone",
    "href": "contributor-docs/style-guide.html#tone",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to Real World Data Science are creating content for their colleagues and peers and should “speak” to them as such."
  },
  {
    "objectID": "contributor-docs/style-guide.html#structure",
    "href": "contributor-docs/style-guide.html#structure",
    "title": "Style guide",
    "section": "Structure",
    "text": "Structure\nEach contribution must, in effect, tell “a story”, and so contributors need to be clear (a) what their story is, (b) why people should be interested, and (c) what its main message or key takeaways are. To help figure this out, we recommend contributors apply the XY Story Formula."
  },
  {
    "objectID": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "href": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "title": "Style guide",
    "section": "Technical content and jargon",
    "text": "Technical content and jargon\nTechnical content is a necessary feature of a site like ours. Without it, an article or other piece of content may be of little practical use to a technical audience. But if there’s too much of it, even experts may struggle to stay engaged. Contributors are also faced with a dilemma when it comes to explaining technical content: explain nothing, and you risk alienating some of your audience; explain everything, and you’ll struggle to establish a clear, strong narrative thread. So, careful consideration is required:\n\nWho is my audience for this article?\nWhat is this audience likely to know already, and what needs to be explained?\nIf something needs to be explained, can I do so briefly and then link to other resources? Or is a full explanation required?\nIn telling my “story”, what are the absolute-need-to-knows, and what are the simply-nice-to-knows?\n\nThinking through these questions will help contributors to find the right mix of valuable, technical content paired with accessible, readable narrative.\nKeep in mind that the same general advice applies to the use of industry jargon. Jargon can be a valuable shorthand when communicating with people working in the same organisation or sector, but those working in different fields may struggle to make sense of it. So, contributors need to think carefully about how much jargon to use, and what needs to be explained."
  },
  {
    "objectID": "contributor-docs/style-guide.html#figuresgraphics",
    "href": "contributor-docs/style-guide.html#figuresgraphics",
    "title": "Style guide",
    "section": "Figures/graphics",
    "text": "Figures/graphics\nAll data visualisations and other graphical outputs directly related to the content of submissions must be presented neatly and cleanly (avoid chart junk). They should also be labelled correctly and legibly, with colours chosen carefully to ensure they can be easily distinguished by all readers. Accompanying captions must be written to support the reader’s understanding of the visual presentation (e.g., “Figure 1: a bar chart” is an insufficient description).\nIf contributors wish to use charts or graphs that are not their own work, they must ensure that such items are correctly sourced and referenced, and that permission to republish has been obtained. A letter or email confirming this permission is required."
  },
  {
    "objectID": "contributor-docs/style-guide.html#data-sources",
    "href": "contributor-docs/style-guide.html#data-sources",
    "title": "Style guide",
    "section": "Data sources",
    "text": "Data sources\nContributors must include within their submissions any links and/or references to the sources of data, code and/or software and software packages on which their analyses are based. We understand that some data sources may not be publicly available, whether for legal, ethical or commercial reasons. However, readers must still be told where the data come from, even if they are not able to access the data themselves."
  },
  {
    "objectID": "contributor-docs/style-guide.html#references",
    "href": "contributor-docs/style-guide.html#references",
    "title": "Style guide",
    "section": "References",
    "text": "References\nCitations are to be formatted in The Chicago Manual of Style author-date format."
  },
  {
    "objectID": "contributor-docs/style-guide.html#use-of-images",
    "href": "contributor-docs/style-guide.html#use-of-images",
    "title": "Style guide",
    "section": "Use of images",
    "text": "Use of images\nImages for general illustration purposes will be sourced and – where necessary and within reason – paid for by Real World Data Science.\n\n\n\n\n\n\n\nNote\n\n\n\nFor all other style-related matters, we follow The Guardian and Observer Style Guide."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html",
    "title": "A demonstration of the law of the flowering plants",
    "section": "",
    "text": "This tutorial will demonstrate a popular method for predicting the day a flower will bloom. There are many reasons why you might want to predict a bloom date. You might be a scientist studying ecosystems stressed by climate change. Or you might be planning a trip to Amsterdam and would like to time your stay to when the tulips are in bloom. Or maybe you are participating in the annual Cherry Blossom Prediction Competition and want some ideas to help you get started.\nIn any case, you might be surprised to learn that the day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The mathematical rule developed in the eighteenth century to make these predictions – now called the “law of the flowering plants” – shaped the direction of statistics as a field and is still used by scientists with relatively few changes.\nWe present the law of the flowering plants as it was stated by Adolphe Quetelet, an influential nineteenth century statistician. Upon completing this tutorial, you will be able to:\nAt the end of the tutorial, we challenge you to design an algorithm that beats our predictions. The tutorial uses the R programming language. In particular, the code relies on the following packages:"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "The law of the flowering plants",
    "text": "The law of the flowering plants\nWe begin by reviewing the law of the flowering plants as it was stated by Adolphe Quetelet. You may already know Quetelet as the inventor of the body mass index. Less known is that Quetelet recorded the bloom dates of hundreds of different plants between 1833 and 1852 at the Brussels Observatory, which he founded and directed. Quetelet reported that a plant flowers when exposed to a specific quantity of heat, measured in degrees of Celsius squared (°C²). For example, he calculated that a lilac blooms when the sum of the daily temperatures squared exceeds 4264°C² following the last frost.\nHe communicated this law in his Letters addressed to HRH the grand duke of Saxe-Coburg and Gotha (Number 33, 1846; translated 1849) and in his reporting On the climate of Belgium (Chapter 4, Part 4, 1848; data updated in Part 7, 1857). A picture of Quetelet and the title page of On the climate of Belgium are displayed in Figure 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Quetelet reported on the law of the flowering plants in On the climate of Belgium (1857). Sources: Wikimedia Commons, Gallica.\n\nQuetelet was not the first to study bloom dates. Anthophiles have recorded the dates that flowers bloom for centuries. Written records of cherry trees go back as far as 812 AD in Japan and peach and plum trees as far as 1308 AD in China. Systematic record keeping began a century before Quetelet with Robert Marsham’s Indications of Spring (1789).\nQuetelet was also not the first to study the relationship between temperature and bloom dates. René Réaumur (1735), an early adopter of the thermometer, noted the relationship before Marsham published his Indications. But Quetelet was the first to systematically study the relationship across a wide variety of plants and derive the amount of heat needed to bloom. An example of Quetelet’s careful record keeping can be seen in Figure 2, one of many tables he reported in his publications.\n\n\n\n\n\n\nFigure 2: Bloom dates at Brussels Observatory observed by Quetelet between 1839 and 1852. Source: Gallica."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Reproducing Quetelet’s law of the flowering plants",
    "text": "Reproducing Quetelet’s law of the flowering plants\nTo reproduce Quetelet’s law, we combine the data in Figure 2 with additional observations from his Letters. We focus on Quetelet’s primary example, the bloom date of the common lilac, Syringa vulgaris, row 18 of Figure 2. We do this because Quetelet carefully describes his methodology for measuring the bloom date of lilacs. For example, Quetelet considers a lilac to have bloomed when “the first corolla opens and shows the stamina.” That event is closest to what the USA Phenology Network describes as “open flowers”, depicted in the center image of Figure 3 below. This detail will become relevant when we attempt to replicate Quetelet’s law in a later section. Note that although we focus on lilacs in this tutorial, the R code is easily edited to predict the day that other plants will bloom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The bloom date occurs when the first corolla opens and shows the stamina (center image). Source: USA National Phenology Network.\n\nIn the R code below, the five-column tibble lilac contains the date each year that Quetelet observed the lilacs bloom at Brussels Observatory. The first three columns are the month, day, and year the lilacs bloomed between 1839 and 1852. These columns are combined to form the fourth column, the full date the lilacs bloomed. The last column converts the date to the day of the year the lilacs bloomed, abbreviated “doy.” That is, “doy” is the number of days it took for the lilacs bloom following January 1. Both “date” and “doy” representations of Quetelet’s observations will be useful throughout this tutorial.\n```{r}\nlilac &lt;-                   \n  tibble(month = c(\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n                   \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"),\n         day   =  c(10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12),\n         year  = 1839:1852,\n         date  = as.Date(paste(month, day, year), format = \"%B %d %Y\"),\n         doy   = parse_number(format(date, \"%j\"))) \n\nlilac %&gt;% \n  kable(align = \"c\",\n        caption = \"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\n\n\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\nMay\n\n\n10\n\n\n1839\n\n\n1839-05-10\n\n\n130\n\n\n\n\nApril\n\n\n28\n\n\n1840\n\n\n1840-04-28\n\n\n119\n\n\n\n\nApril\n\n\n24\n\n\n1841\n\n\n1841-04-24\n\n\n114\n\n\n\n\nApril\n\n\n28\n\n\n1842\n\n\n1842-04-28\n\n\n118\n\n\n\n\nApril\n\n\n20\n\n\n1843\n\n\n1843-04-20\n\n\n110\n\n\n\n\nApril\n\n\n25\n\n\n1844\n\n\n1844-04-25\n\n\n116\n\n\n\n\nMay\n\n\n13\n\n\n1845\n\n\n1845-05-13\n\n\n133\n\n\n\n\nApril\n\n\n12\n\n\n1846\n\n\n1846-04-12\n\n\n102\n\n\n\n\nMay\n\n\n9\n\n\n1847\n\n\n1847-05-09\n\n\n129\n\n\n\n\nApril\n\n\n21\n\n\n1848\n\n\n1848-04-21\n\n\n112\n\n\n\n\nMay\n\n\n2\n\n\n1849\n\n\n1849-05-02\n\n\n122\n\n\n\n\nApril\n\n\n30\n\n\n1850\n\n\n1850-04-30\n\n\n120\n\n\n\n\nMay\n\n\n1\n\n\n1851\n\n\n1851-05-01\n\n\n121\n\n\n\n\nMay\n\n\n12\n\n\n1852\n\n\n1852-05-12\n\n\n133\n\n\n\n\n\n\n\nTo reproduce Quetelet’s law of the flowering plants, we will combine these bloom dates with daily temperature. The daily maximum and minimum temperatures at Brussels Observatory between 1839 and 1852 are available from the Global Historical Climatology Network. The data can be downloaded using the ghcnd_search function contained within the R package rnoaa (2021). The station id for Brussels Observatory is “BE000006447”.\nThe ghcnd_search function returns the maximum and minimum temperature as separate tibbles in a list. In the R code below, we join the tibbles using the reduce function. Note that the temperature is reported in tenths of a degree (i.e. 0.1°C) so we divide by 10 before calculating the temperature midrange, our estimate of the daily temperature.\nThe result is a five-column tibble temp, which contains the year of the temperature record (“year”), the date of the temperature record (“date”), the maximum temperature (“tmax”), the minimum temperature (“tmin”), and the midrange temperature (“temp”). The first 10 rows of the table are below. When you produce the full table yourself, you may notice that a small portion of temperature records are missing. We found that imputing these missing values does not significantly change the results. Therefore, we ignore these days when conducting our analysis.\n```{r}\ntemp &lt;- \n  ghcnd_search(stationid = \"BE000006447\",\n               var = c(\"tmax\", \"tmin\"),\n               date_min = \"1839-01-01\",\n               date_max = \"1852-12-31\") %&gt;%\n  reduce(left_join) %&gt;%\n  transmute(year = parse_number(format(date, \"%Y\")), \n            date, \n            tmax = tmax / 10, \n            tmin = tmin / 10, \n            temp = (tmax + tmin) / 2)\n  \ntemp %&gt;% \n  kable(align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 2: Temperature observed at Brussels Observatory between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 2: Temperature observed at Brussels Observatory between 1839 and 1852.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n1839\n\n\n1839-01-01\n\n\n5.7\n\n\n-0.2\n\n\n2.75\n\n\n\n\n1839\n\n\n1839-01-02\n\n\n6.3\n\n\n0.8\n\n\n3.55\n\n\n\n\n1839\n\n\n1839-01-03\n\n\n7.2\n\n\n1.8\n\n\n4.50\n\n\n\n\n1839\n\n\n1839-01-04\n\n\n8.0\n\n\n1.8\n\n\n4.90\n\n\n\n\n1839\n\n\n1839-01-05\n\n\n5.3\n\n\n0.8\n\n\n3.05\n\n\n\n\n1839\n\n\n1839-01-06\n\n\n10.0\n\n\n1.3\n\n\n5.65\n\n\n\n\n1839\n\n\n1839-01-07\n\n\n8.9\n\n\n1.4\n\n\n5.15\n\n\n\n\n1839\n\n\n1839-01-08\n\n\n3.0\n\n\n0.1\n\n\n1.55\n\n\n\n\n1839\n\n\n1839-01-09\n\n\n0.8\n\n\n-0.1\n\n\n0.35\n\n\n\n\n1839\n\n\n1839-01-10\n\n\n2.8\n\n\n-2.8\n\n\n0.00\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nReproducing Quetelet’s law is now a simple matter of calculating the sum of the squared daily temperature from the day of last frost until the bloom day. We could use the day of last frost reported in Quetelet’s Letters. However, since we will replicate Quetelet’s analysis with recent data in a later section, we use our own definition of the day of last frost. We define the day of last frost to be the day following the last day the maximum temperature is below 0. The R code below creates the function doy_last_frost to extract the day of last frost from the maximum temperature. To demonstrate this function, we then compare the bloom date with the last frost date in 1839, the first year Quetelet observed.\n```{r}\ndoy_last_frost &lt;- function(tmax, doy_max = 100) {\n  dof &lt;- which(tmax[1:doy_max] &lt;= 0)\n  if(length(dof) == 0) 1 else max(dof) + 1\n  }\n\nbloom_day &lt;- \n  lilac %&gt;% \n  filter(year == 1839) %&gt;%\n  pull(doy) + \n  as.Date(\"1839-01-01\")\n  \nfrost_day &lt;- \n  temp %&gt;% \n  filter(year == 1839) %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"1839-01-01\") \n\ntibble(`last frost date` = frost_day, \n       `bloom date` = bloom_day) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n1839-03-08\n\n\n1839-05-11\n\n\n\n\n\n\n\nIf Quetelet’s law of the flowering plants is correct, Table 3 has the following interpretation. On March 8, 1839 the lilacs at Brussels Observatory began “collecting” temperature. The lilacs continued to “collect” temperature until May 11, at which point they exceeded their 4264°C² quota and bloomed. We visualize this theory in Figure 4 with the R packages ggplot2, a member of the set of packages that constitute the “tidyverse” (2019), and plotly.\n```{r}\n(temp %&gt;% \n  filter(date &lt; as.Date(\"1839-06-01\")) %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title = \n      \"Figure 4: According to Quetelet's law, the lilacs bloom when exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(bloom_day, frost_day)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day, bloom_day)),\n                  y = c(-4, -4),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-10, -12)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 4: According to Quetelet’s law, the lilacs bloom when exposed to 4264°C² following the last frost. Author provided, CC BY 4.0.\n\nWe now have all the ingredients necessary to reproduce Quetelet’s findings. Our reproduction is greatly simplified by using the nest function from the tidyr package, another member of the “tidyverse”. For an overview of nest, see the “Nested data” section of Grolemund and Wickham (2017). We will group the data by year, nest, calculate the cumulative squared temperature from the frost date to the bloom date within each year, and then unnest. We ignore temperatures below 0°C. That is, temperatures below 0°C are set to 0°C. We do this because it is clear from Quetelet’s derivation of the law that only positive temperatures should be squared. See the next section for details.\n```{r}\nquetelet &lt;- \n  temp %&gt;% \n  group_by(year) %&gt;% \n  nest() %&gt;% \n  left_join(lilac) %&gt;% \n  mutate(law = map(data, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax) + 1):doy]^2))) %&gt;% \n  unnest(law) %&gt;% \n  ungroup()\n\nquetelet %&gt;% \n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law)/sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\", \n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\", \n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 4: Reproduction of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 4: Reproduction of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4261\n\n\n197\n\n\n[3867, 4656]\n\n\n\n\n\n\n\nThe results show that Quetelet’s findings are indeed reproducible. Quetelet estimated that lilacs bloom once exposed to 4264°C² following the last frost. Our reanalysis suggests a similar amount. However, 4264°C² is the overall average across all years – the estimated amount needed to bloom varies year to year. As a result, the average has a 95% confidence interval of approximately 3870°C² to 4660°C². Quetelet was well aware of this variation. He argued it was due to unobserved factors that influence growing conditions and change each year, and he dedicated significant space in his Letters to discuss them.\nThese unobserved factors limit the accuracy of predictions made using the law. To assess the predictive accuracy of the law, we temporarily ignore the bloom dates Quetelet observed. Instead, we apply the 4264°C² quota to the temperature records at Brussels Observatory to predict the bloom date. We then compare our predictions with the bloom date Quetelet observed. The R code below creates the function doy_prediction to estimate the day the lilac will bloom from temperature records. Table 5 summarizes the accuracy of Quetelet’s law by the mean absolute error and root mean squared error.\n```{r}\ndoy_prediction &lt;- function(temp, tmax)\n  doy_last_frost(tmax) + which.max(cumsum(pmax(temp[(doy_last_frost(tmax) + 1):365], 0, na.rm = TRUE)^2) &gt; 4264)\n\nquetelet %&gt;% \n  mutate(pred = map(data, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup() %&gt;%\n  summarize(mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\", \"root mean squared error (days)\"),\n        caption = \"Table 5: Predictions using Quetelet's law are accurate within a week on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 5: Predictions using Quetelet’s law are accurate within a week on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n5\n\n\n6\n\n\n\n\n\n\n\nTable 5 indicates that predictions made using the law are accurate to within a week on average. For comparison purposes, we also predict the day the lilacs will bloom using the average bloom date between 1839 and 1852. That is, on average the lilac bloomed on April 30 (April 29 on leap years), and we check the accuracy of simply predicting this average date each year. Table 6 indicates the average bloom date yields predictions that are less accurate by an average of two days.\n```{r}\nquetelet %&gt;%\n  summarize(pred = mean(doy),\n            mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 6: Predictions using the average bloom date are off by a week or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 6: Predictions using the average bloom date are off by a week or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n7\n\n\n9"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s derivation of the law of the flowering plants",
    "text": "Quetelet’s derivation of the law of the flowering plants\nQuetelet believed that, as in physics, universal laws govern social and biological phenomenon. Quetelet was not only inspired by physics to describe social and biological patterns using mathematical formulas. He often took his formulas directly from physics. In fact, you may have already recognized similarities between his law and Newton’s second law of motion.\nQuetelet reasoned that temperature exerts a “force” on plants in the same way that gravity exerts a force on a falling object. Newton’s second law states that acceleration is proportional to force. It follows that an object initially at rest and subject to a constant force will travel a distance proportional to time squared. Quetelet simply substituted temperature for time.\nWe briefly elaborate. Let \\(d(t)\\) denote the distance an object travels after time \\(t\\). Let \\(v(t) = d'(t)\\) denote its speed and \\(a(t) = v'(t)\\) its acceleration. If acceleration is constant, i.e. \\(a(t) = c\\),\n\n\\(v(t) = \\int_0^t a(s) \\, ds = \\int_0^t c \\, ds = c t\\)\n\nand\n\n\\(d(t) = \\int_0^t v(s) \\, ds = \\int_0^t c s \\, ds = \\tfrac{c}{2} t^2\\)\n\nQuetelet imagined plants experience time in temperature and bloom after “traveling” distance \\(d_*\\). If a plant is exposed to temperature \\(t_i\\) on day \\(i = 1, 2, \\ldots\\), then the bloom date, \\(n_*\\), is the first day \\(\\sum_{i=1}^{n_*} \\tfrac{c}{2} t_i^2 \\geq d_*\\). Multiplying both sides of the inequality by \\(\\tfrac{2}{c}\\), yields Quetelet’s law: the bloom is the first day, \\(n_*\\), that \\(\\sum_{i=1}^{n_*} t_i^2 \\geq \\tfrac{2}{c} d_*\\).\nThe derivation of laws like the law of the flowering plants was popular in the nineteenth century. But any similarities between the “force” of temperature and the force of gravity are likely coincidental. We are not aware of any biological mechanisms that justify Quetelet’s application of Newton’s law.\nToday, the law of the flowering plants is considered a heuristic, or rule of thumb, that approximates complicated biological mechanisms. Like Quetelet, scientists model plants as experiencing time in temperature instead of calendar time. These temperature units are typically called “growing degree days”. Scientists often find that plants may only be sensitive to temperatures in specific ranges or “modified growing degree days”. Although modern statistical methods can greatly improve the accuracy of predictions, laws like Quetelet’s remain popular because they are simple to communicate and easy to replicate, as we demonstrate in the next section."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Replicating Quetelet’s law of the flowering plants",
    "text": "Replicating Quetelet’s law of the flowering plants\nIn the previous section, we explained how Quetelet derived the law of the flowering plants. Quetelet believed the law of the flowering plants was universal, describing the bloom date of all flowers around the world and in any year. Whether the law can in fact be considered universal requires replicating Quetelet’s results with new data collected at a different location in a different year.\nIn this section, we replicate the law of the flowering plants using lilac bloom dates observed by scientists between 1956 and 2009 at 53 locations throughout the Pacific Northwest (2015). The data can be downloaded from the USA National Phenology Network using the rnpn package (2022). For space considerations, the R code that downloads and cleans the data is provided in the Appendix. Running this code yields the tibble usa_npn. Each row of the tibble corresponds with a bloom date observed at a given site in a given year. There are 31 columns, only seven of which we use in our replication. The remaining columns are documented in the rnpn package, and we will not review them here.\nTable 7 displays six of the seven columns (and only the first 10 rows of the full table). These columns are defined in the same way as the columns of Table 1, except for “site_id”, which denotes the site at which the observation was made. Table 1 does not have a “site_id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nload(url(\"https://github.com/jauerbach/miscellaneous/blob/main/usa_npn.RData?raw=true\"))\n\nusa_npn %&gt;%\n  transmute(site_id, \n            month = first_yes_month, \n            day   = first_yes_day, \n            year  = first_yes_year, \n            date  = as.Date(paste(month, day, year), format = \"%m %d %Y\"),\n            doy) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\n\nTable 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\n\n\n\n\nsite_id\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\n150\n\n\n5\n\n\n25\n\n\n1956\n\n\n1956-05-25\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n22\n\n\n1957\n\n\n1957-05-22\n\n\n142\n\n\n\n\n150\n\n\n5\n\n\n12\n\n\n1958\n\n\n1958-05-12\n\n\n132\n\n\n\n\n150\n\n\n6\n\n\n3\n\n\n1959\n\n\n1959-06-03\n\n\n154\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1960\n\n\n1960-05-27\n\n\n148\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1961\n\n\n1961-05-27\n\n\n147\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1962\n\n\n1962-05-26\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n24\n\n\n1963\n\n\n1963-05-24\n\n\n144\n\n\n\n\n150\n\n\n5\n\n\n28\n\n\n1964\n\n\n1964-05-28\n\n\n149\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1966\n\n\n1966-05-26\n\n\n146\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nThe seventh column we review is “temp”. Each row of “temp” is a tibble of temperature records taken at the nearest station in the Global Historical Climatology Network. The first tibble (again, only the first 10 rows) is displayed in Table 8 below. The columns are defined in the same way as the columns of Table 2, except for “id”, which denotes the location at which the temperature record was made. Table 2 does not have an “id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nusa_npn %&gt;%\n  pull(temp) %&gt;%\n  .[[1]] %&gt;%\n  mutate(year = parse_number(format(date, \"%Y\"))) %&gt;%\n  select(id, year, date, tmax, tmin, temp) %&gt;%\n  kable(align = \"c\",\n        col.names = c(\"id\", \"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 8: Temperature observed at an example pacific northwest site in 1956.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\n\nTable 8: Temperature observed at an example pacific northwest site in 1956.\n\n\n\n\nid\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-01\n\n\n5.6\n\n\n-5.6\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-02\n\n\n1.7\n\n\n-7.2\n\n\n-2.75\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-03\n\n\n3.3\n\n\n-11.7\n\n\n-4.20\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-04\n\n\n4.4\n\n\n-10.0\n\n\n-2.80\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-05\n\n\n7.8\n\n\n0.0\n\n\n3.90\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-06\n\n\n4.4\n\n\n-11.1\n\n\n-3.35\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-07\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-08\n\n\n4.4\n\n\n-4.4\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-09\n\n\n1.7\n\n\n-9.4\n\n\n-3.85\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-10\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nWe are now prepared to replicate Quetelet’s findings. We will use R code nearly identical to the code we used to reproduce Quetelet’s findings earlier. The main difference is due to the fact that temperature records are dependent across sites within a year. To account for this dependence, we compute the cumulative temperature squared from the last frost to the bloom date for each site and year. We then take the average across all sites within a year. Finally, we calculate the standard error and confidence interval using only the variation of the averages across years. Table 9 displays the results.\n```{r}\nusa_npn %&gt;%             \n  group_by(rownames(usa_npn)) %&gt;%\n  mutate(law = \n           map(temp, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax, doy) + 1):(doy - 1)]^2))) %&gt;%\n  unnest(law) %&gt;% \n  group_by(year) %&gt;%    \n  summarize(law = mean(law)) %&gt;%\n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law) / sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\",\n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\",\n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 9: Replication of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 9: Replication of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4329\n\n\n116\n\n\n[4098, 4560]\n\n\n\n\n\n\n\nTable 9 indicates that Quetelet’s findings are replicable in the sense that the confidence interval calculated using Quetelet’s data (Table 4) overlaps with the confidence interval calculated using the USA lilac data (Table 9). The standard error in Table 9 is smaller than Table 4 because the replication uses 54 years of data compared to Quetelet’s 14. Note that in the R code above, we subtract 1 from “doy” to correct for differences in how the bloom date is reported. This correction is not particularly important; the confidence intervals still overlap when this correction is removed.\nWe now investigate the accuracy of Quetelet’s law when applied to the USA lilac data. As before, we make use of the doy_prediction function.\n```{r}\nusa_npn &lt;- \n  usa_npn %&gt;% \n  mutate(pred = map(temp, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup()\n\nusa_npn %&gt;% \n  summarize(mae  = mean(abs(doy - 1 - pred)),\n            rmse = sqrt(mean((doy - 1 - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\", \n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 10: Predictions using Quetelet's law are accurate within about two weeks on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 10: Predictions using Quetelet’s law are accurate within about two weeks on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n10\n\n\n15\n\n\n\n\n\n\n\nTable 10 indicates that the predictions are accurate to within two weeks on average. Recall that the predictions using Quetelet’s own data were accurate to within one week on average (Table 5). We speculate that the decrease in accuracy is due in part to the fact that both Quetelet’s lilacs and the temperature were observed at the same site, Brussels Observatory. In some cases, the USA lilacs were a few miles from where the temperature was recorded.\nAlthough the accuracy of the predictions made using Quetelet’s law is lower when applied to the USA lilac data, Figure 5 indicates that the law produces the correct bloom date on average. The figure plots the predictions made by the law against the actual bloom dates scientists observed. Note that instead of representing prediction-observation pairs as points in a scatter plot, the data are represented using blue contours. We use contours because there are more than 1,500 observations – too many to study using a scatter plot.\n```{r}\n(usa_npn %&gt;% \n   mutate(doy = first_yes_doy) %&gt;%\n   unnest(pred) %&gt;% \n   ungroup() %&gt;%\n   mutate(predicted = as.Date(\"2020-01-01\") + pred,\n          observed = as.Date(\"2020-01-01\") + doy) %&gt;%\n   ggplot() + \n    aes(x = observed, y = predicted) +\n    geom_density2d(contour_var = \"ndensity\") +\n    geom_abline(intercept = 0, slope = 1, linetype = 2) +\n    labs(x = \"date observed\", \n         y = \"date predicted\",\n         title = \"Figure 5: Predictions using Quetelet's law are accurate within about two weeks on average.\") +\n    theme(legend.position = \"none\")) %&gt;%\n  ggplotly(tooltip = \"\") %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 5: Predictions using Quetelet’s law are accurate within about two weeks on average. Author provided, CC BY 4.0.\n\nThe contours are easy to interpret. The blue lines are much like a mountain range observed from above. The inner circles are peaks of high elevation in which many prediction-observation pairs co-occur. The outer circles are areas of low elevation in which few prediction-observation pairs co-occur.\nThe dotted line is the “y = x” line, having zero intercept and unit slope. Prediction-observation pairs that lie on the line indicate perfect predictions. The fact that the dotted line intersects the blue contours at their peak suggests the law derived from Quetelet’s data accurately predicts the typical bloom date of the USA data. This accuracy is impressive given the fact that the USA lilacs were observed more than a century later and on a different continent. The blue curves deviate from the line by about two weeks in the vertical direction, which is consistent with Table 10.\nAn average accuracy of two weeks might not sound impressive. But it is far more accurate than using the average bloom date Quetelet observed, April 30 (April 29 on leap years). The average bloom date yields predictions that are off by an additional eleven days on average.\n```{r}\nusa_npn %&gt;%\n  mutate(doy = first_yes_doy) %&gt;%\n  ungroup() %&gt;%\n  summarize(\n    pred = mean(quetelet$doy), \n    mae  = mean(abs(doy - pred)),\n    rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(\n    dig = 0,\n    align = \"c\",\n    col.names = c(\"mean absolute error (days)\",\n                  \"root mean squared error (days)\"),\n    caption = \"Table 11: Predictions using the average bloom date are off by three weeks or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 11: Predictions using the average bloom date are off by three weeks or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n21\n\n\n24"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Predicting the day the lilac will bloom in Brussels in 2023",
    "text": "Predicting the day the lilac will bloom in Brussels in 2023\nAny weather forecast can become a flower forecast by applying the law of the flowering plants. In this section, we use the AccuWeather forecast to predict the day a hypothetical lilac will bloom in Brussels in 2023. AccuWeather forecasts daily maximum and minimum temperatures three months into the future. We do not evaluate the quality of these forecasts. The purpose of this section is to simply convert them into flower forecasts.\nWe use the AccuWeather forecast as it appeared on the webpage AccuWeather.com on February 19, 2023. AccuWeather reports the forecast for each month on a separate webpage. For reproducibility, we saved each page on the Internet Archive. The following R code creates the function get_weather_table to retrieve each page we saved, extract the forecast contained within that page, and arrange the data as a tibble. The get_weather_table function combines several functions from the rvest package, which is yet another member of the “tidyverse”. In particular, the forecast on each page is contained within the div “monthly-calendar” and can be extracted with the html_nodes and html_text2 functions.\nApplying the get_weather_table function to the url for each page yields a five column tibble temp_br, with columns defined in the same way as the tibble temp, discussed in previous sections. The first 10 rows are below; the data are also available on the author’s GitHub.\n```{r}\n get_weather_table &lt;- function(url)\n  read_html(url) %&gt;% \n  html_nodes(\"div.monthly-calendar\") %&gt;% \n  html_text2() %&gt;%\n  str_remove_all(\"°|Hist. Avg. \") %&gt;%\n  str_split(\" \", simplify = TRUE) %&gt;%\n  parse_number() %&gt;%\n  matrix(ncol = 3, \n         byrow = TRUE,\n         dimnames = list(NULL, c(\"day\", \"tmax\", \"tmin\"))) %&gt;%\n  as_tibble() %&gt;%\n  filter(\n    row_number() %in%\n      (which(diff(day) &lt; 0) %&gt;% (function(x) if(length(x) == 1) seq(1, x[1], 1) else seq(x[1] + 1, x[2], 1))))\n\ntemp_br &lt;-\n  tibble(\n    base_url = \"https://web.archive.org/web/20230219151906/https://www.accuweather.com/en/be/brussels/27581/\",\n    month = month.name[1:5],\n    year = 2023,\n    url = str_c(base_url, tolower(month), \"-weather/27581?year=\", year, \"&unit=c\")) %&gt;%\n  mutate(temp = map(url, get_weather_table)) %&gt;%\n  pull(temp) %&gt;%\n  reduce(bind_rows) %&gt;%\n  transmute(date = seq(as.Date(\"2023-01-01\"), as.Date(\"2023-05-31\"), 1),\n            year = parse_number(format(date, \"%Y\")),\n            tmax,\n            tmin,\n            temp = (tmax + tmin) / 2)\n\ntemp_br %&gt;%\n  relocate(year) %&gt;%\n  kable(dig = 2,\n        align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\",\n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\n\nTable 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n2023\n\n\n2023-01-01\n\n\n15\n\n\n11\n\n\n13.0\n\n\n\n\n2023\n\n\n2023-01-02\n\n\n14\n\n\n5\n\n\n9.5\n\n\n\n\n2023\n\n\n2023-01-03\n\n\n9\n\n\n3\n\n\n6.0\n\n\n\n\n2023\n\n\n2023-01-04\n\n\n13\n\n\n8\n\n\n10.5\n\n\n\n\n2023\n\n\n2023-01-05\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-06\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-07\n\n\n11\n\n\n9\n\n\n10.0\n\n\n\n\n2023\n\n\n2023-01-08\n\n\n10\n\n\n6\n\n\n8.0\n\n\n\n\n2023\n\n\n2023-01-09\n\n\n8\n\n\n5\n\n\n6.5\n\n\n\n\n2023\n\n\n2023-01-10\n\n\n12\n\n\n4\n\n\n8.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\n\nWe now predict the day the lilacs will bloom. The R code below uses the doy_prediction and doy_last_frost functions created in earlier sections and displays the prediction in Table 13. At the time of our writing, the predicted date is April 19. The forecast is easily updated by providing the url to the updated AccuWeather webpage. (You might use the url https://web.archive.org/save to save a webpage to the Internet Archive to ensure your work is reproducible.)\n```{r}\nbloom_day_br &lt;-\n  temp_br %&gt;%\n  summarize(date = doy_prediction(temp, tmax) + as.Date(\"2023-01-01\")) %&gt;%\n  pull(date)\n\nfrost_day_br &lt;- \n  temp_br %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"2023-01-01\") \n\ntibble(`last frost date` = frost_day_br, \n       `bloom date` = bloom_day_br) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 13: Last frost date and lilac bloom date in Brussels in 2023.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\n\nTable 13: Last frost date and lilac bloom date in Brussels in 2023.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n2023-01-27\n\n\n2023-04-19\n\n\n\n\n\n\n\nWe visualize the predictions in Figure 6, which has the same interpretation as Figure 4. If the temperature forecast and Quetelet’s law are correct, on January 27, 2023 the lilacs in Brussels began “collecting” temperature. The lilacs will continue to “collect” temperature until April 19, at which point they will exceed their 4264°C² quota and bloom.\n```{r}\n(temp_br %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title =\n      \"Figure 6: According to Quetelet's law, the lilacs will bloom once exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(frost_day_br, bloom_day_br)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day_br, bloom_day_br)),\n                  y = c(14, 14),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-14, -16)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\n\n\n\n\nFigure 6: According to Quetelet’s law, the lilacs will bloom once exposed to 4264°C² following the last frost. Author provided, CC BY 4.0."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist",
    "text": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist\nIn this tutorial, we stated the law of the flowering plants and explained how Quetelet derived it. We also reproduced and replicated Quetelet’s findings before using his law to predict the day the lilac will bloom in Brussels. We now conclude with a reflection on Quetelet’s legacy.\nThe law of the flowering plants surely stands the test of time. It continues to be used by scientists – with relatively few changes – to plan harvests, manage pests, and study ecosystems stressed by climate change. We speculate the law’s longevity is due to the fact that it balances simplicity with relatively accurate predictions.\nAlthough Quetelet did not discover the law, he did much to advance it. Quetelet founded an international network for “observations of the periodical phenomena” (in addition to numerous statistical societies and publications, including the precursor to the Royal Statistical Society). Quetelet’s network of 80 stations collected observations throughout Europe from 1841 until 1872. In particular, Quetelet collaborated with Charles Morren – who later coined the term phenology, the name of the field that now studies biological life-cycle events like the timing of flower blooms (Demarée and Rutishauser 2011).\nIn recent years, the observations collected through phenology networks have become an important resource for understanding the impacts of climate change. For example, the USA National Phenology Network calculates the Spring Bloom Index, which measures the “first day of spring” using the days lilacs are observed to bloom at locations across the United States. The index is then compared to previous years. Figure 7 shows one comparison, called the Return Interval. The Return Interval is much like a p-value, calculating how frequently more extreme spring indices were observed in previous decades. Bloom dates that are uncommonly early (green) or late (purple) may indicate environments stressed by changing climate. Scientists exploit the relationship between temperature and bloom date to extrapolate the index to areas with few observations.\n\n\n\n\n\n\nFigure 7: The Spring Bloom Index Return Interval measures whether spring is typical when compared to recent decades. Source: USA National Phenology Network.\n\nQuetelet’s emphasis on discovering the universal laws he believed govern social and biological phenomenon has not endured. But data scientists continue to appropriate laws from one area of science to study another. For example, data scientists use neural networks and genetic algorithms to study a wide variety of phenomenon unrelated to neuroscience or genetics. Perhaps Quetelet’s appropriation of Newton’s law, in addition to his careful use of data, make him among the first data scientists?"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Your turn: Do you have what it takes to beat Quetelet’s law?",
    "text": "Your turn: Do you have what it takes to beat Quetelet’s law?\nQuetelet reported that a plant flowers when the sum of the daily temperatures squared exceeds a specific quantity. His prediction rule was state of the art in 1833. But surely you, a twenty-first century data scientist, can do better. Here are some ideas to get you started.\n\nQuetelet squared the temperature before calculating the sum. Would another function of temperature produce a more accurate prediction?\n\nRemove the square so that a plant flowers once the sum of the daily temperatures exceeds a (different) specific quantity. Does this version of the law produce more accurate predictions? What if you use the daily temperatures cubed? (Beginner)\nSuppose a lilac only registers temperatures between 0°C and 10°C. That is, a lilac experiences temperature below the lower limit, 0°C, as 0°C, and above the upper limit, 10°C, as 10°C. Does the accuracy of the predictions improve if you use the temperature the lilac experienced instead of the ambient temperature measured by a weather station? Write a program that finds the lower and upper limits that produce the most accurate predictions. (Intermediate)\nQuetelet used mean absolute error to evaluate the accuracy of his predictions. But his estimate of the specific quantity of heat needed to bloom, 4264°C², does not actually minimize mean absolute error. Write a program that finds the specific quantity that minimizes mean absolute error. Redo part i. and ii. using this function. (Advanced)\n\nQuetelet calculated the sum of the daily temperature squared between the day of last frost and the bloom date. Would another time interval produce more accurate predictions?\n\nWe estimated the day of last frost using the last day the maximum temperature was below 0°C. Try estimating the day of last frost by the last day the midrange temperature was below 0°C? Which estimate yields the most accurate predictions? What if you ignore the day of last frost and simply calculate the sum of the daily temperatures squared between February 1 and the bloom date? When you change the time interval, be sure to calculate the new specific quantity of heat needed to bloom. (Beginner)\nWrite a program that finds the time interval which yields the best predictions. (Intermediate)\nWrite a program that calculates the prediction rule for many different time intervals. Use cross-validation to combine these prediction rules into a single prediction rule. (Advanced)\n\nQuetelet’s law only considers the temperature. Would additional information provide more accurate predictions?\n\nIs the specific quantity of heat needed to bloom different in years with abnormally cold winters? Would the predictions be more accurate if you use one quantity of heat for years with cold winters and a different quantity of heat for years with warm winters? (Beginner)\nIs the estimated quantity of heat needed to bloom similar for locations close in space and time? Write a program that leverages spatial and temporal correlation to improve the accuracy of the predictions. (Intermediate)\nSome biologists report that a plant must be exposed to a fixed amount of cold temperature in the winter – in addition to a fixed amount of warm temperature in the spring – before it can bloom. Augment the law of the flowering plants to require the accumulation of a specific quantity of cold temperature before the accumulation of a specific quantity of warm temperature. Write a program that uses this new law to predict the day the lilac blooms. (Advanced)\n\n\nFeeling good about your prediction algorithm? Show it off at the annual Cherry Blossom Prediction Competition!"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Appendix: Preparing USA NPN Data",
    "text": "Appendix: Preparing USA NPN Data\n```{r}\n# 1. download lilac data using `rnpn`\nusa_npn &lt;- \n  npn_download_individual_phenometrics(request_source = \"Jonathan Auerbach\",\n                                       year = 1900:2050,\n                                       species_ids = 36,                       \n                                       phenophase_ids = c(77, 412))            \n\n# 2. limit analysis to sites that report more than 25 times\nsite_ids &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(n = n()) %&gt;% filter(n &gt; 25) %&gt;% pull(site_id)\n\nusa_npn &lt;- \n  usa_npn %&gt;% \n  filter(site_id %in% site_ids)\n\n# 3. find nearest weather stations for each site\nlocations &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(latitude = first(latitude), \n            longitude = first(longitude))\n\nstations &lt;- \n  ghcnd_stations() %&gt;%\n  filter(first_year &lt;= min(usa_npn$first_yes_year),\n         last_year  &gt;= max(usa_npn$first_yes_year),\n         state != \"\") %&gt;%\n  group_by(id, latitude, longitude, state) %&gt;%\n  summarize(temp_flag = sum(element %in% c(\"TMIN\", \"TMAX\"))) %&gt;%            \n  filter(temp_flag == 2) %&gt;% \n  ungroup()\n\ndist &lt;- function(x, y = stations %&gt;% select(latitude, longitude)) \n  stations$id[which.min(sqrt((x[1] - y[,1])^2 + (x[2] - y[,2])^2)[,1])]\n\nlocations$station_id &lt;- apply(locations, 1, function(x) dist(c(x[\"latitude\"], x[\"longitude\"])))\n\n# 4. get weather data from nearest station using `rnoaa`\nget_station_data &lt;- function(station_id) \n  ghcnd_search(stationid = station_id,\n               var = c(\"tmin\", \"tmax\"),\n               date_min = \"1956-01-01\",\n               date_max = \"2011-12-31\") %&gt;%\n  reduce(left_join, by = c(\"id\", \"date\")) %&gt;%\n  transmute(id, \n            date, \n            tmax = tmax / 10,\n            tmin = tmin / 10)\n\nusa_npn &lt;- \n  locations %&gt;%\n  mutate(temp = map(station_id, get_station_data)) %&gt;%\n  right_join(usa_npn, by = c(\"site_id\", \"latitude\", \"longitude\")) %&gt;% \n  group_by(rownames(usa_npn)) %&gt;% \n  mutate(temp = map(temp, ~ .x %&gt;% \n                      filter(format(date, \"%Y\") == first_yes_year) %&gt;%\n                      mutate(temp = (tmin + tmax) / 2)),\n         num_obs = map(temp,~ sum(format(.x$date,\"%j\") &lt;= 150)),\n         doy = first_yes_doy, year = first_yes_year) %&gt;% \n  unnest(num_obs) %&gt;%  \n  filter(num_obs == 150) %&gt;%\n  ungroup()\n```\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nJonathan Auerbach is an assistant professor in the Department of Statistics at George Mason University. His research covers a wide range of topics at the intersection of statistics and public policy. His interests include the analysis of longitudinal data, particularly for data science and causal inference, as well as urban analytics, open data, and the collection, evaluation, and communication of official statistics. He co-organizes the annual Cherry Blossom Prediction Competition with David Kepplinger and Elizabeth Wolkovich.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Jonathan Auerbach\n\n\n  Text and code are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence, except where otherwise noted.\n\n\n\nHow to cite\n\nAuerbach, Jonathan. 2023. “A demonstration of the law of the flowering plants.” Real World Data Science, April 13, 2023. URL"
  },
  {
    "objectID": "ideas/posts/2023/07/03/trusted-AI.html",
    "href": "ideas/posts/2023/07/03/trusted-AI.html",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "",
    "text": "With artificial intelligence (AI) becoming increasingly prevalent across sectors, so too have conversations about AI ethics. AI ethics provides a repeatable and comprehensive way to assess what we should and should not be doing with AI, and sets out how we ought to design, use, and govern AI products in accordance with key principles. Ethical frameworks are essential to derive sustainable value from AI products and services and build trust.\nA myriad of AI tools that leverage automated or semi-automated decision-making processes have raised important questions that have become foundational in the AI ethics community, such as ‘What does it mean for an algorithm to be fair?’ As an example, AI tools that are used in recruitment may perpetuate biases arising from historical training data. If a model used to generate a shortlist of applicants has been trained on data from past candidates, say, and those candidates – both successful and unsuccessful – are predominantly men, historical patterns that contain various biases will perpetuate to become algorithmic biases that form the model’s decisions. Thus, the model may algorithmically discriminate against women or gender minorities, as individuals from these groups are not well represented in the training data.\nTo ensure the safe and responsible use of AI, the focus moving forward needs to be on the operationalisation of AI ethics into the day-to-day development lifecycle. But, what does this look like in practice? And how might you get started as an ethical AI practitioner? In this article, we unpack these questions and give you, the data scientist, a foundation to begin your journey towards trusted AI. Read along to get an overview of key principles that you should be aware of, what they mean, their underlying technical grounding, and what implementation might look like practically."
  },
  {
    "objectID": "ideas/posts/2023/07/03/trusted-AI.html#ethical-ai-principles",
    "href": "ideas/posts/2023/07/03/trusted-AI.html#ethical-ai-principles",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "Ethical AI principles",
    "text": "Ethical AI principles\nYou have likely heard of several principles in relation to ethical AI, such as fairness or transparency. The context in which you’ve encountered such principles is most probably due to their inclusion in a broader ethical framework. Some of the most popular ethical AI frameworks include the National Institute of Standards and Technology’s AI Risk Management Framework, the UK Data Ethics Framework, and the European Commission’s Ethics Guidelines for Trustworthy AI. Among these and many other frameworks, we can run into what Floridi and Cowls (2019) call “principle proliferation,” whereby it becomes overwhelming for those contributing to AI programmes to know where to begin with ethics due to an excess of choice (p. 2).\nAt the time of writing, there is no single universally accepted standard that dictates which essential ethical AI values or principles should be adhered to during AI development and deployment. However, there are common themes that emerge. In our organisation, EY, we’ve learned from the variety of principles, frameworks, and white papers in the AI ethics community and developed our own Trusted AI Framework comprising five key attributes that we believe assure the trustworthiness of AI:\n\nTransparent\nExplainable\nUnbiased\nResilient\nHigh-performing\n\nIn this article, we take a deeper dive into the first three attributes – transparency, explainability, and unbiasedness (or fairness). These are areas where data scientists can act as critical enablers of ethical AI when they have the right knowledge and toolkits at their disposal.\n\nTransparency\nTransparency is the ability to provide meaningful information to foster awareness and understanding of an AI system. It starts with documenting AI systems in a way that is accessible for a broad audience with a spectrum of technical abilities. It is a simple yet powerful way to build trust in AI. It empowers non-technical stakeholders to critically evaluate AI development decisions, thereby unlocking multi-disciplinary insights that can mitigate reputational or performance risks. Further, it also builds trust with society, as it can enable everyday users to interrogate AI design decisions, product capabilities, and system limitations, thereby permitting users to make informed judgements about technology. Unfortunately, transparency is often misunderstood as disclosing trade secrets or proprietary information, such as source code and datasets. However, transparency can be achieved without disclosing such technically complex information. Instead, it can be as simple as disclosing where and when an AI system is being used, or for what purposes a model should be employed.\nBut what exactly does “documenting AI systems” look like? Documentation should consist of a mix of technical components (system architecture, dataset selection determination, model selection techniques, etc.) and non-technical components (business case, product purpose of use, alignment to overall AI strategy, etc.). The research community has recommended AI documentation standards, such as datasheets for datasets and model cards for model reporting. You can liken datasheets or model cards to the importance placed upon commenting your code – the more information there is available around decisions throughout model development, the greater the certainty that these artefacts will be understood and used as intended moving forward. Proper documentation and governance will help ensure accountability, improve internal and external oversight, and initiate discussions around model optimisation goals and their trade-offs, such as including fairness and accuracy in optimisation objectives.\nWith upcoming AI regulations, transparency requirements will become more integral. For example, the European Union (EU) AI Act introduces specific transparency obligations, such as bot disclosures, for both users and providers of AI systems, which would allow users to opt out of interacting with an AI system. Furthermore, in higher risk use cases, specific technical documentation is needed, which would include details of a system’s intended purpose and descriptions of its development process.\n\n\nExplainability\nOnce transparency is enabled, explainability is a natural next step, especially when an AI product is implemented in a more regulated or high-risk environment. Explainability is the ability to express why an AI system reached a particular decision or understand the features that affect model behaviour. Explainability is a key concern within the field of explainable AI, which, as a discipline, strives to improve trustworthiness by enabling a better understanding of a system’s underlying logic via a suite of technical methods.\nFundamentally, different model architectures mean that some models are more interpretable than others, as the steps used to evaluate their predictions are easier for humans to comprehend. Decision trees, for example, have more human-interpretable characteristics than deep learning models. Different model architectures also mean that there are interpretation tools that are only applicable to certain models, such as regression weights in a linear model.\nAnother approach to consider, then, is model-agnostic interpretation, which encompasses both global interpretability (explanation of an entire model’s behaviour) and local interpretability (explanation of a single prediction). While there are fast-developing techniques and tools for model-agnostic interpretability, let’s take a look at two of the more popular methods available:\n\n\nLocal interpretable model-agnostic explanations (LIME)\n\nThis is an explanation technique that trains local surrogate models, using explainable models such as Lasso or decision trees, to approximate the predictions of a model that is not interpretable by design in order to explain individual model predictions. The idea is to use interpretable features from the surrogate models to create human-friendly explanations where the underlying model cannot. For example, in an image classification model that detects a flower in an image, LIME is able to highlight the parts of the image that explain why the model classifies the image as a flower (see illustration below). This provides an interpretable explanation between the input variable and prediction, which is an essential part of interpretability.\n\n\n\n\n\n\nIllustration of explainable AI processes using LIME on an image classification AI system. Adapted from “Local Interpretable Model-Agnostic Explanations (LIME): An Introduction” and O’Reilly.\n\n\n\n\nShapley Additive Explanations (SHAP)\n\nSHAP (GitHub repo) uses tools and theoretical foundations from game theory, one of which is Shapley values. It works by assigning each feature an importance value for a particular prediction to numerically explain the contribution of various features to a model’s output. For example, in a model that predicts flu, SHAP calculates the importance of sneezing as a feature by removing and adding the subset of other features, leading to different combinations of features that contribute to the prediction. This method provides interpretable solutions for more complex models similar to the equivalent of “weights” in linear models.\n\n\n\n\n\nFairness\nThe area of AI ethics that is central to impending AI regulations, such as the EU AI Act and the New York City AI Law,1 is fairness. AI models are inherently biased because of their underlying training data.2 Thus, when we speak of fairness in the context of AI ethics, we are referring to a combination of technical and non-technical ways to minimise the impacts of algorithmic bias.\nLet’s begin with the technical approaches to fairness. To achieve equitable, reliable, and fair decisions, a diverse and balanced set of examples is needed in training datasets. However, data often contains disparities that, if left unchecked, can perpetuate algorithmic biases and harms. There are various approaches to detect sources of bias, guarantee fairness, or “debias” models. To strive for algorithmic fairness, many papers have proposed various quantitative measures of fairness, with some based on unstated assumptions about fairness in society. Unfortunately, these assumptions are often mutually incompatible, making it difficult to compare fairness metrics to one another – consider, for example, the longstanding debate between equality of outcome and equality of treatment.\nAlthough metrics incompatibilities exist, fairness broadly focuses on equality of opportunity (group fairness), and equality of outcome (individual fairness) to prevent discrimination against certain attributes. Drawing definitions from legal frameworks, the term “protected attribute” refers to the characteristics that are often protected under anti-discrimination laws, such as gender or race. Mathematically, the following metrics are often used to demonstrate scores that support fairness:\n\n\nStatistical parity\n\nThis measure seeks to uncover whether a model is fair towards protected attributes by measuring the difference between the majority and protected class in receiving a favourable outcome. A value of 0 demonstrates the model to be fair.\n\n\n\nDisparate impact\n\nThis compares the percentage of favourable outcomes for the monitored group to the percentage of favourable outcomes for a reference group. The groups compared can be the majority group and minority group, and this score will highlight in whose direction decisions are biased. For example, if a model grants loans to 60% of people in a middle-aged group and only 50% for those of other age cohorts, then the disparate impact is 0.8, which indicates a positive bias towards the middle-aged group and an adverse impact on the remaining cohorts.\n\n\n\nEquality of odds\n\nThis measures the balance of the true positive rate and false positive rate between protected and unprotected groups, which seeks to uncover whether a model performs similarly for the two groups.\n\n\n\nIt is important to remember that statistics are only one side of the fairness problem for machine learning, and one that treats the symptoms of bias as opposed to the underlying causes. In addition to the aforementioned technical approaches, there are a variety of non-technical measures that teams developing AI systems can adopt to augment fairness and inclusion:\n\n\nDefinition of fairness\n\nOrganisations that develop or use AI systems need to define, practically, what it means to be fair. Although there are various quantitative fairness measures, these are based on assumptions of fairness in society, which could be defined for each specific use case.\n\n\n\nDiversity on teams\n\nThere’s been a sharpened focus on the value of team diversity to areas such as productivity and creativity. The same is true for ethics. Ensuring that product teams are composed of a broad cross-section of identities can help to organically drive fairness through diversity of thought and experience.\n\n\n\nEducation and self-reflection\n\nDeveloping knowledge within individuals and teams about the socio-technical aspects of AI – that is, the ways in which AI shapes our social, political, economic, and environmental lives. The more critical a person can be as a data scientist in questioning why something is being built, the more likely they are to proactively recognise risks surrounding fairness.\n\n\n\nConsider the end user\n\nImagine that you are on a development team building an AI solution for a problem in the agricultural sector pertaining to livestock health. Who is best suited to solving the problem: a data scientist or a farmer? As a data scientist, you may have the tools to develop a solution, but given your distance from the end user, you are unlikely to intimately understand the problem in the same way a farmer would. If you cannot understand the problem, you cannot hope to find a solution, much less an ethical one. Recognising the importance of consulting individuals that are representative of end users is key to ensure that your design is fair.\n\n\n\nAI ethics review boards\n\nData science teams should not operate in isolation. Increasingly, organisations are establishing AI ethics review boards or similar forums that are intended to act as checks on the design decisions made throughout AI development. Does your organisation have one?"
  },
  {
    "objectID": "ideas/posts/2023/07/03/trusted-AI.html#in-conclusion",
    "href": "ideas/posts/2023/07/03/trusted-AI.html#in-conclusion",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "In conclusion",
    "text": "In conclusion\nThese three areas – transparency, explainability, and fairness – are the starting points to embed and operationalise AI ethics in technical development. Transparency relies on both technical and non-technical documentation to facilitate discussions with non-technical stakeholders, as well as to create and enforce accountabilities. Explainability helps to build trust in AI output by vesting us with an ability to explain “why”. Finally, adopting both technical and non-technical measures of fairness can ensure that AI products in development do not adversely impact certain groups.\nIn addition to these three areas of AI ethics, within EY we have two other focus areas – resilience and high-performance – that form part of our Trusted AI Framework. We will discuss these in a future article. We’re also keen to explore topics such as generating trust in generative AI! Until then, please share your stories of developing ethical AI projects in the comments below. How are you translating AI ethics from theory into practice?\n\n\n\n\n\n\nFurther reading\n\n\n\n\n\nFor further technical reading, we suggest:\n\nInterpretable Machine Learning, by Christoph Molnar\nFairness and Machine Learning, by Solon Barocas, Moritz Hardt, and Arvind Narayanan\n\nFor further socio-technical reading on AI and data ethics, we suggest:\n\nThe Age of Surveillance Capitalism, by Shoshana Zuboff\nInvisible Women, by Caroline Criado Pérez\nRace after Technology, by Ruha Benjamin\nAlgorithms of Oppression, by Safiya Noble\nAtlas of AI, by Kate Crawford\nWeapons of Math Destruction, by Cathy O’Neil\nData Feminism, by Catherine D’Ignazio and Lauren Klein\n\n\n\n\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nMaxine Setiawan is a social data scientist specialising in trusted AI, and AI and data risk in EY UK&I. With her multi-disciplinary background, she works to help clients understand and manage risks from their data and AI systems, and to ensure AI governance that is fair, accountable, and trustworthy. Maxine holds an MSc in social data science from the University of Oxford.\n\n\nMira Pijselman is the digital ethics lead for EY UK&I, where she focuses on the responsible governance of key emerging technologies, including artificial intelligence, quantum technologies, and the metaverse. A social scientist and philosopher by training, she helps clients to map, understand, secure, and capitalise on their data and technology potential safely. Mira holds an MSc in the social science of the internet from the University of Oxford.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Maxine Setiawan and Mira Pijselman\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\nThumbnail image by Alexa Steinbrück / Better Images of AI / Explainable AI / Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nSetiawan, Maxine and Mira Pijselman. 2023. “Trusted AI: translating AI ethics from theory into practice.” Real World Data Science, July 3, 2023. URL"
  },
  {
    "objectID": "ideas/posts/2023/07/03/trusted-AI.html#footnotes",
    "href": "ideas/posts/2023/07/03/trusted-AI.html#footnotes",
    "title": "Trusted AI: translating AI ethics from theory into practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNew York City Local Law 144.↩︎\nNot statistical bias (usually known as bias-variance trade-off), which compares the training data and target value to approximate errors.↩︎"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2022/12/13/ridesharing.html",
    "href": "ideas/datasciencebites/posts/2022/12/13/ridesharing.html",
    "title": "Determining the best way to route drivers for ridesharing via reinforcement learning",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Dynamic causal effects evaluation in A/B testing with a reinforcement learning framework\nAuthor(s) and year: Chengchun Shi, Xiaoyu Wang, Shikai Luo, Hongtu Zhu, Jieping Ye, Rui Song (2022)\nStatus: Published in Journal of the American Statistical Association, Theory and Methods, open access: HTML, PDF, EPUB.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. See here for more information.\n\n\n\nCompanies often want to test the impact of one design decision over another, for example Google might want to compare the current ranking of search results (version A) with an alternative ranking (version B) and evaluate how the modification would affect users’ decisions and click behavior. An experiment to determine this impact on users is known as an A/B test, and many methods have been designed to measure the “treatment” effect of the proposed change. However, these classical methods typically assume that changing one person’s treatment will not affect others (known as the Stable Unit Treatment Value Assumption or SUTVA). In the Google example, this is typically a valid assumption — showing one user different search results shouldn’t impact another user’s click behavior. But in some situations, SUTVA is violated, and new methods must be introduced to properly measure the effect of design changes.\nOne such situation is that of ridesharing companies (Uber, Lyft, etc.) and how they determine which drivers are sent to which riders (the dispatch problem). Simply put, when a driver is assigned to a rider, this decision impacts the spatial distribution of drivers in the future. Hence the dispatch strategy (our treatment) at the present time will influence riders and drivers in the future, which violates SUTVA and hence invalidates many traditional methods for A/B testing. To tackle this problem, a group of researchers have recently employed a reinforcement learning (RL) framework which can accurately measure the treatment effect in such a scenario. Furthermore, their proposed approach allows for companies to terminate A/B tests earlier if the proposed version B is found to be clearly better. This early stopping can save time and money.\nTo better understand RL and how it can contribute to tackling the issue at hand, it’s first helpful to set some context. In typical RL problems, including the one in this paper, the scenario is modeled with something known as a Markov Decision Process (MDP). A MDP links three sets of variables across time: the state or environment, the treatment or action (the reaction to the environment), and the outcome (the response produced by the environment due to the action). These outcomes can be thought of as rewards which depend on the action taken and the state observed. Over time, the machine learns which actions produce more positive rewards and which bring about worse outcomes. Hence, the actions leading to higher rewards are positively reinforced, thus the name reinforcement learning. A causal diagram of an MDP is shown in Figure 1, where St, At, and Yt are the state, action, and outcome at time t. As one can see, past treatments influence future outcomes by altering the state variables at the present (the so-called “carryover effect” which violates SUTVA).\n\n\n\n\n\nFigure 1: Causal diagram of MDP, where the solid lines represent causal relationships.\nMaking this more concrete, consider an example where the decision-maker is a ridesharing company. The environment or state is whatever variables the decision-maker can measure about the world, like the spatial distribution of drivers, number of pickup requests, traffic, and weather. The company then makes some action on how to dispatch drivers. The combination of the state and action leads to an outcome that can be measured, for example passenger waiting time or driver’s income. The strategy which is used to determine an action is known as a policy. This policy could be designed to take the state into account or simply be fixed regardless of what environment is encountered. Much of the reinforcement learning literature focuses on the former (policies that depend on the state), but the authors argue that fixed designs are the de facto approach in industry A/B testing and hence they focus on that setting. In particular, a common treatment allocation design is the switchback design, where there are two policies of interest (the current dispatching strategy vs a proposed strategy) determined ahead of time and they are employed in alternating time intervals during the A/B test.\nSo how are policies compared to determine the treatment effect? The answer lies in what is known as the value function, which measures the total outcome that would have amassed had the decision-maker followed a given policy. The value function can put more value on short-term gain in outcome or long-term benefits. The two policies in an A/B test each have their own value functions, and a proposed policy is determined to be better if its value is (statistically) significantly higher. In the ridesharing setting, one possible outcome of interest is driver income. An A/B test in that scenario would thus look to see if a proposed policy had greater expected driver income vs the current policy.\nA natural question is when to end the experiment and test for a difference in value. In practice, companies will often simply run the test for a prespecified amount of time, such as two weeks, and then perform an analysis. But if one policy is clearly better than another, that difference could be detectable much earlier and the company is wasting valuable resources by continuing the experiment. To address this issue, the authors take an idea from clinical trials, the “alpha-spending” approach, and adapt it to their framework. Alpha-spending is one way to distribute over time the prespecified “budget” of Type 1 error (the probability of falsely detecting that a new policy is better). In the article’s real-data example, the authors test once a day for each day after one week and are able to detect a significant difference on Day 12. Waiting until Day 14 would have resulted in poorer outcomes since a suboptimal policy would be implemented half the time for two more days.\nOverall, the framework introduced allows for handling of carryover effects, is capable of modeling treatment allocation like the switchback design, and furthermore, allows for possible early stopping. With these three features, the authors argue their approach is highly applicable to the current practice of ridesharing companies (and possibly other industries as well). For interested readers wanting to dive deeper into the methodology presented, you can check out the full article, listen to the first author discuss the material at the Online Causal Inference Seminar (embedded below), or explore the Python implementation available on GitHub.\n\n\n\n\n\nAbout the author\n\nBrian King is a PhD candidate in the Department of Statistics at Rice University and a current NSF Graduate Research Fellow, with research focused on Bayesian modeling and forecasting for time series of counts. Prior to Rice, he graduated from Baylor University with a B.S. in Mathematics and Statistics alongside a secondary major in Spanish and a minor in Computer Science.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/07/17/dsb-live.html",
    "href": "ideas/datasciencebites/posts/2023/07/17/dsb-live.html",
    "title": "Heading to a conference this summer? Share your learnings here",
    "section": "",
    "text": "Three major events in the statistics and data science calendar are taking place over the next few months, and we want to give the wider community the opportunity to sample some of the exciting ideas being discussed. For that, we need your help!\nIf you’re a student or early career researcher and you’re attending one or all of the…\n\nWorld Statistics Congress\nJoint Statistical Meetings\nRoyal Statistical Society International Conference\n\n…we invite you to write about your favourite paper or session as a “Bites” post.\nBites posts are digestible, engaging, non-technical summaries of research papers and presentations, written for an undergraduate-level audience. The goal is to draw attention to key findings, potential applications, and the wider implications of new ideas and developments in statistics and data science. Advice and guidance on how to write a Bites post can be found here, and example posts can be found on our DataScienceBites page.\nFor our summer conference coverage, Real World Data Science is partnering with our friends at MathStatBites. If you write a Bites post and it is accepted for publication, the post will appear on one or both of our sites – depending on the focus of the research you’re writing about.\nIf you have any questions, please feel free to contact us. Otherwise, safe travels, enjoy the conference(s), and we look forward to hearing from you soon!\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image includes photo by Product School on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Heading to a conference this summer? Share your learnings here.” Real World Data Science, July 17, 2023. URL"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html",
    "title": "Pulling patterns out of data with a graph",
    "section": "",
    "text": "About the paper and this post\n\n\n\n\n\nTitle: Extracting the main trend in a data set: The Sequencer algorithm\nAuthor(s) and year: Dalya Baron and Brice Ménard (2021)\nStatus: Published in The Astrophysical Journal, open access: HTML, PDF.\nEditor’s note: This post is republished with permission from MathStatBites to demonstrate the Bites concept. For more information about Bites articles and how to contribute to DataScienceBites, see our notes for contributors.\nLarge volumes of data are pouring in every day from scientific experiments like CERN and the Sloan Digital Sky Survey. Data is coming in so fast that researchers struggle to keep pace with the analysis and are increasingly developing automated analysis methods to aid in this herculean task. As a first step, it is now commonplace to perform dimension reduction in order to reduce a large number of measurements to a set of key values that are easier to visualize and interpret.\nWhen working on the cutting edge, another problem scientists often face is that “we don’t know what we don’t know”. For this reason, we often want to simply ask the data, “What is interesting about you?” This is the realm of “unsupervised” methods, where the data itself drives the analysis, with little to no guidance or human labeling of the data.\nMany physical processes depend continuously on some driving parameter. For example, the evaporation rate of water increases with temperature. We call these continuous variations in datasets “trends”. Describing a dataset by a single trend reduces it to one dimension - an ordered list. Finding such a trend within a high-dimensional dataset is the aim of a method called “The Sequencer” introduced by Baron and Ménard."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#key-insight-a-tree",
    "title": "Pulling patterns out of data with a graph",
    "section": "Key insight: A tree",
    "text": "Key insight: A tree\nThe key insight of Baron and Ménard was to relate trends in data to an object from graph theory called a minimum spanning tree. Given a measure of distance between two data points, for example the usual (Euclidean) distance between two points, we can visualize a dataset as a graph. This graph consists of a node (a dot) for each data point. These nodes are then connected by an edge (a line), labeled by the distance between the two data points. The minimum spanning tree is a reduction of this graph to include only enough of the smallest distance edges so that no node is isolated.\nWhat Baron and Ménard realized is that datasets that are “trendy” have elongated and narrow minimum spanning trees. As shown in Figure 1, a totally random dataset results in a graph with many branches while a perfect sequence results in a perfect linear graph. Then, they use connectivity of the nodes in the minimum spanning tree to return an ordering of the data that follows the main trend in the dataset. However, a sequence is all you get. It is up to us to understand and interpret what this trend represents.\n\n\n\n\n\nFigure 1: Examples demonstrating that data with stronger trends have more narrow and elongated minimum spanning trees. Adapted from Baron and Ménard (2021), Figure 1. Figure used under CC BY 4.0.\nSometimes, the ordering of observations within a data point matters, like in time series data. Measurements taken close in time are more likely to be correlated than measurements taken after a long time delay. Baron and Ménard were careful to include a measure of distance that takes this ordering into account, unlike our usual notion of distance. They argue that this gives them an edge over other common dimension reduction techniques such as t-SNE and UMAP, and even go so far as to use The Sequencer to optimize the hyperparameters used by these other methods!"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#when-does-it-fail",
    "title": "Pulling patterns out of data with a graph",
    "section": "When does it fail?",
    "text": "When does it fail?\nIt is important to acknowledge that no statistical or machine-learning tool is a cure-all. And, the authors themselves are quick to point out several limitations that can hinder the application of their method. The Sequencer can struggle when the data has a large dynamic range, a variety of signal strengths relative to noise, or there are multiple trends present in the data. In each case, Baron and Ménard propose ways to mitigate these problems, but practitioners still need to be wary when applying The Sequencer in those instances."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "href": "ideas/datasciencebites/posts/2023/01/24/pulling-patterns.html#what-discoveries-await",
    "title": "Pulling patterns out of data with a graph",
    "section": "What discoveries await?",
    "text": "What discoveries await?\nTo demonstrate the power of their method, Baron and Ménard apply The Sequencer to several real datasets where a pattern was already known and show that The Sequencer recovers that pattern. Examples include ordering spectral measurements of stars by their temperature and quasars by their redshift, a measure of their distance from us on Earth.\nBut, what about new patterns? The team has already applied The Sequencer to mine seismographic data and discover previously unknown formations deep within the earth, at the boundary between the core and mantle. By sequencing the seismic waves, they realized that the main trend was the amplitude of diffraction off of these structures, which they were then able to localize beneath Hawaii and the Marquesas (DOI: 10.1126/science.aba8972).\nFor more demonstrations and discoveries, or even to upload your own data for sequencing, check out the project website. Data sleuths can also download all of the code directly from GitHub and sequence to their hearts’ content!\n\n\n\n\n\n\nAbout the author\n\nAndrew Saydjari is a graduate student in physics at Harvard researching the spatial and chemical variations of dust in the interstellar medium. He favors using interpretable statistics and large photometric and spectroscopic surveys.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor."
  },
  {
    "objectID": "feeds.html",
    "href": "feeds.html",
    "title": "RSS feeds",
    "section": "",
    "text": "Latest content\nrealworlddatascience.net/latest-content.xml\n\n\nCase studies\nrealworlddatascience.net/case-studies/index.xml\n\n\nIdeas\nrealworlddatascience.net/ideas/index.xml\n\n\nCareers\nrealworlddatascience.net/careers/index.xml\n\n\nViewpoints\nrealworlddatascience.net/viewpoints/index.xml"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "href": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "",
    "text": "Anyone who has ever worked in a retail store will be familiar with the concept of cross-selling. A customer wants a can of paint? Try to sell them some paintbrushes. That new cellphone they’ve just decided to buy? They’ll probably need a case to protect it. Online retailers (and digital services of all sorts) have taken this idea and run with it, to great success. Sophisticated algorithms sort through data on a customer’s past transactions, and those of similar-looking customers, to identify and recommend other products a customer might be interested in.\nA large amount of cross-selling, whether attempted in store by a sales assistant or online by an algorithm, relies on the concept of complementarity: that certain products are often bought and/or used together. Relationships between products might be obvious – paint and paintbrushes, for example – or they may be obscure and only revealed through the analysis of large datasets. In a 2021 paper that highlights complementarity’s relevance to association analysis, Puka and Jedrusik put forward “a new measure of complementarity in market basket data”, which sheds light on how product recommendations can be derived.\nInspired by complementarity-based ideas prevalent in microeconomics, Puka and Jedrusik begin by collecting some established ideas from traditional market basket analysis, the key one being “confidence”. In this case, we’re talking about the confidence that item A leads (in a way) to item B (which we can express in notation as conf({A} → {B})). Take a look at Table 1 (below), which presents a numbered list of 18 shopping trips, with details of what was purchased on each trip. Notice how two of the trips (1 and 3) resulted in sales of both milk (B) and cornflakes (A), while five trips (1, 3, 7, 17, and 18) had cornflakes. Under the assumption that someone already has cornflakes in their trolley, the probability that they will buy milk is 2/5 = 0.4. So, conf({cornflakes} → {milk}) = 0.4. The closer this number gets to one, the more automatic the cornflakes–milk connection becomes. This number can therefore be used to recommend an item that is related in some way to another already bought."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "href": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#asymmetry-and-tolerance",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "Asymmetry and tolerance",
    "text": "Asymmetry and tolerance\nMilk and cornflakes are reasonably complementary, and we can see from Figure 1 above that, regardless of whether you start by picking up milk or cornflakes, the probabilities of a shopper buying the other item are broadly similar: conf({cornflakes} → {milk}) = 0.4, while conf({milk} → {cornflakes}) = 0.33. There is a small amount of asymmetry in the probabilities in this particular example, but asymmetry can be more extreme for other pairs of items. This leads to the idea of one- and two-sided complementarity. Two items sharing a smallish asymmetry – like milk and cornflakes – will be connected through two-sided complementarity, while large asymmetries indicate one-sided complementarity. Such imbalances will be quite common when, for instance, items of hugely different prices are involved. When someone buys a house, for example, they may want to buy a bookcase, but buying a bookcase doesn’t mean someone wants to buy a house: this would be an instance of one-sided complementarity.\nPuka and Jedrusik capitalize on this observation. They define two items to be “basket complementary” if the two probabilities – the normal and its opposite – remain close and reasonably high. The items need to share a bond that is blind to the direction: seeing you bought one, no matter which, means you are (almost equally) likely to buy the other.\nIt is rare that the two probabilities should be exactly the same, of course, and the authors allow some deviation. Along the red diagonal line of perfect equality (Figure 1) we may lay tolerance bands marking degrees of product inseparability. This, if need be, may lead to the notion of being complementary at such-and-such a tolerance level – 0%, 1%, 5%, etc. – generating a score of sorts. In cases where a dot representing the two-way dependencies between two items falls within a narrow band – corresponding to a smaller tolerance – the more inseparable the items are, and the more sensible a cross-selling recommendation may become."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "href": "ideas/datasciencebites/posts/2023/03/02/basket-complementarity.html#in-conclusion",
    "title": "Using ‘basket complementarity’ to make product recommendations",
    "section": "In conclusion",
    "text": "In conclusion\nA large part of the world we inhabit, particularly the economy, is powered by recommendations: from strangers, friends and algorithms. That applies not only to the things we buy but also to the things we watch or read. (Perhaps you arrived at this article because of a tweet that Twitter thought you might like, or maybe it was suggested to you by Google News because of your past reading habits.) Whatever the intent of these recommendations, the key challenge is in knowing which two things are functionally or thematically intertwined. Which item or product is, by default, synonymous with which? Puka and Jedrusik deliver an answer: two items that are basket complementary to each other, preferably at a slim tolerance, are inextricably linked. One may be safely offered – perhaps always – whenever the other is already in the shopping basket.\nThe relative simplicity and interpretability of basket complementary may provide small-scale retailers, starved of analytical wherewithal, a sane and safe strategy for developing their product offer. It might also serve as a benchmark to keep other, more sophisticated recommendation algorithms in check. (In weather forecasting, for example, it is often seen that naive benchmarks – such as using today’s temperature to predict tomorrow’s – frequently outperform more advanced models.)\nBasket complementarity could also be used to help individuals understand their own shopping habits and the links between the things they buy. I’ve built an interactive dashboard where you can enter your own receipt lists and filter associations based on various confidence thresholds. The underlying code is also available.\n \n\n\n\n\nAbout the author\n\nMoinak Bhaduri is assistant professor in the Department of Mathematical Sciences, Bentley University. He studies spatio-temporal Poisson processes and others like the self-exciting Hawkes or log-Gaussian Cox processes that are natural generalizations. His primary interest includes developing change-detection algorithms in systems modeled by these stochastic processes, especially through trend permutations.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Moinak Bhaduri\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nBhaduri, Moinak. 2023. “Using ‘basket complementarity’ to make product recommendations.” Real World Data Science, March 2, 2023. URL"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2023/07/13/choosing-right-forecast.html",
    "href": "ideas/datasciencebites/posts/2023/07/13/choosing-right-forecast.html",
    "title": "Choosing the right forecast",
    "section": "",
    "text": "Nobel laureate Niels Bohr is famously quoted as saying, “Prediction is very difficult, especially if it’s about the future.” The science (or perhaps the art) of forecasting is no easy task and lends itself to a large amount of uncertainty. For this reason, practitioners interested in prediction have increasingly migrated to probabilistic forecasting, where an entire distribution is given as the forecast instead of a single number, thus fully quantifying the inherent uncertainty. In such a setting, traditional metrics of assessing and comparing predictive performance, such as mean squared error (MSE), are no longer appropriate. Instead, proper scoring rules are utilized to evaluate and rank forecast methods. A scoring rule is a function that takes a predictive distribution along with an observed value and outputs a real number called the score. Such a rule is said to be proper if the expected score is maximized when the predictive distribution is the same as the distribution from which the observation was drawn.\nMany proper scoring rules exist, such as the continuous ranked probability score (CRPS) and the logarithmic score. Choosing which rule to use is not necessarily straightforward. Furthermore, forecast methods are often selected not based on a single score, but rather averages of scores from many probabilistic forecasts, which can introduce new challenges affecting how one might rank competing forecasts. In the paper under discussion, Bolin and Wallin define several properties of scoring rules that help clarify how the rules behave when multiple forecast scores are averaged. Additionally, they introduce a new class of proper rules that aims to overcome some of the deficiencies of other common scoring rules.\n\n\n\n\n\n\nAbout the paper\n\n\n\n\n\nTitle: Local scale invariance and robustness of proper scoring rules\nAuthor(s) and year: David Bolin and Jonas Wallin (2023)\nStatus: Published in Statistical Science, DOI: 10.1214/22-STS864.\n\n\n\nThe authors argue that situations are often encountered where forecasts are derived and subsequently averaged for observations with different inherent variability. One example might be financial data, such as stock returns, where there are commonly periods with much higher variance (known as volatility in the financial setting). Such processes can be represented using a model known as stochastic volatility, where the variance of observed data evolves randomly over time. Figure 1 plots an example path of the data-generating process under such a model. When data exhibits this varying uncertainty, many proper scoring rules will assign a score whose magnitude changes for those observations with more variability, a characteristic the authors term scale dependence. Some rules will ‘punish’ observations with higher uncertainty, and others may ‘reward’ such observations. Hence, when averaging multiple scores, observations will not be treated symmetrically, which the authors argue can “lead to unintuitive forecast rankings.”\n\n\n\n\n\n\nFigure 1: Left, a time series of volatility, and right, the resulting observations under a standard stochastic volatility model.\n\nThus, an ideal scoring rule will not suffer from scale dependence. The lack of scale dependence is a property that the authors term local scale invariance. The logarithmic score possesses this attribute, but the CRPS and other scoring rules, like the Hyvärinen score, do not. To address this issue, the authors propose a new class of scoring rules which exhibits local scale invariance. Among this class is a scoring rule dubbed the scaled CRPS (SCRPS), which features many of the desirable qualities of the CRPS but overcomes the scale dependence issue.\nOf course, if local scale invariance is all that matters, then we could just use the logarithmic score in all scenarios. But there is another issue to consider when averaging forecast scores – the presence of outliers. In many scenarios, we might encounter observations that are very far outside the normal range, and we don’t want our average forecast performance measure to be greatly thrown off if such an oddity is observed. In other words, we want our proper scoring rules to be robust. In their article, Bolin and Wallin formalize the concept of robustness for scoring rules and show that, in many cases, the logarithmic score is not robust. Yet they also prove their proposed class of scaled scoring rules is not generally robust, although they show that the scoring rules can be modified to be robust (a new scoring rule they term robust SCRPS). Under such a modification, however, the scoring rule would no longer be local scale invariant in the strict sense. Indeed, under the proposed definitions of local scale invariance and robustness, finding a scoring rule that can simultaneously satisfy both criteria seems difficult. The authors conjecture that it may even be impossible.\nHence, this paper raises many questions for future consideration but achieves its goal of showing that evaluating probabilistic forecasts by averaging proper scoring rules is not necessarily a simple matter. Different scoring rules will lead to different rankings of forecasting methods, and the underlying properties of each scoring rule must be considered on a case-by-case basis. Although not discussed in this summary, the authors also compare scoring rules in several scenarios and present the theory behind the ideas examined here. For interested readers who want to dig more into these ideas, check out the full paper published in Statistical Science.\n\n\n\n\nAbout the author\n\nBrian King is currently a senior machine learning research engineer at Arm, working on applying machine learning to hardware verification. He recently completed his PhD in statistics at Rice University, where his research focused on Bayesian modeling and forecasting for time series of counts.\n\n\n\n\n\nAbout DataScienceBites\n\nDataScienceBites is written by graduate students and early career researchers in data science (and related subjects) at universities throughout the world, as well as industry researchers. We publish digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space, with the goal of making scientific papers more accessible. Find out how to become a contributor.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Brian King\n\n\nThis post is republished with permission from MathStatBites. Thumbnail image by Brendan Church on Unsplash.\n\n\n\nHow to cite\n\nKing, Brian. 2023. “Choosing the right forecast.” Real World Data Science, July 13, 2023. URL"
  },
  {
    "objectID": "ideas/posts/2023/11/06/how-to-open-science.html",
    "href": "ideas/posts/2023/11/06/how-to-open-science.html",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "",
    "text": "Open science is about making your research freely accessible to others. This includes your data, your code and any outputs (such as reports or articles).\nMany people in research, or working or studying in higher education, will be familiar with open science as a concept. As a lecturer, I was aware of it and frequently made use of open data for teaching and research, but it was not until it became a requirement from my funder that I took the opportunity to run my own research as open science by design.\nMost tools that I was already familiar with could be used to support open science, but I soon realised that there were some steps and planning that I first needed to learn. As I discovered more about the processes and principles of open science, I came to see that making my research open would not require much additional time and effort. However, I felt that a succinct guide to open science would certainly help me – and others – to make the transition more easily. So, I set out to write such a guide.\nThis is the result! It is not meant to be an exhaustive document. Rather, I will explain the route I took to open science and what options are out there for others looking to follow suit."
  },
  {
    "objectID": "ideas/posts/2023/11/06/how-to-open-science.html#what-is-open-science",
    "href": "ideas/posts/2023/11/06/how-to-open-science.html#what-is-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "What is open science?",
    "text": "What is open science?\n“Open science refers to the process of making the content and process of producing evidence and claims transparent and accessible to others” (Munafò et al. 2017). The open science principles are:\n\nOpen source\n\nAny data, code or output is accessible and usable in software that is freely available and with an open license. What this means in practice is that, for example, when sharing data, the .csv format is used rather than .xlsx, as the latter requires closed source software (Microsoft Excel) to run.\n\nOpen data\n\nResearch data should be freely accessible. One approach to open data is to adhere to the FAIR Data Principles (Wilkinson et al. 2016). FAIR stands for Findable, Accessible, Interoperable, and Reusable, and these principles can be implemented as a step to help make your work open science. However, they are not the only way, nor are they a guarantee that your work will automatically meet the definition of “open science” if you implement them.\n\nOpen access\n\nAccess to published papers and/or outputs is freely available to all. This can be achieved, for example, by sharing published papers in a pre-print server.\n\n\n\n\n\n\n\n\nWhat is a pre-print server?\n\n\n\n\n\nPre-print servers are online repositories that enable you to share versions of your manuscript before or while your manuscript is under review. Examples of such repositories include ArXiv and MedRxiv.\n\n\n\n\n\n\nPre-print server example from MedRxiv.\n\n\n\n\nOne additional benefit of open science is that it supports reproducible research. This means that others can download your data and code, re-run the analysis, and see if they obtain the same results. To get the full benefit of open science and promote reproducibility, code needs to be written with enough explanations or comments to help others understand the logic of the various stages of an analysis."
  },
  {
    "objectID": "ideas/posts/2023/11/06/how-to-open-science.html#steps-to-open-science",
    "href": "ideas/posts/2023/11/06/how-to-open-science.html#steps-to-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "Steps to open science",
    "text": "Steps to open science\nIn this section, I will outline steps you can take to easily make your research open science. There will be situations where it is not possible to make all aspects of research open – for example, due to privacy and consent issues related to data. It is still possible to share some elements of such projects, but potentially this involves additional work – to create suitable demo data, say, or generate synthetic data in order to provide data that has comparable trends but preserves privacy. It may also be possible to share the data when it is requested on a case-by-case basis. I am not going to cover this here, but it is worth considering whether open science is possible in each case.\n\nBefore you begin…\nPre-registering an analysis plan for your research helps establish that your research is confirmatory (hypothesis testing) rather than exploratory (hypothesis generating). If you have some hypotheses or research questions that are the foundation of your research, it is worth pre-registering. If your research is exploratory, pre-registration is not necessarily applicable. Although pre-registration in itself is not a requirement for open science, the process of pre-registration can all be completed within repositories such as the Open Science Framework (OSF). Pre-registering your analysis plan will add value and rigour to you research.\nIf your research doesn’t require pre-registration, jump straight to Step 1.\n\n\n\n\n\n\nWhat is pre-registration?\n\n\n\n\n\nPre-registration involves completing a form before you start your analysis to explain the primary research questions, the covariates of interest, and the methods you plan to use and why. Haroz (2022) provides more detail on how apps like OSF, Zenodo and Figshare support pre-registration. This video also gives more details.\nBelow is an example of a pre-registration.\n\n\n\n\n\n\n\n\n\n\nStep 1\nDoes your research plan require you to write a lot of code for analysis purposes, perhaps in collaboration with others? If the answer is No, skip to Step 2. If Yes:\n\nConsider setting up a GitHub repository (or repo), especially if this is a collaborative project and it is likely that more than one person will be working on the code. Don’t forget to invite your collaborators to join the repo!\nGitHub repos can be set to private and then made public at the appropriate time, so development work can take place behind closed doors and then released to the wider world when ready.\nEnsure that your code is commented properly so that it is reusable and, eventually, your results are reproducible.\n\n\n\nStep 2\nGitHub is a great tool for developing code collaboratively, but it may not be right for you – or indeed the only tool to use – if you have a lot of other material to work with and release as part of your research project. If that’s the case:\n\nSet up an area for your project on an open science repository such as OSF, Zenodo or Figshare. (If you use OSF then setting up an OSF repository is quick and easy – head to osf.io. OSF allows many integrations, including to GitHub, through the use of add-ons.)\nYou can start by setting your repository as private and then make it public at the appropriate time.\nUpload all project files, and don’t forget to invite your collaborators.\nAdd ORCIDs for every team member.\n\n\n\n\n\n\n\nWhat is an ORCID?\n\n\n\n\n\nAn ORCID is a persistent digital identifier that you own and control. It allows you to connect your ID with your professional information – affiliations, grants, publications, peer reviews, and more. You can set one up at orcid.org.\n\n\n\n\n\nStep 3\nIf you are ready to submit your research to a journal or conference, consider the following steps before you submit:\n\nCheck that there is enough information in GitHub (if using) and OSF (if using) about the project. This should include instructions for someone to be able to access your files, use the data and run the code.\nMake the GitHub and/or OSF repositories publicly visible.\nIf submitting to a journal that requires anonymous links, generate them and copy them into the manuscript. (In OSF, for example, it is possible to create anonymous links to your repository in case of double-blind submission requirements.)\nShare a copy of your manuscript on a pre-print server – but don’t forget to check the journal or conference policy on pre-prints before you do!\n\n\n\n\n\n\n\nApps and websites to support open science\n\n\n\n\n\nThis is by no means a complete list but instead features the apps and websites that are commonly used when research projects include data and code.\n\nOpen Science Framework (OSF)\nOSF is a free web app that supports researchers with sharing, archiving, registration and collaboration. The Open Science Framework website is worth checking out and includes a guide to help users get started. Once a project is public in the OSF it will have a DOI and a permanent link, so it can be cited. OSF can also support the tracking of versions of your file. One drawback can be that there is a limit on the maximum size of file that can be uploaded.\n\n\n\n\n\n\nSample OSF repository.\n\n\n\nFigshare\nThis web app supports storing and sharing research outputs (papers, FAIR data, and non-traditional research outputs). Like OSF, Figshare provides a DOI for your files and is similarly limited in the maximum size of file that can be upload.\n\n\nZenodo\nAnother general purpose open repository. As with Figshare, Zenodo also provides a DOI.\n\n\nGitHub\nGitHub is a web app that offers distributed version control. It is very commonly used for software development, especially when there are multiple developers. Although you can share code and many file types through GitHub, accessing and collaborating on projects can be a daunting experience for those who are not familiar with the way GitHub works. Also, GitHub is not always required as it is possible to share your code through OSF, for example. If you want to know more about using GitHub in support of open science and reproducibility, read “The road to reproducible research”."
  },
  {
    "objectID": "ideas/posts/2023/11/06/how-to-open-science.html#example-my-own-route-to-open-science",
    "href": "ideas/posts/2023/11/06/how-to-open-science.html#example-my-own-route-to-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "Example: my own route to open science",
    "text": "Example: my own route to open science\nIn my case, my project did not involve a heavy amount of coding or a large number of researchers, so I opted to use OSF to store the ethics approval documents, the survey questions (which drove the data collection), the data in .csv format, and the outputs. I also then linked this to Figshare from my institution and published the article on MedRxiv at the same time as I submitted it to a journal for review. The paper was eventually published in BMJ Open. The steps I took in this case were sufficient for the work to be recognised as embracing open science principles."
  },
  {
    "objectID": "ideas/posts/2023/11/06/how-to-open-science.html#plot-your-own-route-to-open-science",
    "href": "ideas/posts/2023/11/06/how-to-open-science.html#plot-your-own-route-to-open-science",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "Plot your own route to open science",
    "text": "Plot your own route to open science\n\n\n\n\nflowchart TD\n  D(\"- Set up GitHub repo\n  - Set repo as private\n  - Add collaborators\")\n  F(\"- Set up an OSF repository\n  - Set project as private\n  - Add collaborators and their ORCIDs\")\n  A(Pre-register statistical analysis plan?) -- Yes --&gt; B(Complete pre-registration through, e.g., Open Science Framework) --&gt; C(Does your research involve writing lots of code?) -- Yes --&gt; D --&gt; E(Do you plan to share data and other research material?) -- Yes --&gt; F --&gt; G(Research project is finished and ready to submit to journal or conference)\n  A -- No --&gt; C -- No --&gt; E -- No --&gt; G\n  G --&gt; H(Have you used repos?) -- Yes --&gt; I(Change repo settings - GitHub and/or OSF - to public) --&gt; J(Does publication permit sharing manuscripts to pre-print servers?) -- Yes --&gt; K(Submit to pre-print server) --&gt; L(Does publication require anonymous link to OSF repo for double-blind review?) -- Yes --&gt; M(Generate anonymous link and add to submission) --&gt; N(Submit your work)\n  H -- No --&gt; J -- No --&gt; L -- No --&gt; N"
  },
  {
    "objectID": "ideas/posts/2023/11/06/how-to-open-science.html#in-summary",
    "href": "ideas/posts/2023/11/06/how-to-open-science.html#in-summary",
    "title": "How to ‘open science’: A brief guide to principles and practices",
    "section": "In summary…",
    "text": "In summary…\nTo make your research open science, you need to:\n\nMake any data you collect or generate available to download and reuse.\nPre-register your statistical analysis plan.*\nMake your code available for download, and document it clearly so others can reuse it.\nMake any supporting material and outputs available for download in formats that are open source.\nIf publishing to a journal or conference, share manuscripts in a pre-print server.*\n\n\n* May not be relevant or applicable, depending on the nature of your work.\n\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nIsabel Sassoon is a senior lecturer in computer science and data science at Brunel University London and a member of the Real World Data Science editorial board.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Isabel Sassoon\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence Thumbnail photo by Basil James on Unsplash.\n\n\n\nHow to cite\n\nSassoon, Isabel. 2023. “How to ‘open science’: A brief guide to principles and practices.” Real World Data Science, November 6, 2023. URL"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html",
    "title": "Creating Christmas cards with R",
    "section": "",
    "text": "When you think about data visualisation in R (R Core Team 2022), you’d be forgiven for not jumping straight to thinking about creating Christmas cards. However, the package and functions we often use to create bar charts and line graphs can be repurposed to create festive images. This tutorial provides a step-by-step guide to creating a Christmas card featuring a snowman – entirely in R. Though this seems like just a fun exercise, the functions and techniques you learn in this tutorial can also transfer into more traditional data visualisations created using {ggplot2} (Wickham 2016) in R.\nThe code in this tutorial relies on the following packages:"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "title": "Creating Christmas cards with R",
    "section": "Let’s build a snowman!",
    "text": "Let’s build a snowman!\nBefore we jump in to writing R code, let’s take a step back and think about what you actually need to build a snowman. If you were given some crayons and a piece of paper, what would you draw?\nYou might draw two or three circles to make up the head and body. Perhaps some smaller dots for buttons and eyes, and a (rudimentary) hat constructed from some rectangles. Some brown lines create sticks for arms and, of course, a triangle to represent a carrot for a nose. For the background elements of our Christmas card, we also need the night sky (or day if you prefer), a light dusting of snow covering the ground, and a few snowflakes falling from the sky.\nNow lines, rectangles, circles, and triangles are all just simple geometric objects. Crucially, they’re all things that we can create with {ggplot2} in R."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "title": "Creating Christmas cards with R",
    "section": "Build a snowman with R",
    "text": "Build a snowman with R\nLet’s start with the background. The easiest way to start with a blank canvas in {ggplot2} is to create an empty plot using ggplot() with no arguments. We can also remove all theme elements (such as the grey background and grid lines) with theme_void(). To change the background colour to a dark blue for the night sky, we can edit the plot.background element of the theme using element_rect() (since the background is essentially just a big rectangle).\nIn {ggplot2} fill is the inner colour of shapes whilst colour is the outline colour. You can specify colours in different ways in R: either via the rgb() function, using a character string for a hex colour such as \"#000000\", or using a named colour. If you run colors(), you’ll see all the valid named colours you can use. Here, we’ve picked \"midnightblue\".\nLet’s save this initial plot as an object s1 that we’ll keep adding layers to. Saving plots in different stages of styling as objects can help to keep your code more modular.\ns1 &lt;- ggplot() +\n  theme_void() +\n  theme(\n    plot.background = element_rect(\n      fill = \"midnightblue\"\n      )\n  )\ns1\nNext we’ll add some snow on the ground. We’ll do this by drawing a white rectangle along the bottom of the plot. There are two different functions that we could use to add a rectangle: geom_rect() or annotate(). The difference between the two is that geom_rect() maps columns of a data.frame to different elements of a plot whereas annotate() can take values passed in as vectors. Most of the {ggplot2} graphs you’ll see will use geom_*() functions. However, if you’re only adding one or two elements to a plot then annotate() might be quicker.\nSince we’re only adding one rectangle for the snow, it’s easier to use annotate() with the \"rect\" geometry. This requires four arguments: the minimum and maximum x and y coordinates of the rectangle – essentially specifying where the corners are. We can also change the colour of the rectangle and its outline using the fill and colour arguments. Here, I’ve used a very light grey instead of white.\nIf we don’t set the axis limits using xlim() and ylim(), the plot area will resize to fit the area of the snow rectangle. The night sky background will disappear. You can choose any axis limits you wish here – but the unit square will make it easier to find the right coordinates when deciding where to position other elements. Finally, we add coord_fixed() to fix the 1:1 aspect ratio and make sure our grid is actually square with expand = FALSE to remove the additional padding at the sides of the plot.\ns2 &lt;- s1 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0, xmax = 1,\n    ymin = 0, ymax = 0.2,\n    fill = \"grey98\",\n    colour = \"grey98\"\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  coord_fixed(expand = FALSE)\ns2\n\n\n\n\n\n\n\n\n\n\nTo finish off the background, we’ll add some falling snowflakes. We first need to decide where on the plot the snowflakes will appear. We’ll be plotting lots of snowflakes, so manually typing out the coordinates of where they’ll be would be very inefficient. Instead, we can use functions to generate the locations randomly. For this we’ll use the uniform distribution. The uniform distribution has two parameters – the lower and upper bounds where any values between the bounds are equally likely. You can generate samples from a uniform distribution in R using the runif() function.\nWhen generating random numbers in R (or any other programming language), it’s important to set a seed. This means that if you give your code to someone else, they’ll get the same random numbers as you. Some people choose to use the date as the random seed and since we’re making Christmas cards, we’ll use Christmas day as the random seed – in yyyymmdd format, of course!\nWe create a variable n specifying how many snowflakes we’ll create. Creating a variable rather than hard coding the variables makes it easier to vary how many snowflakes we want. Since our plot grid goes between 0 and 1 in both the x and y directions, we generate random numbers between 0 and 1 for both the x and y coordinates and store the values in a data.frame called snowflakes.\nset.seed(20231225)\nn &lt;- 100\nsnowflakes &lt;- data.frame(\n  x = runif(n, 0, 1),\n  y = runif(n, 0, 1)\n)\nNow we can plot the snowflakes data using geom_point() – the same function you’d use for a scatter plot. Since we’re using a geom_*() function, we need to tell {ggplot2} which columns go on the x and y axes inside the aes() function. To plot the snowflakes, we’re going to make using of R’s different point characters. The default when plotting with geom_point() is a small black dot, but we can choose to use a small star (close enough to a snowflake!) by setting pch = 8 and changing the colour to \"white\".\ns3 &lt;- s2 +\n  geom_point(\n    data = snowflakes,\n    mapping = aes(\n      x = x,\n      y = y\n    ),\n    colour = \"white\",\n    pch = 8\n  )\ns3\nNow comes the part where we start rolling up some snowballs! Or, in the case of an R snowman, we draw some circles. Unfortunately, there isn’t a built-in geom_*() function in {ggplot2} for plotting circles. We could use geom_point() here and increase the size of the points but this approach can look a little bit fuzzy when the points are very large. Instead, we’ll turn to a {ggplot2} extension package for some additional geom_* functions - {ggforce} (Pedersen 2022).\nThe geom_circle() function requires at least three elements mapped to the aesthetics inside aes(): the coordinates of the centre of the circle given by x0 and y0, and the radii of each of the circles, r. Instead of creating a separate data frame and passing it into geom_circle(), we can alternatively create the data frame inside the function. The fill and colour arguments work as they do in {ggplot2} and we can set both to \"white\".\ns4 &lt;- s3 +\n  geom_circle(\n    data = data.frame(\n      x0 = c(0.6, 0.6),\n      y0 = c(0.3, 0.5),\n      r = c(0.15, 0.1)\n    ),\n    mapping = aes(x0 = x0, y0 = y0, r = r),\n    fill = \"white\",\n    colour = \"white\"\n  )\ns4\n\n\n\n\n\n\n\n\n\n\nWe can use geom_point() again to add some more points to represent the buttons and the eyes. Here, we’ll manually specify the coordinates of the points. For the buttons we add them in a vertical line in the middle of the snowman’s body circle, and for the eyes we add them in a horizontal line in the middle of the head circle.\nSince no two rocks are exactly the same size, we can add some random variation to the size of the points using runif() again. We generate five different sizes between 2 and 4.5. For reference, the default point size is 1.5. Adding scale_size_identity() means that the sizes of the points are actually equally to the sizes we generated from runif() and removes the legend that is automatically added when we add size inside aes().\ns5 &lt;- s4 +\n  geom_point(\n    data = data.frame(\n      x = c(0.6, 0.6, 0.6, 0.57, 0.62),\n      y = c(0.25, 0.3, 0.35, 0.52, 0.52),\n      size = runif(5, 2, 4.5)\n    ),\n    mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns5\nTo add sticks for arms, we can make use of geom_segment() to draw some lines. We could also use geom_path() but that is designed to connect points across multiple cases, whereas geom_segment() draws a single line per row of data – and we don’t want to join the snowman’s arms together!\nTo use geom_segment() we need to create a data frame containing the x and y coordinates for the start and end of each line, and then pass this into the aesthetic mapping with aes(). We can control the colour and width of the lines using the colour and linewidth arguments. Setting the lineend argument to \"round\" means that the ends of the lines will be rounded rather than the default straight edge.\ns6 &lt;- s5 + \n  geom_segment(\n    data = data.frame(\n      x = c(0.46, 0.7),\n      xend = c(0.33, 0.85),\n      y = c(0.3, 0.3),\n      yend = c(0.4, 0.4)\n    ),\n    mapping = aes(x = x, y = y, xend = xend, yend = yend),\n    colour = \"chocolate4\",\n    lineend = \"round\",\n    linewidth = 2\n  )\ns6\n\n\n\n\n\n\n\n\n\n\nWe’ll now add a (very simple) hat to our snowman, fashioned out of two rectangles. We can add the rectangles as we did before using the annotate() function and specifying the locations of the corners of the rectangles. We start with a shorter wider rectangle for the brim of the hat, and then a taller, narrower rectangle for the crown of the hat. Since we’ll colour them both \"brown\", it doesn’t matter if they overlap a little bit.\nThis might be one of the situations we should have used geom_rect() instead of annotate() but it might take a lot of trial and error to position the hat exactly where we want it, and this seemed a little easier with annotate().\ns7 &lt;- s6 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.46, xmax = 0.74,\n    ymin = 0.55, ymax = 0.60,\n    fill = \"brown\"\n  ) +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.50, xmax = 0.70,\n    ymin = 0.56, ymax = 0.73,\n    fill = \"brown\"\n  )\ns7\nNow we can move on to the final component of building a snowman – the carrot for his nose! We’re going to use a triangle for the nose. Unfortunately, there are no built-in triangle geoms in {ggplot2} so we’ll have to make our own. There are different ways to do this, but here we’re going to make use of the {sf} package (Pebesma 2018). The {sf} package (short for simple features) is designed for working with spatial data. Although we’re not working with maps, we can still use {sf} to make shapes – including polygons.\nWe start by constructing a matrix with two columns – one for x coordinates and one for y. The x coordinates start in the middle of the head and go slightly to the right for the triangle point. The y coordinates take a little bit more trial and error to get right. Note that although triangles only have three corners, we have four rows of points. The last row must be the same as the first to make the polygon closed. The matrix is then converted into a spatial object using the st_polygon() function, and we can check how it looks using plot().\nnose_pts &lt;- matrix(\n  c(\n    0.6, 0.5,\n    0.65, 0.48,\n    0.6, 0.46,\n    0.6, 0.5\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\nnose &lt;- st_polygon(list(nose_pts))\nplot(nose)\n\n\n\n\n\n\n\n\n\n\nWe can plot sf objects with {ggplot2} using geom_sf(). geom_sf() is a slightly special geom since we don’t need to specify an aesthetic mapping for the x and y axes – they are determined automatically from the sf object along with which type of geometry to draw. If your sf object has points, points will be drawn. If it has country shapes, polygons will be drawn. Like other geom_*() functions, we can change the colour and fill arguments to a different colour – in this case \"orange\" to represent a carrot!\nYou should see a Coordinate system already present. Adding new coordinate system, which will replace the existing one. message when you run the following code. The is because geom_sf forces it’s own coordinate system on the plot overriding our previous code specifying coord_fixed(). If you run it without the coord_sf(expand = FALSE), the extra space around the plot will reappear. We can remove it again with expand = FALSE.\ns8 &lt;- s7 +\n  geom_sf(\n    data = nose,\n    fill = \"orange\",\n    colour = \"orange\"\n  ) +\n  coord_sf(expand = FALSE)\ns8\n\nYou could skip the sf part of this completely and pass the coordinates directly into geom_polygon() instead. However, I’ve often found it quicker and easier to tinker with polygon shapes using sf.\n\nA key part of any Christmas card is the message wishing recipients a Merry Christmas! We can add text to our plot using the annotate() function and the \"text\" geometry (you could instead use geom_text() if you prefer). When adding text, we require at least three arguments: the x and y coordinates of where the text should be added, and the label denoting what text should appear. We can supply additional arguments to annotate() to style the text, such as: colour (which changes the colour of the text); family (to define which font to use); fontface (which determines if the font is bold or italic, for example); and size (which changes the size of the text). The \"mono\" option for family tells {ggplot2} to use the default system monospace font.\ns9 &lt;- s8 +\n  annotate(\n    geom = \"text\",\n    x = 0.5, y = 0.07,\n    label = \"Merry Christmas\",\n    colour = \"red3\",\n    family = \"mono\",\n    fontface = \"bold\", size = 7\n  )\ns9"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "title": "Creating Christmas cards with R",
    "section": "Sending Christmas cards in R",
    "text": "Sending Christmas cards in R\nNow that we’ve finished creating our Christmas card, we need to think about how to send it. You could save it as an image file using ggsave(), print it out, and send it in the post. Or you could also use R to send it!\nThere are many different R packages for sending emails from R. If you create a database of email addresses and names, you could personalise the message on the Christmas card and then send it automatically as an email from R. If you want to automate the process of sending physical cards from R, you might be interested in the {ggirl} package from Jacqueline Nolis (Nolis 2023). {ggirl} allows you to send postcards with a ggplot object printed on the front. {ggirl} is also an incredible example of an eCommerce platform built with R! Note that {ggirl} can currently only send physical items to addresses in the United States."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "title": "Creating Christmas cards with R",
    "section": "Other Christmas R packages",
    "text": "Other Christmas R packages\nIf you’re curious about making Christmas cards with R but you don’t have the time to make them from scratch, you’ll likely find the christmas R package (Barrera-Gomez 2022) helpful. This package from Jose Barrera-Gomez can generate lots of different Christmas cards, many of them animated and available in different languages (English, Catalan and Spanish).\nEmil Hvitfeldt has also created a Quarto extension that gives the effect of falling snowflakes on HTML outputs – including revealjs slides which is perfect for festive presentations!\nHave you made your own Christmas cards with R? We’d love to see your designs!\n\n\n\n\n\n\nInspired by Nicola’s tutorial, Real World Data Science has indeed made its own Christmas card design. Check out our attempt over at the Editors’ Blog!\n\n\n\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nNicola Rennie is a lecturer in health data science in the Centre for Health Informatics, Computing, and Statistics (CHICAS) within Lancaster Medical School at Lancaster University. She’s an R enthusiast, data visualisation aficionado, and generative artist, among other things. Her personal website is hosted at nrennie.rbind.io, and she is a co-author of the Royal Statistical Society’s Best Practices for Data Visualisation.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Nicola Rennie\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nRennie, Nicola. 2023. “Creating Christmas cards with R.” Real World Data Science, December 12, 2023. URL"
  },
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#site-content",
    "href": "ts-and-cs.html#site-content",
    "title": "Terms and conditions",
    "section": "Site content",
    "text": "Site content\nThis site and the “Real World Data Science” and “RWDS” brands and logos are copyright © The Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page. We make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged on this website so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish this site in its entirety."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nRWDS editors and contributors recommend external web links on the basis of their suitability and usefulness for our users. Selection and addition of links to our website is entirely a matter for RWDS and for RWDS alone.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders of any product, service, policy or opinion of the organisation or individual. RWDS, its editors, the RSS, or other partners and funders are not responsible for the content of external websites."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of RWDS is final and no correspondence will be entered into.\nIf you wish to report a concern, please email b.tarran@rss.org.uk."
  },
  {
    "objectID": "ts-and-cs.html#software-and-services",
    "href": "ts-and-cs.html#software-and-services",
    "title": "Terms and conditions",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for this site are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct.\nThis site is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nReal World Data Science is hosted by GitHub Pages.\nThis site uses Google Analytics 4 for web analytics reporting.\nUser comments and reaction functionality is provided by giscus, a comments system powered by GitHub Discussions. Use of this comment functionality is governed by the Contributor Covenant Code of Conduct."
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown policy",
    "text": "Notice and Takedown policy\nIf you are a rights holder and are concerned that you have found material on our site for which you have not given permission, or is not covered by a limitation or exception in national law, please contact us in writing stating the following:\n\nYour contact details.\nThe full bibliographic details of the material.\nThe exact and full url where you found the material.\nProof that you are the rights holder and a statement that, under penalty of perjury, you are the rights holder or are an authorised representative.\n\nContact details:\nNotice and Takedown,\nLicensing,\n12 Errol Street,\nLondon EC1Y 8LX\nweb@rss.org.uk\nUpon receipt of notification, the ‘Notice and Takedown’ procedure is then invoked as follows:\n\nWe will acknowledge receipt of your complaint by email or letter and will make an initial assessment of the validity and plausibility of the complaint.\nUpon receipt of a valid complaint the material will be temporarily removed from our website pending an agreed solution.\nWe will contact the contributor who deposited the material, if relevant. The contributor will be notified that the material is subject to a complaint, under what allegations, and will be encouraged to assuage the complaints concerned.\nThe complainant and the contributor will be encouraged to resolve the issue swiftly and amicably and to the satisfaction of both parties, with the following possible outcomes:\n\nThe material is replaced on our website unchanged.\nThe material is replaced on our website with changes.\nThe material is permanently removed from our website.\n\n\nIf the contributor and the complainant are unable to agree a solution, the material will remain unavailable through the website until a time when a resolution has been reached."
  },
  {
    "objectID": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "href": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "title": "Terms and conditions",
    "section": "Contributor Covenant Code of Conduct",
    "text": "Contributor Covenant Code of Conduct\n\nOur pledge\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\nOur standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\nEnforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\nScope\nThis Code of Conduct applies within all community spaces (encompassing this site, our GitHub repository, our social media channels, and any RWDS-organised online and offline events). It also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nNote that unless prior permission is agreed in writing with the editor of RWDS, only the editor and editorial board of RWDS may officially represent the community. Comment to the media must only be given by appointed representatives and must be approved by the RSS press office.\n\n\nEnforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at b.tarran@rss.org.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\nEnforcement guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\nAttribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "contributor-docs/datasets.html",
    "href": "contributor-docs/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "It’s easy to find datasets online. What’s more difficult is finding quality datasets that are suitable for specific training and development needs. On Real World Data Science we aim to solve that problem.\nOur Datasets section will provide a curated list of recommended datasets along with detailed notes and guidance on what each dataset contains, how it is structured, and how best to make use of it. In particular, we want to highlight messy rather than pristine datasets – ones that capture the imperfections and oddities found in real-world data – so that users can practice not only data analysis and modelling, but data cleaning and preparation too!"
  },
  {
    "objectID": "contributor-docs/datasets.html#structure",
    "href": "contributor-docs/datasets.html#structure",
    "title": "Datasets",
    "section": "Structure",
    "text": "Structure\nIf you have a dataset to recommend, your submission must cover the following areas:\n\nDataset name\nLink to source\nWhat data science tasks/methods can this dataset be used to demonstrate?\nHave you used this dataset for your own teaching/learning? (see Advice and recommendations below)\nWhy was the dataset originally created?\nWhen was it created?\nWho created it?\nLicences/restrictions?\nSize of dataset\nData types/description\nReal/synthetic data?"
  },
  {
    "objectID": "contributor-docs/datasets.html#advice-and-recommendations",
    "href": "contributor-docs/datasets.html#advice-and-recommendations",
    "title": "Datasets",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nHelp others to make good use of your recommended dataset. If you’ve had experience using a recommended dataset for your own teaching and learning, please consider creating an exercise for platform users to complete. If you encountered the dataset as part of a training course, competition or exercise created by a third party, make sure to give them a namecheck."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html",
    "href": "contributor-docs/datasciencebites.html",
    "title": "Data Science Bites",
    "section": "",
    "text": "Our DataScienceBites blog publishes digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space. Our goal is to make scientific papers more widely accessible.\nPosts are targeted at undergraduate level. Each presents a non-technical overview of a new research paper and its key findings, potential applications, and implications.\nWe welcome contributions from graduate students and early career researchers in data science (or related subjects) at universities throughout the world, as well as industry researchers. Contributors must have a passion for science communication and a drive to explain, clarify and demystify."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "href": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "title": "Data Science Bites",
    "section": "Basic structure of a Bites post",
    "text": "Basic structure of a Bites post\n\n\nInformation box\n\nGive the full title of the paper you are discussing, the name(s) of its author(s) and year of publication. Say where the paper is published, whether it is a pre-print or peer-reviewed publication, whether it is open access or not, and include links to authorised HTML and/or PDF versions.\n\n\n\nIntroduction and background\n\nEase readers into the paper you are writing about, and help them to see why it is worth their attention. Sometimes a paper is sufficiently ground breaking to be attention grabbing in its own right. But more often than not you will have to find a way to “hook” people in. When writing about data science tools and methods, for example, it can be helpful to start by outlining a typical application scenario or use case. Readers may be more familiar with the use case than the tool, so this provides valuable framing within which you can talk about the shortcomings of existing methods and the advances promised by the new research. Ultimately, you want to get readers to the point where they understand the problem, question or challenge that this new research paper aims to solve.\n\n\n\nResearch overview\n\nHere is where you summarise the work done by the paper’s authors. Remember to keep the discussion non-technical and jargon-free, and explain important terms and concepts as necessary. Be careful not to oversimplify!\n\n\n\nTakeaways and implications\n\nPut this paper and its contributions into the appropriate context for readers. Does it make modest but important improvements to existing knowledge or research processes? Will it help others to address new questions, issues and challenges? What further work is needed to build on these advances?\n\n\n\nFurther reading\n\nReaders may wish to learn more about the research or the broader subject matter, so feel free to point them to additional resources: videos, podcasts, textbooks, conference presentations, etc."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#word-count-target",
    "href": "contributor-docs/datasciencebites.html#word-count-target",
    "title": "Data Science Bites",
    "section": "Word count target",
    "text": "Word count target\n500–1,000 words."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "href": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "title": "Data Science Bites",
    "section": "Two ways of contributing",
    "text": "Two ways of contributing\n\nRegular contributors commit to publishing an agreed number of posts (6–12) each year. Regular contributors will collaborate closely with editors on the development of the blog, and experienced contributors will be invited to support and mentor new regular contributors to help grow the DataScienceBites team.\nGuest contributors make one-off or ad hoc contributions to the site. Proposals are to be submitted in the form of a content brief."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#next-steps",
    "href": "contributor-docs/datasciencebites.html#next-steps",
    "title": "Data Science Bites",
    "section": "Next steps",
    "text": "Next steps\nIf you are interested in contributing to DataScienceBites:\n\nIdentify a new data science publication that you are interested in writing about.\nReview the blog index to make sure we haven’t already covered this publication.\nDecide whether you would like to be a regular contributor or guest contributor.\nContact Real World Data Science editor Brian Tarran to discuss.\n\n\n\n\n\n\n\nTwo things to keep in mind\n\n\n\nDataScienceBites contributors are not permitted to write about their own research papers.\nIf contributors are in any way affiliated with the authors of papers they write about, this must be disclosed as part of their submission."
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "href": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "title": "Data Science Bites",
    "section": "About the ScienceBites family",
    "text": "About the ScienceBites family\nDataScienceBites is published by Real World Data Science and is part of the ScienceBites galaxy of sites. See sciencebites.org for an overview of the ScienceBites mission."
  },
  {
    "objectID": "contributor-docs/exercises.html",
    "href": "contributor-docs/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises on Real World Data Science will provide users with the opportunity to put knowledge, skills and new learnings into practice, helping them to challenge and refine problem-solving approaches while strengthening the analytical mindset.\nWe will achieve this by supporting contributors to design tasks that replicate real-world data scientific processes."
  },
  {
    "objectID": "contributor-docs/exercises.html#structure",
    "href": "contributor-docs/exercises.html#structure",
    "title": "Exercises",
    "section": "Structure",
    "text": "Structure\nThe structure of each published exercise will vary based on the nature of the task(s) being set by contributors and the outcomes they have in mind. But, in general, exercises must do more than simply ask users to, e.g., download dataset – analyse – report.\n\n\nSet the scene\n\nProvide a believable, realistic scenario for the exercise. Establish the “client challenge” within that context, the resources available to the data scientist, and the outputs expected.\n\n\n\nGive users space to map the problem themselves\n\nHave users translate the “client challenge” into a data analytic question that can be answered using the resources available.\n\n\n\nMake data exploration, cleaning and tidying part of the process\n\nData exploration and preparation are an important part of most – if not all – data science projects, so let users loose on messy datasets so they can figure out for themselves (a) what they’re working with and (b) what analytical approach makes sense given the features of the data and the problem at hand.\n\n\n\nIntegrate ethics\n\nPrompt users to think about and work through ethical questions and issues that are relevant to the exercise: the challenge they’ve been set, the data they’ve been given, their proposed approach to analysis and modelling, etc.\n\n\n\nEncourage users to document their work and their thinking\n\nThrough computational notebooks (e.g., Jupyter notebooks), users can record not only what they’ve done and how they’ve done it, but why.\n\n\n\nMake data presentation part of the expected project outputs\n\nAsk users to think about presenting to specific audiences: not just fellow data scientists, but to decision-makers, policy experts, the public – whatever makes most sense given the exercise scenario."
  },
  {
    "objectID": "contributor-docs/exercises.html#advice-and-recommendations",
    "href": "contributor-docs/exercises.html#advice-and-recommendations",
    "title": "Exercises",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nInvite users to share their work. If users have followed the advice to document their work and thinking, encourage them to share their computational notebooks (including their results and outputs) on their own websites, in a GitHub repository, through social media, etc. This could be a great way for others to discover your exercise, and we could also link to a selection of these notebooks through the exercise page itself.\nThink about building in hints and tips. Some users – depending on their prior level of experience – might appreciate some additional guidance here and there.\n\n\n\n\n\n\nSpoiler warning\n\n\n\n\n\nCollapsible callouts like this make hints and tips visible only to those who want to see them.\n\n\n\nBring different resources together. Exercises provide an ideal opportunity to draw together different strands of the Real World Data Science platform: case studies could provide inspiration or pointers for how to tackle a particular challenge; explainers might contain useful information about the tools and techniques to apply to specific types of data; and if you’re looking for a suitable set of data, it may already be listed in our datasets section."
  },
  {
    "objectID": "contributor-docs/case-studies.html",
    "href": "contributor-docs/case-studies.html",
    "title": "Case studies",
    "section": "",
    "text": "Case studies are a core feature of the Real World Data Science platform. Our case studies are designed to show how data science is used to solve real-world problems in business, public policy and beyond.\nA good case study will be a source of information, insight and inspiration for each of our target audiences:"
  },
  {
    "objectID": "contributor-docs/case-studies.html#structure",
    "href": "contributor-docs/case-studies.html#structure",
    "title": "Case studies",
    "section": "Structure",
    "text": "Structure\nCase studies should follow the structure below. It is not necessary to use the section headings we have provided – creativity and variety are encouraged. However, the areas outlined under each section heading should be covered in all submissions.\n\n\nThe problem/challenge\n\nSummarise the project and its relevance to your organisation’s needs, aims and ambitions.\n\n\n\nGoals\n\nSpecify what exactly you sought to achieve with this project.\n\n\n\nBackground\n\nAn opportunity to explain more about your organisation, your team’s work leading up to this project, and to introduce audiences more generally to the type of problem/challenge you faced, particularly if it is a problem/challenge that may be experienced by organisations working in different sectors and industries.\n\n\n\nApproach\n\nDescribe how you turned the organisational problem/challenge into a task that could be addressed by data science. Explain how you proposed to tackle the problem, including an introduction, explanation and (possibly) a demonstration of the method, model or algorithm used. (NB: If you have a particular interest and expertise in the method, model or algorithm employed, including the history and development of the approach, please consider writing an Explainer article for us.) Discuss the pros and cons, strengths and limitations of the approach.\n\n\n\nImplementation\n\nWalk audiences through the implementation process. Discuss any challenges you faced, the ethical questions you needed to ask and answer, and how you tested the approach to ensure that outcomes would be robust, unbiased, good quality, and aligned with the goals you set out to achieve.\n\n\n\nImpact\n\nHow successful was the project? Did you achieve your goals? How has the project benefited your organisation? How has the project benefited your team? Does it inform or pave the way for future projects?\n\n\n\nLearnings\n\nWhat are your key takeaways from the project? Are there lessons that you can apply to future projects, or are there learnings for other data scientists working on similar problems/challenges?"
  },
  {
    "objectID": "contributor-docs/case-studies.html#advice-and-recommendations",
    "href": "contributor-docs/case-studies.html#advice-and-recommendations",
    "title": "Case studies",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nYou do not need to divulge the detailed inner workings of your organisation. Audiences are mostly interested in understanding the general use case and the problem-solving process you went through, to see how they might apply the same approach within their own organisations.\nGoals can be defined quite broadly. There’s no expectation that you set out your organisation’s short- or long-term targets. Instead, audiences need to know enough about what you want to do so they can understand what motivates your choice of approach.\nUse toy examples and synthetic data to good effect. We understand that – whether for commercial, legal or ethical reasons – it can be difficult or impossible to share real data in your case studies, or to describe the actual outputs of your work. However, there are many ways to share learnings and insights without divulging sensitive information. This blog post from Lyft uses hypotheticals, mathematical notation and synthetic data to explain the company’s approach to causal forecasting without revealing actual KPIs or data.\nPeople like to experiment, so encourage them to do so. Our platform allows you to embed code and to link that code to interactive coding environments like Google Colab. So if, for example, you want to explain a technique like bootstrapping, why not provide a code block so that audiences can run a bootstrapping simulation themselves.\nLeverage links. You can’t be expected to explain or cover every detail in one case study, so feel free to point audiences to other sources of information that can enrich their understanding: blogs, videos, journal articles, conference papers, etc."
  },
  {
    "objectID": "contributor-docs/explainers.html",
    "href": "contributor-docs/explainers.html",
    "title": "Explainers",
    "section": "",
    "text": "On Real World Data Science, Explainers are the stories behind the stories of data science in action. They are deep-dive explorations of the ideas, concepts, tools, and methods that make data science projects possible. In particular, we are keen to explore and explain the statistical underpinnings of modern data science techniques.\nA good Explainer will lead audiences through the what, when, how, and why of its chosen topic. The ultimate goal is to equip data scientists with the information and insight they need to make smarter, more informed analytical choices.\nThere are many different but effective ways of structuring an explainer and plentiful written examples in major media outlets like The Guardian and Vox, but these are generally written for a non-technical audience. Examples of technical explainers (with interactive elements) can be found on Amazon’s Machine Learning University."
  },
  {
    "objectID": "contributor-docs/explainers.html#structure",
    "href": "contributor-docs/explainers.html#structure",
    "title": "Explainers",
    "section": "Structure",
    "text": "Structure\nThe following outline is a basic guide to structuring an Explainer:\n\n\nHook\n\nIntroduce your topic, and explain why audiences should pay attention. For example, does your Explainer link to one of our published case studies? Does it focus on a tool or method that has been the subject of recent attention? Is it a foundational idea that is relevant to all sorts of data science applications?\n\n\n\nHigh-level summary\n\nA short, largely non-technical explanation of your topic. A good way to view this section is as an accessible condensed version of your complete Explainer. In thinking of it in this way, you can subtly signpost to audiences the areas you’ll be covering and the questions you’ll be answering throughout the remainder of your contribution.\n\n\n\nHistory and background\n\nIt can be useful from a practical perspective to explain how ideas, concepts, tools, and methods have developed over time. Applications may have become more complex in recent years, so exploring the origins of data science techniques might lead you to discover simpler use cases that can help support and illustrate your high-level summary.\n\n\n\nThe how, the when, the why\n\nThis section of your Explainer will likely be split into multiple subsections as you seek to build up your audience’s understanding of your chosen topic. It can be helpful to think about the sorts of questions an audience member might ask and to structure your contribution so that it directly addresses those questions (Q&A/FAQ formats are commonly used in explainer-type articles). If the focus of your Explainer is a data science method, for example, you’ll want to address the following:\n\n\n\nHow does it work and how is it applied (perhaps with an example or simulation)?\nWhat are the underlying assumptions?\nHow is performance checked and assessed?\nHow should outputs be interpreted?\nWhat are the pros and cons, the strengths and limitations of the approach?\nWhat are the optimal use cases, and when should the method be avoided altogether?\nAre there alternatives that people should know about?\n\n\nKey takeaways\n\nThis serves as your final summary: a chance to remind your audience of what they’ve learned from your Explainer and the main points they should keep in mind.\n\n\n\nTell me more\n\nIt’s sensible to assume that some of your audience will have further questions and will want to learn more about the topic. If you have additional sources of information to recommend, make sure to share them here."
  },
  {
    "objectID": "contributor-docs/explainers.html#advice-and-recommendations",
    "href": "contributor-docs/explainers.html#advice-and-recommendations",
    "title": "Explainers",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nFocus on what’s important. Your Explainer can’t hope to explain everything, so you need to be clear about what’s essential for your audience to know and what isn’t. Make good use of links and references to point audiences to other valuable sources of information that can enrich their understanding of your topic.\nBe clear about your target audience and their expected prior level of knowledge. In keeping with the point above, you need to be clear in your own mind about how much you expect your audience to know already about the general topic or subject matter. You can then structure your contribution accordingly. It might also be helpful to state explicitly, at the outset of your contribution, what assumptions you’ve made about your audience and highlight any background reading that might be beneficial.\nPlan out your route. To help you decide what to cover in your Explainer, we recommend first writing out your high-level summary of the topic and also your key takeaways. This provides you with a start point (A) and an end point (B) for your contribution. The challenge then is to figure out the main points or questions you will need to address to help your audience progress from point A to point B in a way that’s logical and intuitive for them to follow."
  },
  {
    "objectID": "viewpoints/posts/2023/10/20/ai-for-humanity.html",
    "href": "viewpoints/posts/2023/10/20/ai-for-humanity.html",
    "title": "An AI for humanity",
    "section": "",
    "text": "This is the text of a talk Martin Goodson gave to the European Commission in Brussels on October 10, 2023. It is republished with permission from the Royal Statistical Society Data Science and AI Section Newsletter Substack. The views expressed are the author’s own and do not necessarily represent those of the RSS.\nFor years academics have published studies about the limits of automation by AI, suggesting that jobs requiring creativity were the least susceptible to automation. That turned. out. well.\nActually, that’s not completely true: some said that jobs that need a long period of education, like teaching and healthcare, were going to be the hardest of all to automate. Oh. dear.\nLet’s face it, all predictions about the limits of AI have been hopelessly wrong. Maybe we need to accept that there aren’t going to be any limits. How is this going to affect our society?\nStudies came out from Stanford and MIT this year, looking at the potential of AI assistants to improve the productivity of office workers. Both came to the same conclusion – that the workers with the lowest ability and least experience were the ones who gained the most in productivity.\nIn other words, AI has made human knowledge and experience less valuable.\nResearchers at Microsoft and Open AI wrote something important on this phenomenon that I’d like to quote in full:\nLet’s talk about the fairness of this. Because the AI models didn’t invent medicine, accountancy or engineering. They didn’t learn anything directly from the world – human experts taught AI models how to do these things. And they [the human experts] did it without giving their permission, or even knowing that it was happening.\nThe large tech companies have sucked up all of human knowledge and culture and now provide access to it for the price of an API call. This is a huge transfer of power and value from humanity to the tech companies.\nBiologists in the 1990s found themselves in a very similar position. Celera Genomics was trying to achieve commercial control over the human genome. To stop this happening, the publicly funded Human Genome Project (HGP) resolved to sequence the human genome and release the data for free on a daily basis, before Celera could patent any of it.\nThe HGP was criticised because of ethical concerns (including concerns about eugenics), and because it was thought to be a huge waste of money. The media attacked it, claiming that a publicly funded initiative could not possibly compete with the commercial sector. Fortunately for humanity, a group of scientists with a vision worked together to make it a success.\nAnd it was a huge success: in purely economic terms it produced nearly $1 trillion in economic impacts for investment of about $4 billion. Apart from the economics, the Human Genome Project accelerated development of the genomic technologies that underlie things like mRNA vaccine technology.\nThe parallels to our current situation with AI are striking. With OpenAI, just like Celera, we have a commercial enterprise that launched with an open approach to data sharing but eventually changed to a more closed model.\nWe have commentators suggesting that a publicly funded project to create an open-source AI would be ethically dubious, a waste of money and beyond the competency of the public sector. Where the analogy breaks down is that unlike in the 1990s, we do not have any strong voices arguing on the other side, for openness and the creation of shared AI models for all humanity.\nPublic funding is needed for an “AI for humanity” project, modelled on the Human Genome Project. How else can we ensure the benefits of AI are spread widely across the global population and not concentrated in the hands of one or two all-powerful technology companies?\nWe’ll never know what the world would have looked like if we’d let Celera gain control over the human genome. Do we want to know a world where we let technology companies gain total control over artificial intelligence?"
  },
  {
    "objectID": "viewpoints/posts/2023/10/20/ai-for-humanity.html#faq",
    "href": "viewpoints/posts/2023/10/20/ai-for-humanity.html#faq",
    "title": "An AI for humanity",
    "section": "FAQ",
    "text": "FAQ\n\nHow about all the ethical considerations around AI – shouldn’t we consider this before releasing any open-source models?\nOf course. Obviously, there are ethical implications that need to be considered carefully, just as there were for the genome project. At the start of that project, the ethical, legal, and social issues (or ELSI) program was set up. The National Institutes of Health (NIH) devoted about 5% of their total Human Genome Project budgets to the ELSI program and it is now the largest bioethics program in the world. All important ethical issues were considered carefully and resolved without drama.\n\n\nAren’t there enough community efforts to build open-source AI models already?\nThere are good projects producing open-source large language models, like Llama 2 from Meta and Falcon from the TII in the United Arab Emirates. These are not quite as powerful as [Open AI’s] GPT-4 but they prove the concept that open-source models can approach the capabilities of the front-running commercial models; even when produced by a single well-funded lab (and a state-funded lab in the case of the TII). A coordinated international publicly funded project will be needed to surpass commercial models in performance.\nIn any case, do we want to be dependent on the whims of the famously civic-minded Mark Zuckerberg [CEO of Meta] for access to open-source AI models? We shouldn’t forget that the original Llama model was released with a restrictive licence that was eventually changed to something more open after a community outcry. We are lucky they made this decision. But the future of our societies needs to rely on more than luck.\n\n\nHow about the UK Government AI Safety Summit and AI Safety Institute – won’t they be doing similar work?\nAbsolutely not! The limit of the UK Government’s ambition seems to be to set the UK up as a sort of evaluation and testing station for AI models made in Silicon Valley. This is as far from the spirit of the Human Genome Project as it’s possible to be.\nSir John Sulston, the leader of the HGP in the UK, was a Nobel Prize-winning scientific hero who wanted to stop Celera Genomics from gaining monopolistic control over the human genome at all costs. The current UK ambition would be like reducing the Human Genome Project to merely testing Celera Genomics’ data for errors.\n\n\nHow will an international ‘AI for humanity’ project avoid the devaluation of human knowledge and experience, and consequent job losses?\nIt may not be possible to avoid this. But governments will at least be able to mitigate societal disruption if they can redistribute some of the wealth gained via AI (e.g., via universal basic income). They will not be able to do this if all of the wealth accrues to only one or two technology companies based in Silicon Valley.\n\n\nHow about existential risk?\n‘Existential risk’ is a science fiction smokescreen generated by large tech companies to distract from the real issues. I cannot think of a better response than the words of Prof Sandra Wachter at the University of Oxford: “Let’s focus on people’s jobs being replaced. These things are being completely sidelined by the Terminator scenario.”\n\n\n\n\n\n\nMartin Goodson will be speaking live at the Royal Statistical Society on October 31, 2023, as part of a panel discussion on “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” Register now for this free in-person debate. The event forms part of the AI Fringe programme of activities, which runs alongside the UK Government’s AI Safety Summit (1–2 November).\n\n\n\n\nDiscover more Viewpoints\n\n\n\n\n\nAbout the author\n\nMartin Goodson is the former chair of the RSS Data Science and AI Section (2019–2022). He is the organiser of the London Machine Learning Meetup, the largest network of AI practitioners in Europe, with over 11,000 members. He is also the CEO of AI startup, Evolution AI.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Martin Goodson\n\n\nThumbnail image by Etienne Girardet on Unsplash."
  },
  {
    "objectID": "viewpoints/posts/2023/09/06/biased-algorithms.html",
    "href": "viewpoints/posts/2023/09/06/biased-algorithms.html",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "",
    "text": "Bad data does not only produce bad outcomes. It can also help to suppress sections of society, for instance vulnerable women and minorities.\nThis is the argument of my new book on the relationship between various forms of racism and sexism and artificial intelligence (AI). The problem is acute. Algorithms generally need to be exposed to data – often taken from the internet – in order to improve at whatever they do, such as screening job applications, or underwriting mortgages.\nBut the training data often contains many of the biases that exist in the real world. For example, algorithms could learn that most people in a particular job role are male and therefore favour men in job applications. Our data is polluted by a set of myths from the age of “enlightenment”, including biases that lead to discrimination based on gender and sexual identity.\nJudging from the history in societies where racism has played a role in establishing the social and political order, extending privileges to white males –- in Europe, North America and Australia, for instance –- it is simple science to assume that residues of racist discrimination feed into our technology.\nIn my research for the book, I have documented some prominent examples. Face recognition software more commonly misidentified black and Asian minorities, leading to false arrests in the US and elsewhere.\nSoftware used in the criminal justice system has predicted that black offenders would have higher recidivism rates than they did. There have been false healthcare decisions. A study found that of the black and white patients assigned the same health risk score by an algorithm used in US health management, the black patients were often sicker than their white counterparts.\nThis reduced the number of black patients identified for extra care by more than half. Because less money was spent on black patients who have the same level of need as white ones, the algorithm falsely concluded that black patients were healthier than equally sick white patients. Denial of mortgages for minority populations is facilitated by biased data sets. The list goes on."
  },
  {
    "objectID": "viewpoints/posts/2023/09/06/biased-algorithms.html#machines-dont-lie",
    "href": "viewpoints/posts/2023/09/06/biased-algorithms.html#machines-dont-lie",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "Machines don’t lie?",
    "text": "Machines don’t lie?\nSuch oppressive algorithms intrude on almost every area of our lives. AI is making matters worse, as it is sold to us as essentially unbiased. We are told that machines don’t lie. Therefore, the logic goes, no one is to blame.\nThis pseudo-objectiveness is central to the AI-hype created by the Silicon Valley tech giants. It is easily discernible from the speeches of Elon Musk, Mark Zuckerberg and Bill Gates, even if now and then they warn us about the projects that they themselves are responsible for.\nThere are various unaddressed legal and ethical issues at stake. Who is accountable for the mistakes? Could someone claim compensation for an algorithm denying them parole based on their ethnic background in the same way that one might for a toaster that exploded in a kitchen?\nThe opaque nature of AI technology poses serious challenges to legal systems which have been built around individual or human accountability. On a more fundamental level, basic human rights are threatened, as legal accountability is blurred by the maze of technology placed between perpetrators and the various forms of discrimination that can be conveniently blamed on the machine.\nRacism has always been a systematic strategy to order society. It builds, legitimises and enforces hierarchies between the haves and have nots."
  },
  {
    "objectID": "viewpoints/posts/2023/09/06/biased-algorithms.html#ethical-and-legal-vacuum",
    "href": "viewpoints/posts/2023/09/06/biased-algorithms.html#ethical-and-legal-vacuum",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "Ethical and legal vacuum",
    "text": "Ethical and legal vacuum\nIn such a world, where it’s difficult to disentangle truth and reality from untruth, our privacy needs to be legally protected. The right to privacy and the concomitant ownership of our virtual and real-life data needs to be codified as a human right, not least in order to harvest the real opportunities that good AI harbours for human security.\nBut as it stands, the innovators are far ahead of us. Technology has outpaced legislation. The ethical and legal vacuum thus created is readily exploited by criminals, as this brave new AI world is largely anarchic.\nBlindfolded by the mistakes of the past, we have entered a wild west without any sheriffs to police the violence of the digital world that’s enveloping our everyday lives. The tragedies are already happening on a daily basis.\nIt is time to counter the ethical, political and social costs with a concerted social movement in support of legislation. The first step is to educate ourselves about what is happening right now, as our lives will never be the same. It is our responsibility to plan the course of action for this new AI future. Only in this way can a good use of AI be codified in local, national and global institutions.\n\n\n\n\nDiscover more Viewpoints\n\n\n\n\n\nAbout the author\n\nArshin Adib-Moghaddam is professor in global thought and comparative philosophies, SOAS, University of London.\n\n\n\n\n\nCopyright and licence\n\nThis article is republished from The Conversation under a Creative Commons license. Read the original article.\n\n\nImage by Alan Warburton / © BBC / Better Images of AI / Quantified Human / Licenced by CC-BY 4.0."
  },
  {
    "objectID": "viewpoints/posts/2023/08/17/data-science-and-games.html",
    "href": "viewpoints/posts/2023/08/17/data-science-and-games.html",
    "title": "Where do AI, data science, and computer games intersect?",
    "section": "",
    "text": "Game studios have cemented their place among the fastest-growing media industries. In recognition of this, we hosted an event in June through the Royal Statistical Society (RSS) Merseyside Local Group to explore AI and data science in computer game development. This was an amazing opportunity to engage with a different, in-vogue domain that has unique ties to data science. We showcased two fantastic presentations covering both academic and industry perspectives.\nStanley Wang, a data scientist at SEGA Europe, opened the event by showing the methods that SEGA uses to collect, process, and apply data on player decisions in-game. It was a revealing glimpse at how smoothly in-game data collection is integrated into SEGA’s digital platforms and the ways these data can be used to engage game-centred communities – for example, running special celebrations once milestones are hit for in-game events (revenue made, goals scored, etc.) or offering real-time integration with streaming platforms so viewers can see detailed statistics on in-game progress. Stanley showed one particular example where data collection fed directly into development decisions for Endless Space, a competitive strategy game where players vie for galactic conquest. During the beta (a period where a game is available to play but still considered in-testing before commercial release), SEGA were able to monitor how well-balanced the playable alien factions were based on real-time win rate data, which led to improvements to game mechanics for the final release.\nWe also learned how SEGA’s data science teams are using clustering methods to identify different game-playing behaviours in Two Point Hospital, a simulation game where players design, build, and manage a hospital through various scenarios. After compiling high-dimensional in-game data such as objectives achieved, treatment of staff, and even furniture choices, various clustering algorithms (including k-means clustering) were used to identify common sets of player behaviour. Stanley highlighted that when using these sorts of unsupervised learning methods, it’s useful to get insights from multiple models to inform methodological decisions like number of clusters chosen or how to treat outliers. SEGA identified four distinct types of player from these analyses, which you can hear more about from Stanley in the video below. The approach allowed the company to better understand gamers’ motivations and experiences with a view to designing future game content.\n\nOur second speaker, Dr Konstantinos Tsakalidis, a lecturer in the Department of Computer Science at the University of Liverpool, presented exciting new ideas to teach computer games developers of the future. Dr Tsakalidis walked us through the curriculum for a dynamic new undergraduate program that reflects the latest software development technologies and the theory behind them. The course outline was designed around building knowledge and practice from the fundamentals upwards, starting from game physics as a prerequisite for game mechanics, game mechanics being a prerequisite for game content, and game content being a prerequisite for game AI. Combined with the continuous active involvement of students at each stage, this represented a great model of constructivist teaching. Dr Tsakalidis also proposed that practical game development (and subsequent assessments) should follow the latest research on data science and AI in computer games.\n\nDiscover more Viewpoints\n\n\n\n\n\nAbout the author\n\nAlice-Maria Toader is a PhD student at the University of Liverpool and a committee member of the RSS Merseyside Local Group. Liam Brierley is a research fellow in health data science at the University of Liverpool and chair of the RSS Merseyside Local Group.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Alice-Maria Toader and Liam Brierley\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Jezael Melgoza on Unsplash.\n\n\n\nHow to cite\n\nToader, Alice-Maria and Liam Brierley. 2023. “Where do AI, data science, and computer games intersect?” Real World Data Science, August 17, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/10/04/niclas-thomas.html",
    "href": "careers/career-profiles/posts/2023/10/04/niclas-thomas.html",
    "title": "‘I fell in love with math, really, and fell into data science because of that’",
    "section": "",
    "text": "A passion for maths and solving mathematical problems led Niclas Thomas to a PhD in machine learning with a focus on medical research. But then a conversation with a recruiter steered his career towards data science in the retail sphere. After stints at Tesco, Sainsbury’s, and Gousto, Thomas is now head of data science for Next, the clothing retailer.\nIn this interview with Real World Data Science, Thomas reflects on his career journey so far, from hands-on coding work to team leadership and management. He also argues for the importance of communication and storytelling as part of the data science skill set."
  },
  {
    "objectID": "careers/career-profiles/posts/2023/10/04/niclas-thomas.html#transcript",
    "href": "careers/career-profiles/posts/2023/10/04/niclas-thomas.html#transcript",
    "title": "‘I fell in love with math, really, and fell into data science because of that’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\nBrian Tarran\nNiclas Thomas, thank you for joining us today. I hope you’re well.\nNiclas Thomas\nI am indeed thanks. Thank you for having me.\nBrian Tarran\nToday we’re meeting because we want to find out a little bit about your career in data science, how you got into it, what you’re doing now, where you see both your career and data science as a profession going next. So do you mind– can we start by giving us a brief introduction to who Niclas Thomas is?\nNiclas Thomas\nYeah, of course. Yeah. So I’m currently working as head of data science at Next. My background is academia originally, a maths degree. I did my PhD in machine learning, and more in medical research, so more of an applied machine learning position where the idea was to try and predict, ultimately, predict disease from a given sample of data from blood – can you actually predict future disease? – which I think is a really interesting area; I love medical research. And then [I] switched over to more commercial role and worked in several retail data science roles: so, Tesco, Sainsbury’s, Gousto, and then now, as I said, currently head of data science at Next, where I run a team, and I imagine most listeners will be familiar with what Next do: a retail, a clothing brand on the whole, where the idea is, obviously to sell some great stuff, great products and put the right product in front of the right customer.\nBrian Tarran\nCan you tell us, what does your job involve? What are your sort of main tasks and responsibilities in that role?\nNiclas Thomas\nYeah, so I suppose I’m lucky enough to have been head of data science at several different companies: Sainsbury’s, Gousto, and Next. So it’s always interesting to compare the role of the head of data science in each of those three. At the moment, I think there’s a core focus on, well, ultimately making sure the teams are efficient as possible. And that really means just making sure our tech stack – what tools, what programming languages, what software we use on a day to day basis – is set up for success and make sure the team have what they need to be able to do the job as efficiently as possible, whether that’s using Python or R, whether that’s how we develop code, and how we work with other people as well, being a big part of that, then. So how do we work with other software engineers? How do we work with web developers, then, to make sure that the work we do actually gets in the hands of the business and ultimately in the hands of the customer. So that’s one aspect: it’s just making sure the team is set up for success, both in terms of the ways they work and what tools they have to work as well, then. I guess the other side of that coin is what we actually work on. So understanding the value of potential work we could do, and helping the team understand what that value is, and, and ultimately giving direction of what things we want to work on next. Obviously, that’s not my decision in isolation, but understanding on the one hand, what other stakeholders want to do, what my superiors wants to do, as well. And trying to put that all into the mix to understand these are the next best projects to work on given a finite amount of people to work on these problems. And then ultimately, then, the last part, then, is ultimately helping the team deliver those projects, those products as well then, which usually means calling on my experience of having solved these problems myself, either directly when I was earlier in my career or indirectly through leading others then or, you know, being the head of a team and working with some other great people and to learn from their experiences as well.\nBrian Tarran\nWhat does data science mean to you, personally? I’m not asking you to define it for everybody. But for you, what is what is data science?\nNiclas Thomas\nYeah, I wish I’d come up over the years with a great definition of this. But yeah, I mean, really, it just, I mean, at the very highest level, it just means using data to drive business value, I suppose, as I guess in my– which probably reflects the fact that it’s more of a business role that I have. But I think that in its broadest sense, I think that’s true: using data to drive insights and make decisions for the business. There are more, I guess, detailed definitions of that. So, for example, the way I’ve always differentiated between data analytics and data science is that if you want to make repeated decisions on a daily or weekly basis, then that’s when it becomes more about a data science question versus a data analytics question, because data analytics is generally about answering large one off ad hoc questions, rather than making the same decision over and over again and using methods appropriate for that. But, ultimately, that’s what data science means to me, I think: making repeated decisions using data and the scientific method to use data for good.\nBrian Tarran\nAnd so what do you think is your most important skill as a data scientist given that definition that you have of data science?\nNiclas Thomas\nIn my role, I suppose communication ultimately becomes the most important thing. I’d say definitely earlier in my career, and I think if you’re the person actually delivering and implementing the algorithm, I think that the technical skill set obviously is really important then. But ultimately, I almost see my role as the head of data science as a hybrid– as a link between my team and the rest of the business, then. So it’s really about being able to, on the one hand, translate technical concepts into non technical descriptions of what we’re actually doing, making sure the rest of the business can understand and vice versa, then making sure I understand the business process and business terminology well enough to be able to translate that for the team, as and when needed, into a vision for a project, a product, then, and develop a strategy for that. So I think that the communication both in the strictest sense of being able to talk that through with, with my team, with other team members, with stakeholders, as well, but also more in the looser sense, then, of being able to define that strategy, being able to define what the roadmap for a particular project or a product might look like.\nBrian Tarran\nCan you talk us through your so your education and your training that led up to your kind of first data science job, your first data science role.\nNiclas Thomas\nI suppose the first time, the first time I– actually, I’d never heard about it, I think, when a recruiter approached me. This is probably going back into 2014, when I was maybe eight months into my postdoc after my PhD. I think– obviously it did exist before that, although I suppose the terminology wasn’t quite as widespread going back almost 10 years now where the term is a lot more rife. So my original background, I did a master’s in maths originally, four years. And then I remember being– the last year of that, then, I was applying for a few jobs, and I applied for one at the Met Office, where the focus obviously was predicting weather, forecasting. And I wasn’t successful in that job. But I did notice that the, on the job spec at the time, it was PhD preferred was one of the specs on that role. It was probably the first time I thought about taking on a PhD as more of a career move rather than as the natural progression to an academic career, more of a business career move if you like, then of actually how it can help you in more business settings. So that was at least when I decided to do my PhD and thought it’s certainly not going to be– and this was back in 2008, so at the time of the financial issues at the time when getting jobs was harder anyway, so it felt like a win-win of doing something that would be– I was clear I wanted to work in a data role of some sort. And that combined with the fact that I thought it would be a good career move and the financial climate at the time wasn’t brilliant. So I took on a PhD then. And then in terms of actually getting into, into my first data science position was, as I said, just after I finished my PhD, I had been working about six months, eight months as a postdoc, and then a recruiter just described a role that was available at Tesco at the time. And it sounded a lot of what I was doing in my current postdoc role at the time – making predictions based on data and exactly the same techniques – sounded really interesting. And it must have been the way the recruiter sold it at the time as well then, because it’s something I was really keen to take on and then made my move off the back of that then. So yeah, kind of moved into it a little bit, I guess, semi deliberately from taking a PhD on first, but always with the view of moving over to a business role at some point after that.\nBrian Tarran\nBut it wasn’t like you started out your further education thinking, “I want to be a data scientist, what do I need to do to kind of get there? What are the subjects I need to focus on? What are the topics I need to research?\nNiclas Thomas\nYeah. Oh, absolutely. Yeah, it certainly wasn’t by design at the very start of my journey. I fell in love with math, really, and just fell into data science because of that, really, I loved numbers and loved solving maths problems. So that’s why I did a degree in it first of all, then and certainly, you know, even midway through my degree, then I wasn’t really sure what I wanted to do. It was more, as you say, just by chance, then, that there were a few opportune moments that came around then, that opportunities came around at the right time to fall into that career.\nBrian Tarran\nDoing a PhD in machine learning as you did, that was quite a – in hindsight – a smart choice of PhD to pursue, I think, right?\nNiclas Thomas\nI think so. Yeah, I suppose it was– still even at that stage it wasn’t necessarily, again, the terminology ‘data science’ wasn’t really around. Certainly, when I started my PhD in 2009 2010. It wasn’t really terminology, at least it may have been in usage a little bit in terms of being on, you know, if you look for jobs on LinkedIn or Indeed, but it certainly wasn’t terminology that that I would have been particularly familiar with.\nBrian Tarran\nYour first job in data science was at Tesco. You mentioned that you were you were kind of recruited to that role there. How does it compare to your current role? So I guess, you know, what’s the difference between being a data scientist versus head of data science as you are now?\nNiclas Thomas\nYeah, I think there are probably more similarities than differences, I would say. We were quite lucky in the setup in Tesco that the recruitment strategy seemed to be more focused around people who already had some experience in, generally, either already had business experience or a PhD. So we were fairly independent in solving our own– the project that we were working on and working on that. Not necessarily with the head of data science guiding us, you know, day by day, in terms of the actual nitty gritty and the technical detail, which is great, then. So it did mean that we had responsibility and ownership for our product quite early on. So yeah, I really enjoyed that. I suppose I was writing a lot more code in those days than I do now. I rarely, if ever write code at the moment. So I think that’s probably true for the last maybe three or four years, I think, only occasionally getting my hands dirty. And even when it is, it’s not really to build an algorithm, it’s more to inquire about what data we have to solve the algorithm then. So even when I do get my hands dirty, it’s more in the very early stages of the whole algorithm development lifecycle. So I think that’s probably the biggest difference is just the actual ownership of development there – probably expected, I would say, but it’s– I think that’s one of the beauties of being in your first job or two in data science. I think the– I think in most places I’ve seen, I think you’ll get ownership of, of the work, the stuff that you work on, on a day to day basis, quite early on. And you’ll be expected to contribute code and ideas for that as well, which I think most people would love. I certainly loved it at the time.\nBrian Tarran\nWhat was the most important thing you learned in your first year in that job?\nNiclas Thomas\nI think, again, it’s probably a lot around the ways of working, I would say – of the various ways you can [work], which I never really thought about it before. Working in academia, it was quite isolated, I suppose. You work on your own project, you work on your own work and don’t really– or at least, I found I didn’t really work with anyone else that much. Maybe that was the nature of my work as well, we’d obviously be dependent on people working in a lab to get data. But I think the day to day work, I was working quite in isolation, whereas the team aspect of working, I think, was a steep learning curve then – so agile methodology, and everything around that, which was very, very new to me. And the various ways you can do that. I’m generally not someone for overly putting processes in place in a team, only where necessary. But I think there’s some great learnings from that as well. It certainly started to shape how I think I would want to run a team if and when I got to that position.\nBrian Tarran\nSo, Nick, what have been your career highlights so far?\nNiclas Thomas\nI think in terms of– there was one product we built in Sainsbury’s in particular. So in terms of, on a product level of replenishment. So how do you most efficiently get products from the back of the store onto the shelves of an individual store? And what’s the most optimal strategy to do that, which I love for a variety of reasons. A, it was one of the first full data science products that we had deployed and worked on as a team in Sainsbury’s. So there was that kind of milestone about it. I think it also stood out as a really nice move away from classic machine learning – i.e., making a prediction, a classification model – to something that was a bit more operations research based and more based on optimization. So using graph theory, making a graph network of a store. And using that to solve the problem of taking a route through the store, for example, a bit like a Google Maps for a store basically, was how we always pitched it to our stakeholders, and how can you choose the best route and again, moving more into a bit more of a vehicle routing problem, then: if you’ve got two different trolleys, how do you decide what items to put on trolley one versus trolley two? So there’s loads of interesting stuff on the technical side of things and it was, again, I felt it was probably one of the highlights – as well as the end product, it was also the one I worked on at the very start. So actually, the understanding whether it would be possible to do that, what kind of technical approach. So I think certainly from a product perspective, that’s probably stuck in my mind. Aside from that, on a more personal level, I guess, I did decide to write a book off the back of my PhD. Just mainly on my experiences from my PhD and postdoc. I mean, it’s not like a confessional. But more on the– just working with non data scientists and making it more accessible was really what I really focused on there. So having worked with clinicians, immunologists and others as part of the medical research that I did, I felt that data can be accessible if you pitch it in a way and make it easy to use. And so that was the purpose of what was largely an educational textbook.\nBrian Tarran\nDo you want to give a short plug for the book, what it’s called and where people can find it?\nNiclas Thomas\nYeah, so it’s, Data Science for Immunologists is the name of the book. It’s available on Amazon. I’m one of the two co-authors on that then. And we do have a website, datascienceforimmunologists.com, as well then if you did want to visit and you can either buy the book, there’s a link on that website or just go straight to Amazon and it’s available there.\nBrian Tarran\nThis next question, we’ve gone from highlights to lowlights. Have there been any mistakes or regrets that you’ve had along the way in your data science career so far?\nNiclas Thomas\nThe main mistakes I think I’ve made before is not valuing, A, communication or soft skills, but B, the leadership and management as well then. And I think especially it’s something, when working at Gousto as well that was something that was a big focus of the team and something that I really took from my time there as well was the, I guess, the art of good management and good leadership, you know, what the difference is between the two. So I wouldn’t say there’s any one bang event that’s a mistake or regret, but it’s probably, as ever, it’s probably I would have put more emphasis on it sooner had I known that how important those skills would be.\nBrian Tarran\nYeah, but I think that’s understandable to a certain extent. If you’re coming from, I guess, a role that’s very hands on, doing things yourself, getting into the messy details of a project, it can sometimes be hard to kind of take a step back and adopt more of a kind of leadership, management position, can’t it?\nNiclas Thomas\nYeah, definitely. Yeah, definitely. I would agree with that. And I think it’s also, I’d probably say for a lot of people starting out, and certainly it was for me, that the technical– the technical aspect is probably why you get into a role in data science in the first place, that you just love solving problems, basically, whether that’s with code or with pen and paper. And so that’s, that’s what you want to do. And getting your mind focused elsewhere away from that is probably not viewed as the most fun thing to do, I probably wouldn’t have, when I was starting out in 2014, 2015, I probably wouldn’t have thought it was as fun or as interesting to do that as I do now, maybe. So I think that’s the other reason why it probably doesn’t get as much focus earlier on in my career anyway, at least, as it probably deserved.\nBrian Tarran\nHow do you think your– how do you see your role, I guess, evolving over the rest of your career in data science?\nNiclas Thomas\nI suppose on a personal level, for me it’s, I’m always thinking of what, 10 years down the line, do I still want to be focused just on data science? Or do I want to be focused on a data role, more broadly? I suppose that’s always the main question to ask. And so by that I mean, looking at data engineering as well, data analytics, and being responsible for a wider group. I think the way the field is going anyway, I think a lot more companies seem to move to vertical management rather than horizontal. So by that, I mean having heads of data in different areas of the business. So rather than having a head of data and a head of analytics, you might have a head of data for certain aspects of the business and another head of data then that’s responsible for both in other areas of the business, then. So either way, I think that the broadening of responsibilities and not just being responsible for data science is probably one way I would see my career potentially moving. At the moment, I love just focusing just on the data science, I’m really happy doing that now. But I think that could be one way that my focus changes in the future.\nBrian Tarran\nWhat personal or professional advice would you give for anyone wanting to be a data scientist?\nNiclas Thomas\nYeah, so first of all, the balance between the soft and hard skills. I think I’ve alluded to it before, but the– don’t put too much– I mean, still emphasise on the technical skills are really important, but don’t feel like it’s the be all end all. I think just understanding the softer side of how you communicate, how you tell a story, for example, and storytelling with data, I think is really important. So I’d say that’s probably one focus area. I think that the second would probably, and maybe it’s a harder one to act on, but being passionate, I think, because whenever I’m looking to recruit anyone new into my team, I think it’s as much about understanding what the potential of that person is as is what is their current performance or where their current capability is – how good they could be in the future is arguably more important. And I think a lot of that comes to ultimately someone’s– whether they have a fixed or growth mindset. So by that, I mean, ultimately, do they want to learn or not, and if they really want to learn, as a lot of data scientists do, but if they have a huge passion for or about data science, and wanting to learn about just how to get better – whether that’s a better coder, better at maths, anything around that – then if you have that attitude, I think then it’s, A, you can have a great impact on our team, but B, I think it’s a sign of someone who can be a great performer in the future.\nBrian Tarran\nSo what do you think will be the main challenges facing data science as a field over the next few years?\nNiclas Thomas\nI think probably, certainly, currently maybe living up to the hype, I suppose. And matching I suppose the classic Gartner Hype Cycle of, it feels like we’re probably at the stage where there’s a lot of– the hype has been around for a few years of data science now and I think making sure we tackle the right problems, I suppose, is one of the – and by ‘we’ I mean, Next as a business or whatever business we’re working in at the time – I think it’s making sure we’re working on the right things. Because I think a lot of people will be keen to have data scientists as part of their work and the product they’re trying to build. What is the best place to spend our time, and what projects we should be working on most I think is– becomes important then because, as I say, there’s a huge demand for data scientists time, I think, in every company. And so choosing where we spend that time wisely, I think, becomes the key challenge and the important decisions for, especially for a head of data science like myself to make then, to make sure we’re best using the team’s capacity, then.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I fell in love with math, really, and fell into data science because of that.’” Real World Data Science, October 4, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/06/28/claire-morton.html",
    "href": "careers/career-profiles/posts/2023/06/28/claire-morton.html",
    "title": "‘I was inspired by the power that numerical data have to tell stories and promote policy change’",
    "section": "",
    "text": "This week, in celebration of Pride, Real World Data Science is collaborating with the JEDI Outreach Group of the American Statistical Association (ASA) and the ASA LGBTQ+ Advocacy Committee to highlight the achievements of statisticians and data scientists from across the LGBTQ+ spectrum.\nMembers of the committee nominated two individuals to be featured as part of our career profile series, and so we are pleased to bring you interviews with Claire Morton (below) and Albert Lee.\nRead on to discover more about Claire’s data science career (so far).\n\n\n\nHi, Claire. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nMy name is Claire Morton, and I’m an undergraduate student studying mathematical and computational science and environmental justice at Stanford University. I’m particularly interested in using statistics and data science to work with community-based organizations and advance evidence-based environmental justice policy. I have conducted quantitative research on tools to classify disadvantaged communities, oil wells, climate resilience, housing justice, and the connections between soil and health.\nWhat drew you to study statistics and data science?\nI really enjoyed my math, statistics, and coding classes in school. I was also inspired by the power that numerical data have to tell stories and promote policy change.\nWhat do you think is your most important skill as a data scientist?\nListening to others. Listening allows me to learn new statistics skills from my mentors and to learn about how best I can work with community partners on their priorities in my research.\nHow does your gender and/or sexual identity factor into your career?\nI am a lesbian, and, at the start of college, I didn’t have any mentors who shared my identity. I’ve now found several through the ASA and queer communities at my university, and I’m continually inspired by the achievements of queer statisticians, mathematicians, and computer scientists. My research hasn’t explicitly connected statistics and queerness yet, but I’m interested in working on projects involving hard-to-reach populations, such as queer people, in the future.\n\n\n\n\n\n\nClaire Morton\n\n\n\n\n\nIt’s important to be able to take initiative to learn skills, talk to people, and solve problems as they come up – but it’s also critical to not be afraid of asking for help when you need it.\n\n\n\nHow did you get into data science?\nIn high school, I worked in a cell biology lab. As part of that work, I learned to model cellular processes and analyze data from my experiments. I realized that those elements were my favorite parts of the science I was doing, so I decided to study math, computer science, and/or statistics in college. I had always been interested in environmental issues, and so I got involved in quantitative research about environmental justice. I realized that this type of research allowed me to connect the skills I have to my passions, so I’ve kept working in these areas ever since.\nWhat, or who, first inspired you to pursue this career path?\nMy mom! She’s also a statistician, and she has encouraged and mentored me throughout my academic journey. I’m inspired by her success as a woman in statistics.\nWhat hurdles or challenges have you faced in your studies?\nMy classes can be tough, which makes it hard to stay motivated sometimes. I also struggled to maintain a healthy work-life balance at the start of college. Finally, it has been tough to learn some of the ins and outs of the research process and publications – how best to engage with research mentors, what it looks like to write and submit a paper, and some of the nuances of working in academia. I think my next big challenge is deciding what to do after college, though I’ve been trying to reframe the question as an opportunity rather than a hurdle. I’m excited to continue doing research at the intersection of statistics and public policy in the future.\nWhat was your first job in data science, and how does it compare to your current role?\nMy first job in data science was as a researcher, working at a non-profit called Physicians, Scientists, and Engineers for Healthy Energy (PSE). As part of this job, I worked with community-based organizations to code quantitative optimization models to locate climate resilience hubs in California that took community priorities into account. I’m currently a student, which involves less research but gives me the chance to focus on learning new skills for my next job. I hope to be able to work in a role like my job at PSE, combining statistics/data science, community engagement, and policy impact, in the future.\nWhat was the most important thing you learned in your first year on the job?\nThe importance of being adaptable and self-directed. Research projects shift and change as you uncover new information, and it’s useful to be able to shift with them. It’s important to be able to take initiative to learn skills, talk to people, and solve problems as they come up – but it’s also critical to not be afraid of asking for help when you need it.\nWhat have been your career highlights so far?\nOne was publishing research about the demographics of people living near oil wells in California, which informed policymakers about racial and socioeconomic differences in exposure to oil wells and is part of a long-standing effort from activists and researchers to protect the health of Californians. I’ve also gotten to work directly with organizers on several mapping projects, which was deeply fulfilling. Finally, I loved getting to present my research at the Joint Statistical Meetings last year, and I look forward to presenting my undergraduate thesis this year.\nWhat three things are at the top of your reading/study list?\nSome statistical areas I’m hoping to learn more about are spatial statistics and survey methods. Some books I’m excited to read are The Color of Law, Data Feminism, and Thicker Than Blood: How Racial Statistics Lie. \nWhat advice would you give for anyone wanting to study statistics and data science?\nFind mentors that inspire you, support your career goals, and challenge you to learn and grow as a statistician.\nWhat new ideas or developments in the field are you most excited about or intrigued by?\nI’m really interested in combining quantitative research with community-based research, so I think that cross-disciplinary developments are exciting. I’m intrigued by AI tools, and I’m interested to see how these tools change what the day-to-day of being a statistician looks like and the skills that are most sought after in a statistician.\nAnd what do you think will be the main challenges facing the profession over the next few years?\nThe main challenges will be related to statistical literacy, both for the people consuming and doing statistics. While statistical methods and data becoming more accessible is a positive development, it has meant that more analyses are done incorrectly and that more misleading results are publicized (and absorbed) as truth. It’s getting much easier to twist numbers to support whatever we want them to say, and I think this will continue to challenge both statisticians and non-statisticians in the future.\n\n\n\n\n\n\nAbout the ASA Pride Scholarship\n\n\n\nThe ASA Pride Scholarship was established to raise awareness for and support the success of LGBTQ+ statisticians and data scientists and allies. The scholarship will celebrate their diverse backgrounds and showcase the invaluable skills and perspectives these individuals bring to the ASA, statistics, and data science.\nApply or nominate someone for the ASA Pride Scholarship.\n\n\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Claire Morton is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘I was inspired by the power that numerical data have to tell stories and promote policy change.’” Real World Data Science, June 28, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/06/20/chanuki-seresinhe.html",
    "href": "careers/career-profiles/posts/2023/06/20/chanuki-seresinhe.html",
    "title": "‘Once I started to see what was possible with data science, there was no going back’",
    "section": "",
    "text": "Hi, Chanuki. Thank you for sharing your career story with Real World Data Science. Please tell us a little about yourself and your role in data science.\nI am Chanuki Seresinhe, head of data science at Zoopla and Hometrack. My commercial career in data science began in 2018 at Channel 4. Since then, I have worked at a few different companies – from startups to scale-ups – before ending up here at Zoopla in 2022.\nI am also the founder of beautifulplaces.ai, which is a continuation of my University of Warwick and Alan Turing Institute PhD work where I provided the first large-scale evidence that beautiful places are connected to our wellbeing.\nWhat does your job involve?\nMy role at Zoopla involves managing data scientists both for Zoopla and Hometrack. At Zoopla, we use data science to create an engaging experience for users who want to buy, sell and rent properties. At Hometrack, the data scientists mainly work on an automated valuation model that provides property valuations to most of the major mortgage lenders in the UK.\nAs a leader in data science, my role primarily involves helping stakeholders across the business to best leverage data science to reach our business goals, as well as ensuring my data science team has all the support and mentoring they need to design, develop and maintain high performing data science algorithms.\nWhat does “data science” mean to you?\nThat is a really good question! One thing I have noticed is that people who aren’t familiar with the field often confuse data science and data analytics. There are indeed many similarities – both require quite a bit of knowledge to be able to leverage the correct insights from both structured and unstructured data (data hidden within images, for example). However, data science is essential when you need to make inferences. For instance, you are not only analysing data to see what certain consumers may prefer, but you are also predicting what similar consumers might prefer. Thus, having a strong grounding in statistics is really important for anyone working in this field.\n\n\n\n\n\n\nChanuki Seresinhe\n\n\n\n\n\nIn data science, getting the model right is not enough, and working with people across the business to make sure the model can be integrated into the business processes is essential.\n\n\n\nWhat do you think is your most important skill as a data scientist?\nAside from a good grounding in the technical aspects of data science (which is possible for really anyone to pick up from the many good courses that are available), the most important skill is how you can leverage data science to create products that actually create value for the business. I find this to be the most challenging journey that junior data scientists find themselves having to navigate. They are really excited about the technology, and get carried away with wanting to perfect their algorithms. But when you are building commercial products, what is really important is to constantly engage with stakeholders to make sure you are building something that actually has a tangible business benefit. Early release of a model for user testing is also essential, as models only really get better once you have real user input.\nHow did you get into data science?\nIt was somewhat by accident. I previously had a long career in digital and decided to take a career break to return to university and study economics. When I was working on my Master’s degree in behavioral economic science at the University of Warwick, I saw an ad for a PhD to “use online data to understand human behaviour” and I thought this was perfect, as it combined my prior knowledge with a new area I was increasingly becoming drawn to. I quickly taught myself how to program in Python and convinced my supervisors to take me on, and from there on, I came to love data science!\nWhat, or who, first inspired you to become a data scientist?\nIt was more about realising what you could do with data science. In my PhD, I was quantifying the connection between beautiful places and our wellbeing. While this has long been an intuitive connection, we were not able to test this on a large scale due to lack of data. Being able to use data science to start predicting the beauty of outdoor images was fascinating as it opened up a whole new method for potential research combining beauty with various wellbeing ratings. Once I started to see what was possible with data science, there was no going back.\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nFor me personally it was the challenge of moving sideways into a leadership role after my PhD and not having to start all the way from the bottom. I would have loved to continue in academia and expand my research even further, but starting from the bottom earning a tiny salary after I had taken quite a large career break to do my PhD wasn’t an option for me. So I decided to go back into the commercial world and look for a senior role from the get go and luckily Channel 4 agreed to give me my first commercial stab at data science.\nWhat are the challenges that you face now, as a working data scientist?\nTrying to keep up with everything that is constantly changing in the world of data science. I love the rapid change but it can also be quite time consuming to make sure you are on top of it and giving the right advice to people.\nWhat was your first job in data science, and how does it compare to your current role?\nMy first job was working as a senior data scientist at Channel 4. As a senior data scientist, even though you have additional goals to help run the team and be a mentor to more junior data scientists, you still get a great deal of time to do coding and develop your own projects. When you move more into a management role, the time you have to develop data science models diminishes. People also expect you to give in-depth guidance when you haven’t actually had much time to deep dive into a project. So, I am often trying hard to make sure I am on top of what is going on even when I have limited time, and really focus on building a strong team that can support each other and collaborate often to create better data science products. Learning to delegate is key!\nWhat was the most important thing you learned in your first year on the job?\nHow hard it is to actually get organisational buy-in to use data science at scale. It is really easy to get approval to build a proof of concept (POC). However, if you do not use the time when developing your POC to also make sure to get the right stakeholder on board, your project is dead before it even starts. So, in data science, getting the model right is not enough, and working with people across the business to make sure the model can be integrated into the business processes is essential.\nWhat have been your career highlights so far?\nIt has been great being able to give talks about my research, and data science in general, all around the world. I have actually come to love public speaking, and I hope that as I continue to be recognised for my expertise, I can encourage and aid potential data scientists with their careers – especially minority women, as I think that diversity in the field is very important. This is a role that is fit for people from all kinds of backgrounds and I hope I am exemplifying this.\nHave there been any mistakes or regrets along the way?\nIn smaller companies, it can easily happen that the founders don’t fully understand data science and often use data science as a buzzword to get investors on board. Whenever you take on a new job, and data science is just getting established, make sure the founders or leaders are actually fully onboard with integrating data science into the product and understand what this means. See if they know how tricky data science can be when first integrating into a product and are actually willing to overcome the challenges with you to eventually reap the huge benefits data science can bring.\nHow do you think your role will evolve over the rest of your career?\nI see my role evolving into being more strategic and less about the data science day-to-day modelling. It is more about being able to advise companies on how to make use of data science as a strategy and helping them figure out where in the product or process to inject it to get the most out of it for the business as a whole.\nIf you were starting out in data science now, what would you put at the top of your reading or study list?\nPractise how you would apply using data science for a real life problem. Seek a placement, as this will pay dividends in being able to speed up your learning.\nIf you don’t understand the statistics involved in data science, make sure to upskill in that area before starting your first role. A lot of junior data scientists focus on learning how to code or get carried away with the modelling without first learning the importance of preparing the data in the correct way so that your predictions can work well in a real life setting.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nTry to find ways to stand out from the average data scientist. When we open up applications for data science jobs, I get hundreds of applications for each one. I am looking for people who can not only do data science but who also have other stand-out qualities that they can bring to the business. This can be something along the lines of effective stakeholder engagement to deep expertise in a certain domain or technology.\nWhat new ideas or developments in the field of data science are you personally most excited about or intrigued by?\nI am really interested to see where generative AI will take us – particularly about how it can help us improve the speed of our own performance. It feels like generative AI can be a technology that can help everyone – even the everyday person – as it can help speed up so many processes, from coding to writing to ideating. While the technology is still in its early days, it is progressing rapidly and I am very curious to see where this will lead in the next year!\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nGenerative AI’s latest breakthroughs have made AI capabilities accessible to the broader public, but it has also stoked fears around the use of AI. The headline-grabbing narratives around AI and existential threat is distracting from other conversations that are really important. There are some very real issues we do need to solve – from biases in AI to the impact generative AI can have on wages and workforce – but this needs to be approached in a constructive and thoughtful way.\nWe need to find a way to engage with the public in a more meaningful way – rather than scaremongering – to have public debates on issues that actually matter.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo of Chanuki Seresinhe is not covered by this licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Once I started to see what was possible with data science, there was no going back.’” Real World Data Science, June 20, 2023. URL"
  },
  {
    "objectID": "careers/career-profiles/posts/2023/05/24/jasmine-holdsworth.html#transcript",
    "href": "careers/career-profiles/posts/2023/05/24/jasmine-holdsworth.html#transcript",
    "title": "‘For me, data science is about bridging the gap between business requirements and the data that businesses have’",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove some repetitions.\n\n\n\n\nWhat does your job at Expedia involve?\nI would probably call myself more on the analyst side. So, while my day-to-day job doesn’t necessarily involve AI, ML and productionalising models, it’s more taking business goals or requirements and taking the data and essentially bridging the gap between the two. I am on the incrementality analytics team. So, what that means is I measure the short-term returns from our marketing efforts that we have. And I do that via geotesting. So, I’m essentially working in the geotesting part of the company if you like. And before that I worked in the customer data section. So, essentially looking at customer data and working with that.\n\n\nHow long have you been working in data science?\nMore in an analyst role, but probably about seven years now, I began in Stack Overflow just as a data analyst, and then worked at DAZN – which is like a Netflix for sports – as a data analyst, and then joined here as a senior analyst, and then moved into data science in the last couple of years. I would, I would credit Stack Overflow as the place where my career kind of was birthed, if you like. I started there as an account manager, so with hardly any technical background at all required, and then moved into a role that was essentially looking after, or reporting the metrics of advertising campaigns for companies that would advertise on Stack Overflow. So that required a little technical knowledge, not much – a few pivot tables and things like that. But then the longer I stayed there, the further my career developed, and they had, at the time – probably still do – some fantastically smart people that work there, as you can imagine. I was sponsored to do a General Assembly data analytics course, which was focused around Excel, dashboarding, and SQL and essentially fell in love with data analysis. It was the most technical subject matter that I had experienced to that point, and I found a real natural affinity to it, particularly SQL. And then [I] moved into more of a data analyst role within Stack Overflow, so – as you can probably imagine – an absolute sea of proprietary data that needed analysing, and started learning R, or rather being taught R within Stack Overflow, and loved it. I think I was there for three-and-a-half years, and then moved into a data analyst role at DAZN. At this point, I did a data science General Assembly bootcamp course, and fell in love with that. And then I decided that I really loved General Assembly as a concept; I actually started a second job teaching there, so the courses that I had previously taken I was now teaching, first as a teaching assistant, and then as a lead instructor, which was one of the most, yeah, one of the most amazing experiences I’ve had actually, I learned a lot from that. And then I got a job as a senior analyst within Expedia Group, which is where I am now, and then moved into a data science role, which is what I’m doing currently. So, I actually left school at 16, and had to go into a full-time employment. And the General Assembly education that I took a part in was my first of that type. So, when I realised that data science was absolutely something that I really wanted to dedicate the rest of my life to, I decided to take on a part-time data science bachelor’s degree, which I am now about a year away from finishing. Because I’m doing it part time it takes a bit longer. But yeah, so I will have my data science bachelor’s completed, hopefully, by 2024.\n\n\nWho or what inspired you to work in data science?\nThere were two big inspirations into getting into data sciences. So, they were actually the data scientists that I worked with at Stack Overflow. They were the first two data scientists, I believe, that Stack Overflow had ever hired. I worked very closely with them as an analyst and one of them was, had previously worked – I don’t know if it was officially an astrophysicist – but had studied black holes, and I remember thinking that was just amazing. And the other was, was very famous within the field. And they spent a lot of time giving me one-on-one training on R and SQL and basic analysis, and I was so inspired by these two individuals that I, it was also a career path that I didn’t really know existed.\n\n\n\n\n\n\nJasmine Holdsworth\n\n\n\n\n\nWhat was impressed upon me in that first year [in data science] was the importance of statistics and interpreting statistics in a way that’s honest.\n\n\n\n\n\nWhat does data science mean to you?\nFor me, it is bridging the gap between the business requirements and the data that businesses have. So, you’ll have business goals, requirements that kind of come down the line and there’s a lot of data that’s being collected, and, essentially, you have to try and be the bridge between the two. So, not just doing very complicated analyses, with very sophisticated models – at least not in my role – it’s about being able to create analysis that’s interpretable, that you can present to non-technical stakeholders that they’re going to understand to a degree. So, I do know that in different roles in different companies, it will be slightly different. But yeah, for me, it’s about making data, yeah, interpretable, to the non-technical stakeholders to enable them to do their job better.\n\n\nWhat is your most important skill as a data scientist?\nI like to think that there is one responsibility around how statistics are interpreted. So, just making sure that when you’re giving someone a statistic, that they understand what it can be used for, what it can’t be used for, and that it doesn’t kind of get halfway around the company before, you know, without any danger of it being misinterpreted. And I do think that the other is just being a translator. So, as well as teaching with General Assembly, I teach people within my company, things like SQL, R, and some basic data analysis. And I feel like it’s taking what is quite a technical, complicated subject and almost translating it into, if you like, English, so that people can kind of get some sense of what something may mean, without necessarily having to have the degree to back it up.\n\n\nWhat hurdles did you face in becoming a data scientist?\nTowards the beginning of my career to say – 5, 6 years ago – it was quite hard to get interviews. It was never hard to get interviews with technical people within companies, because you can– a technical person can see whether or not you know what you’re talking about. But recruiters don’t, and if someone is a recruiter for a technical role, their proxy for whether or not you can do the role is what’s your level of education, which is completely understandable and that’s what education is for. But it did mean that sometimes I applied for roles that were well below my, my level, and if I did so through a recruiter, then I wouldn’t hear anything back. But if I spoke to a technical person within that company, then it would be fine.\n\n\nHow did you overcome those hurdles?\nActually, I guess the story of how I joined Expedia is quite relevant in that way. So, I presented some, just some fun analysis that I did at an R-Ladies meetup, and I was already talking to a recruiter within Expedia Group and I said to them, oh, well, I happen to be visiting your offices to present at this meetup, so maybe I can meet you there. And they actually sent the manager of the team that they wanted me to join. So, this manager attended the meetup, watched me present, and then they ended up hiring me, which is really great. But I do really think that that was a result of being able to see me on stage, talking about stuff that I had done, showing code that I had written, and it kind of bypassed a few steps. So yeah, I would definitely say meetups and connections are very helpful to overcome that.\n\n\nThe most important lesson from your first year in data science?\nI think that what was impressed upon me in that first year – and what really drove me to do the bootcamp courses and then, ultimately, the degree – above everything, actually, was the importance of statistics and interpreting statistics in a way that’s honest. So, I feel like– I feel like with coding, that comes quite naturally to me, and writing SQL queries, R, that was all kind of fine. That didn’t require a lot. But I really, I had an amazing manager who taught me a lot about, essentially, if you’re going to go and speak to this company about the campaign that they’ve run on our website, then you need to impress that X doesn’t necessarily mean Y, it just gives evidence to, or alludes to, and essentially just making sure that how you’re communicating things is as accurate as possible.\n\n\nAny mistakes or regrets in your career so far?\nWhen I look back on my career, I think the things that have really stayed with me that I’ve really learned from, mistakes wise, are around small little mistakes around how you interpret data. Maybe it was a, like, years ago, summing the wrong cell in Excel but not checking two or three or four times before that goes out. I’m now quite– I over-check everything. I think that the most important part of our job, as well as being the translation, is being the correct translation. You need to be reliable. People need to know that if you put out analysis that they can trust you. So, I would say I regret every small, tiny little data error that I ever made, which I can’t even recall right now, but I know have kind of cumulated enough that it has made me a very fastidious checker, I suppose.\n\n\nHow do you see your role in data science evolving?\nI definitely see myself being an individual contributor in a consumer-facing company, just because that is basically what I’ve been doing up to this point. I don’t really have any desire to get into people management. I very much love being stuck in a room with my laptop, solving problems. Above all else, it still makes me happy. However, I do also love knowledge sharing – I love teaching, whether it’s with General Assembly, or whether it’s within the company that I work now. And I would like to kind of balance those two goals moving forward. So, keeping my role within my company as like an individual contributor and actually being like the front face for the, for the analysis that’s happening rather than kind of managing it. But also making sure that I carve out time to upskill others, because data science as a field, I mean, as you all know, is growing so much and people are coming in from different backgrounds. And I’m lucky enough to be able to kind of speak to a few people like that and do some very casual mentorship. And it makes me happy to see, so I hope that as my career develops, I will see more people maybe with backgrounds a little bit more like mine, come through and bring some diversity to the sector.\n\n\nNew developments or ideas you are most excited about?\nIt would be remiss of me to not mention like ChatGPT or generative AI, etc. But honestly, I am more interested in – or vaguely interested, I should say – in wearable technology. So, I’ve read a few very, very interesting papers and articles that talk about the development of wearable technology, not just your kind of watches, but potentially clothing, etc., that can be used for people with specific health problems to really help pinpoint, like, inflection points in time when something might happen. For example, a heart attack is about to happen, or is imminent, or is happening. So I actually feel like at the moment, this is perhaps going slightly under the radar, compared with more, you know, sexy developments like chatbots and things. But I’m very interested to see in the next one to two decades how ubiquitous wearables will be, and how closely entwined that will become with healthcare. So that’s something that I’m keeping half an eye on.\n\n\nAny words of advice for budding data scientists?\nYou will never stop learning at all because, frankly, the field is moving very, very quickly. So, even if you were to kind of consider yourself an absolute expert today, tomorrow that may not be the case. You will constantly be learning. And I have found that learning the same thing several times through different mediums and having things explained to you different ways is so valuable. Because you may think that you understand something from, say, your bootcamp, but then when you read about it as part of your degree – this is obviously personal to me – you read about it in a different way. And you think, Oh, I’ve never thought of it like that. And then you watch a YouTube video and someone visualises it and you think, okay, I understand this all a little bit deeper now. So, constantly revising what you do know and learning anything that’s new as it comes up, I think everyone at every stage of career can kind of, can do that.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘For me, data science is about bridging the gap between business requirements and the data that businesses have.’” Real World Data Science, May 24, 2023. URL"
  },
  {
    "objectID": "careers/posts/2023/05/18/chatgpt-data-science-pt2.html",
    "href": "careers/posts/2023/05/18/chatgpt-data-science-pt2.html",
    "title": "Large language models: Do we need to understand the maths, or simply recognise the limitations?",
    "section": "",
    "text": "Part 1 of our conversation with the Royal Statistical Society’s Data Science and AI (DS&AI) Section ended on a discussion around the need to verify that large language models (LLMs), when embedded in workflows and operational processes, are working as intended. But there was also acknowledgement that this could be difficult to achieve, not least of all because, as Giles Pavey said, “nobody knows exactly how these things work – not even the people who build them.” And then, of course, there is the speed at which developments are taking place: Trevor Duguid Farrant made the point that an expert may not even have a chance to finish reviewing the latest version of an LLM before a new iteration is rolled out.\nThese issues – of verification, explainability and interpretability – are of particular interest to data scientists like Anjali Mazumder, whose work focuses on the impact AI technologies could have, and are having, on society and individuals.\nIn part 2 of our Q&A about ChatGPT and other LLM-powered advances, and what all of this might mean for data science, Mazumder kicks off the conversation by setting out her perspective.\nOur full list of interviewees, in order of appearance, are:\n\nAnjali Mazumder, AI and Justice & Human Rights Theme Lead at the Alan Turing Institute, and DS&AI committee member.\nDetlef Nauck, head of AI and data science research at BT, and editorial board member, Real World Data Science.\nMartin Goodson, CEO and chief scientist at Evolution AI, and DS&AI committee member.\nLouisa Nolan, head of public health data science, Public Health Wales, and DS&AI secretary.\nPiers Stobbs, VP science at Deliveroo, and DS&AI committee member.\nTrevor Duguid Farrant, senior principal statistician at Mondelez International, and DS&AI committee member.\nGiles Pavey, global director for data science at Unilever, and DS&AI vice-chair.\nAdam Davison, head of data science at the Advertising Standards Authority, and DS&AI committee member.\n\n\n\n\nAnjali Mazumder: I work in research, but I also sit in the crux of government, industry, and civil society, looking at how they’re using these technologies. For me, it’s about knowing what the opportunities are but also understanding the limitations, the risks and the harms, and how we balance those and put in place oversight mechanisms that act as safeguards to ensure that these technologies don’t cause harm. We’re taking a very socio-technical approach that requires an interdisciplinary team to understand these issues and what should be done. Part of this is about not only the outcomes and the impact but also the upstream side of it – recognising that these models have been built on the work of people who have done the labelling, and that this also has implications – to say nothing of the associated environmental issues or energy issues!\nDetlef Nauck: I think the regulators really have to look at this. It has come completely out of left field for them. All the regulators that we are monitoring, they regulate the space as it was three years ago – they are mainly concerned about predictive models and bias. But if you look at, say, what Microsoft wants to do – putting GPT into Office 365 and into Bing – that will completely change how people interact with and consume information. I think the large tech companies really have a responsibility here, when they make this public, to make sure that people understand what this technology actually is, and how it can be used and has to be used.\nAlso, they need to open up about how these things have been built. There are a lot of stories around how OpenAI used cheap labour in order to do the labelling and reinforcement learning for ChatGPT, and these things have to become public knowledge; they need to become part of some kind of Kitemark for these models: “Ethically built, properly checked, hallucinate only a little bit. Whatever you do, don’t take it for granted. Check it!” That’s the kind of disclaimer they need to put on these models.\n\n\n\n\nIf you look at what Microsoft wants to do – putting GPT into Office 365 and into Bing – that will completely change how people interact with and consume information. Large tech companies really have a responsibility here, when they make this public, to make sure that people understand what this technology actually is.\n\n\n\nRegulatory principles always seem to stress that AI systems should be understandable, and we should be able to explain how we get particular outputs. But a lot of our conversation has focused on how we don’t really know how these models work. So, is that, in itself, a problem, and is it something that the data science community can help with – to dig into how these things work and try and get that across – to help meet these principles of explainability and interpretability?\nDN: That’s a very specialist job, I would say – specialist research into how these mathematical structures work. It’s not something I could do, and I’ve not seen any significant work there. One thing that we are interested in is whether we can do knowledge embedding, so that you can “teach” concepts that these models can then use to communicate, and that would lead to smaller systems where you have some understanding of what’s inside. But all of this kind of work, I think, is very much just beginning.\nMartin Goodson: Do we actually need this? There’s sort of a big assumption there that you need to understand how LLMs work in order to build in the kinds of things that we care about as a society. But we don’t understand how humans think. Of course, we can ask a human, “Why did you make that decision?” You can’t understand the cause of that decision – that’s a complex neuroscience question. But you can ask what the reason is for making a decision, and you can ask an LLM what its reasoning is as well. I think a lot of these questions about explainability are stuck in the past, when you’re trying to explain how a linear model works. It’s really not the same thing when you’ve got an LLM where you can just say, “Why did you make that decision?”\nLouisa Nolan: I was going to say something very similar. Most people don’t know how most things work…\nDN: My point was, these things are largely still like the Improbability Drive in the Hitchhiker’s Guide to the Galaxy. You press a button, and you don’t really know what comes out, and that’s the problem we need to get our heads around.\nLN: But people don’t know what percentages are, and yet we still use them for decision making. I don’t think people need to understand the maths behind LLMs, and I think it would be a hopeless job to expect everybody to do that. What we do need to understand is what LLMs can and can’t do. What’s the body of work that they are drawing from? What isn’t in there? What are the things that you need to check? So, for some things, it’ll be brilliant: if you’ve written something and you want it rewritten for a nine-year-old; if you want to summarise a paper; if you want to write code, as long as you already know how to code – these could be real labour-saving tasks. If you’re using ChatGPT to write a thesis about something that you haven’t looked at, that’s where the danger is. It’s this kind of simple understanding that people need to get in their heads – and the maths, except for the people who care about it, is beside the point, and probably detrimental, because it means that people won’t engage with it.\nDN: I agree, but I wasn’t talking about the general public. I meant, the people who build these things – they should know what they’re doing.\n\n\n\n\nThere’s a big assumption that you need to understand how LLMs work in order to build in the kinds of things that we care about as a society. But we don’t understand how humans think. You can ask a human what their reason is for making a decision, and you can ask an LLM what its reasoning is as well.\n\n\n\nWe talked there about communication. There was a webinar recently by the Royal Statistical Society’s Glasgow Local Group, and the presenter, Hannah Rose Kirk, showed how you can take tabular data and statistical results and ask ChatGPT to produce a nice paragraph or two that explains the numbers. Is this the sort of thing that any of you have experimented with? Have you had any successes at using ChatGPT to translate data into readable English that decision makers can act on?\nPiers Stobbs: I have an interesting use case. We had a basic survey: 200-odd responses, multiple languages, and we just said, “Please summarise the results of this survey contained in this CSV file.” And it came up with five or six relevant bullet points. What was amazing was that we could then interrogate it. For example, “Please compare the results that were in English versus in French, and describe the differences.” Again, it did it, but then you have the issue of, was it all correct? Well, the bulk of it was. Now I am intrigued by whether you can ask it to do correlations and some actual statistical things on a dataset, and does it get that right? I don’t know. We’ve not really got to that. But, to go back to one of the earlier discussion points around productivity, that initial survey work could have easily taken a week of someone’s time if we didn’t run it through ChatGPT.\nTrevor Duguid Farrant: Piers, in this case you’re interested in checking and seeing if it’s right. If you’d asked a group of people to do that survey for you and get the results, you’d have just accepted whatever they gave you back. You wouldn’t have questioned it.\nPS: That’s true. And the results were plausible, certainly.\nAM: I think one of the challenges is that the results could seem like they’re plausible, right – whether that’s a statistical output or a text output. This was not a proper experiment, but I asked ChatGPT about colleagues and friends who are quite prominent researchers, asking, “Who is so-and-so?”, and it produced biographies that were quite plausibly them, but it wasn’t them. It might have listed the correct PhD, say, but the date was off by a year, or the date was correct but it was from the wrong institution. So, depending on what the issue is, these seemingly plausible results could have more serious implications.\nLN: So, just to join those two things together: for me, the question is not, “Do we understand how ChatGPT works?” As Martin says, we don’t understand how humans work, and surely we’re trying to develop something that enhances human thinking in some way. The more important question for me is, “How do we know that what is produced is useful?”\n\n\n\n\nFor me, the question is not, ‘Do we understand how ChatGPT works?’ We don’t understand how humans work, and surely we’re trying to develop something that enhances human thinking in some way. The more important question is, ‘How do we know that what is produced is useful?’\n\n\n\nGiles, you mentioned previously that you’re doing some work at Unilever around how to minimise hallucination. I don’t know how much you can say on what direction that’s going in, and how successful you expect that to be, but that’s obviously going to be a really important part of refining these models to be more widely usable.\nGiles Pavey: I’m by no means an expert, but there’s quite a lot you can do with both the architecture of it and also the pre-prompts that you put in – more or less saying, “Quote what the source is, and if you’re not sure, then tell me you’re not sure.” I think what’s interesting is the question of whether we’ll have to rely on OpenAI or Microsoft to do that work, and it will be just another thing that we have to trust them for. Or, will it be something that people within an organisation can put in themselves?\nMG: I think it’s absolutely critical that open-source models are developed that can compete with these tech companies, otherwise there’s going to be a huge transfer of power to these companies.\nGP: Arguably, the single biggest issue is, who elected Sam Altman (no-one) and are we as society happy with him having so much power over our future?\nTo close us out, I’d like to return to a question Trevor posed earlier, which is: How might organisations like the Royal Statistical Society help companies to embrace LLMs and start using them, so that everyone can benefit from the technology?\nAdam Davison: My instinct is that there’s some great parallel here with the stuff that the Data Science and AI Section have been doing in general, where we’ve said, “OK, there’s lots of good advice out there on how to do things in data science, but how do you make it practical? How do you make it real? How do you apply those ethical principles? How do you make sure you have people with the right technical understanding in charge of projects to get value?” If, five years ago, the hype around data science was leading organisations to hire 100 data scientists in the hope that something innovative would happen, well then, we don’t want those same organisations now thinking that they need to hire 100 prompt engineers and keep their fingers crossed for something special. Our focus has been on “industrial strength data science”, so I think we can extend that to show what “industrial strength LLM usage” looks like in practice.\n\n\n\n\n\n\nWant to hear more from the RSS Data Science and AI Section? Sign up for its newsletter at rssdsaisection.substack.com.\n\n\n\n\n\n\n\n← Read part one\n\n\n\n\nBack to Careers\n\n\n\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence. Thumbnail image by Google DeepMind on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Large language models: Do we need to understand the maths, or simply recognise the limitations?” Real World Data Science, May 18, 2023. URL"
  },
  {
    "objectID": "about-rwds.html",
    "href": "about-rwds.html",
    "title": "Welcome to Real World Data Science",
    "section": "",
    "text": "Welcome to the home of Real World Data Science, a new project from the Royal Statistical Society, in partnership with the American Statistical Association. This site and its content are being developed by data science practitioners and leaders with a single goal in mind: to help you deliver high quality, ethical, impactful data science in your workplace."
  },
  {
    "objectID": "about-rwds.html#what-are-our-aims",
    "href": "about-rwds.html#what-are-our-aims",
    "title": "Welcome to Real World Data Science",
    "section": "What are our aims?",
    "text": "What are our aims?\nReal World Data Science aims to be a trusted, go-to source for high-quality, engaging and inspiring content which helps data science students, practitioners and leaders to:\n\ndiscover and learn more efficiently;\n\nacquire practical problem-solving skills;\n\nshare their knowledge and accomplishments publicly;\n\nwork smarter, ethically, and more effectively."
  },
  {
    "objectID": "about-rwds.html#what-we-provide",
    "href": "about-rwds.html#what-we-provide",
    "title": "Welcome to Real World Data Science",
    "section": "What we provide",
    "text": "What we provide\nResources are created to meet the needs of our target audiences. These include:\n\nCase studies – showing how data science is used to solve real-world problems in business, public policy and beyond.\nExplainers – interrogating the underlying assumptions and limitations of data science tools and methods, to help data scientists make smarter, more informed analytical choices.\nExercises – to challenge and develop the analytical mindset that all data scientists need to succeed.\n\nAdvice – interviews, Q&As, and FAQs on such topics as data science ethics, career paths, and communication, to support professional development.\n\nWe are also curating resources to help data scientists identify trustworthy, high-quality content. These include:\n\nTraining guides – step-by-step approaches and recommended sources for learning new skills and methods.\nDatasets – tagged and sorted to help educators and practitioners find data to meet their teaching and training needs.\nFeeds – who and what to follow to keep up with new ideas and developments."
  },
  {
    "objectID": "about-rwds.html#how-you-can-get-involved",
    "href": "about-rwds.html#how-you-can-get-involved",
    "title": "Welcome to Real World Data Science",
    "section": "How you can get involved",
    "text": "How you can get involved\nSee our open call for contributions."
  },
  {
    "objectID": "case-studies/posts/2023/11/16/learning-from-failure.html",
    "href": "case-studies/posts/2023/11/16/learning-from-failure.html",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "",
    "text": "Incarcerated youth are an exceptionally vulnerable population, and body-worn cameras are an important tool of accountability both for those incarcerated and the staff who supervise them. In 2018 the Texas Juvenile Justice Department (TJJD) deployed body-worn cameras for the first time, and this is a case study of how the agency developed a methodology for measuring the success of the camera rollout. This is also a case study of analysis failure, as it became clear that real-world implementation problems were corrupting the data and rendering the methodology unusable. However, the process of working through the causes of this failure helped the agency identify previously unrecognized problems and ultimately proved to be of great benefit. The purpose of this case study is to demonstrate how negative findings can still be incredibly useful in real-world settings."
  },
  {
    "objectID": "case-studies/posts/2023/11/16/learning-from-failure.html#background",
    "href": "case-studies/posts/2023/11/16/learning-from-failure.html#background",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Background",
    "text": "Background\nFrom the outset of the rollout of body-worn cameras, TJJD faced a major issue with implementation: in 2019, body worn cameras were an established tool for law enforcement, but there was very little literature or best practice to draw from for their use in a correctional environment. Unlike police officers, juvenile correctional officers (JCOs) deal directly with their charges for virtually their entire shift. In an eight-hour shift, a police officer might record a few calls and traffic stops. A juvenile correctional officer, on the other hand, would record for almost eight consecutive hours. And, because TJJD recorded round-the-clock for hundreds of employees at a time, this added up very quickly to a lot of footage.\nFor example, a typical dorm in a correctional center might have four JCOs assigned to it. Across a single week, these four JCOs would be expected to record at least 160 hours of footage.\n\n\n\n\n\n\nFigure 1: Four JCOs x 40 hours per week = 160 hours of footage.\n\nThis was replicated across every dorm. Three dorms, for example, would produce nearly 500 hours of footage, as seen below.\n\n\n\n\n\n\nFigure 2: Three dorms x four JCOs x 40 hours per week = 480 hours of footage.\n\nFinally, we had more than one facility. Four facilities with three dorms each would produce nearly 2,000 hours of footage every week.\n\n\n\n\n\n\nFigure 3: Four facilities x three dorms x four JCOs x 40 hours per week = 1,960 hours of footage.\n\nIn actuality, we had a total of five facilities each with over a dozen dorms producing an anticipated 17,000 hours of footage every week – an impossible amount to monitor manually.\nAs a result, footage review had to be done in a limited, reactive manner. If our monitoring team received an incident report, they could easily zero in on the cameras of the officers involved and review the incident accordingly. But our executive team had hoped to be able to use the footage proactively, looking for “red flags” in order to prevent potential abuses instead of only responding to allegations.\nBecause the agency had no way of automating the monitoring of footage, any proactive analysis had to be metadata-based. But what to look for in the metadata? Once again, the lack of best-practice literature left us in the lurch. So, we brainstormed ideas for “red flags” and came up with the following that could be screened for using camera metadata:\n\nMinimal quantity of footage – our camera policy required correctional officers to have their cameras on at all times in the presence of youth. No footage meant they weren’t using their cameras.\nFrequently turning the camera on and off – a correctional officer working a dorm should have their cameras always on when around youth and not be turning them on and off repeatedly.\nLarge gaps between clips – it defeats the purpose of having cameras if they’re not turned on.\n\nIn addition, we came up with a fourth red flag, which could be screened for by comparing camera metadata with shift-tracking metadata:\n\nMismatch between clips recorded and shifts worked – the agency had very recently rolled out a new shift tracking software. We should expect to see the hours logged by the body cameras roughly match the shift hours worked."
  },
  {
    "objectID": "case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-1-quality-control-and-footage-analysis",
    "href": "case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-1-quality-control-and-footage-analysis",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Analysis, part 1: Quality control and footage analysis",
    "text": "Analysis, part 1: Quality control and footage analysis\nFor this analysis, I gathered the most recent three weeks of body-worn camera data – which, at the time, covered April 1–21, 2019. I also pulled data from Shifthound (our shift management software) covering the same time period. Finally, I gathered HR data from CAPPS, the system that most of the State of Texas used at the time for personnel management and finance.1 I then performed some quality control work, summarized in the dropdown box below.\n\n\n\n\n\n\nInitial quality control steps\n\n\n\n\n\nSkimR is a helpful R package for exploratory analysis that gives summary statistics for every variable in a data frame, including missing values. After using the skim function on clip data, shift data, and HR data, I noticed that the clip data had some missing values for employee ID. This was an error which pointed to data entry mistakes – body-worn cameras do not record footage on their own, after all, so employee IDs should be assigned to each clip.\nFrom here I compared the employee ID field in the clip data to the employee ID field in the HR data. Somewhat surprisingly, IDs existed in the clip data that did not correspond to any entries in the HR data, indicating yet more data entry mistakes – the HR data is the ground truth for all employee IDs. I checked the shift data for the same error – employee IDs that did not exist in the HR data – and found the same problem.\nAs well as employee IDs that did not exist in the HR data, I also looked for employee IDs in the footage and shift data which related to staff who were not actually employed between April 1–21, 2019. I found some examples of this, which indicated yet more errors: staff cannot use a body-worn camera or log a shift if they have yet to begin working or if they have been terminated (system permissions are revoked upon leaving employment).\nI made a list of every erroneous ID to pass off to HR and monitoring staff before excluding them from the subsequent analysis. In total, 10.6% of clips representing 11.3% of total footage had to be excluded due to these initial data quality issues, foreshadowing the subsequent data quality issues the analysis would uncover.\nThe full analysis script can be found on GitHub.\n\n\n\nIn order to operationalize the “red flags” from our brainstorming session, I needed to see what exactly the cameras captured in their metadata. The variables most relevant to our purposes were:\n\nClip start\nClip end\nCamera used\nWho was assigned to the camera at the time\nThe role of the person assigned to the camera\n\nUsing these fields, I first created the following aggregations per employee ID:\n\n\n\n\n\n\nNumber of clips = Number of clips recorded.\n\n\n\n\n\n\n\nDays with footage = Number of discrete dates that appear in these clips.\n\n\n\n\n\n\n\n\n\nFootage hours = Total duration of all shot footage.\n\n\n\n\n\n\n\nSignificant gaps = Number of clips where the previous clip’s end date was either greater than 15 minutes or less than eight hours before current clip’s start date.\n\n\n\n\n\nI used these aggregations to devise the following staff metrics:\n\n\n\n\n\n\nClips per day = Number of clips / Days with footage.\n\n\n\n\n\n\n\nFootage per day = Footage hours / Days with footage.\n\n\n\n\n\n\n\n\n\nAverage clip length = Footage hours / Number of clips.\n\n\n\n\n\n\n\nGaps per day = Gaps / Days with footage.\n\n\n\n\n\nOnce I established these metrics for each employee I looked at their respective distributions. Standard staff shift lengths at the time were eight hours. If staff were using their cameras appropriately, we would expect to see distributions centered around clip lengths of about an hour, eight or fewer clips per day, and 8-12 footage hours per day. We would also expect to see 0 large gaps.\n\n\nShow the code\n\n```{r}\nlibrary(tidyverse)\n\nFootage_Metrics_by_Employee &lt;- read_csv(\"Output/Footage Metrics by Employee.csv\")\n\nFootage_Metrics_by_Employee %&gt;% \n  select(-Clips, -Days_With_Footage, -Footage_Hours, -Gaps) %&gt;% \n  pivot_longer(-Employee_ID, names_to = \"Metric\", values_to = \"Value\") %&gt;% \n  ggplot(aes(x = Value)) +\n  geom_histogram() +\n  facet_wrap(~Metric, scales = \"free\")\n```\n\n\n\n\n\n\nBy eyeballing the distributions I could tell most staff were recording fewer than 10 clips per day, shooting about 0.5–2 hours for each clip, for a total of 2–10 hours of daily footage, with the majority of employees having less than one significant gap per day. Superficially, this appeared to provide evidence of widespread attempts at complying with the body-worn camera policy and no systemic rejection or resistance. If this were indeed the case, then we could turn our attention to individual outliers.\nFirst, though, we thought we would attempt to validate this initial impression by testing another assumption. If each employee works on average 40 hours per week – a substantial underestimate given how common overtime was – we should expect, over a three-week period, to see about 120 hours of footage per employee in the dataset. This is not what we found.\nAverage footage per employee was 70.2 hours over the three-week period, meaning that the average employee was recording less than 60% of shift hours worked. With so many hours going unrecorded for unknown reasons, we needed to investigate further.\nSurely the shift data would clarify this…"
  },
  {
    "objectID": "case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-2-footage-and-shift-comparison",
    "href": "case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-2-footage-and-shift-comparison",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Analysis, part 2: Footage and shift comparison",
    "text": "Analysis, part 2: Footage and shift comparison\nWith the data on shifts worked from our timekeeping system, I could theoretically compare actual shifts worked to the amount of footage recorded. If there were patterns in where the gaps in footage fell, that comparison might help to explain why.\nIn order to join the shift data to the camera data, I needed a common unit of analysis beyond “Employee ID.” Using only this value would produce a nonsensical table that joined up every clip of footage to every shift worked.\nFor example, let’s take employee #9001005 at Facility Epsilon between April 1–3. This employee has the following clips recorded during that time period:\n\n\n\n\nEmployee_ID\nClip_ID\nClip_Start\nClip_End\n\n\n\n\n9001005\n156421\n2019-04-01 05:54:34\n2019-04-01 08:34:34\n\n\n9001005\n155093\n2019-04-01 08:40:59\n2019-04-01 08:54:51\n\n\n9001005\n151419\n2019-04-01 09:03:16\n2019-04-01 11:00:30\n\n\n9001005\n153133\n2019-04-01 11:10:09\n2019-04-01 12:39:51\n\n\n9001005\n151088\n2019-04-01 12:57:51\n2019-04-01 14:06:44\n\n\n9001005\n150947\n2019-04-02 05:56:34\n2019-04-02 09:48:50\n\n\n9001005\n151699\n2019-04-02 09:54:23\n2019-04-02 12:17:15\n\n\n\n\nWe can join this to a similar table of shifts logged. This particular employee had the following shifts scheduled from April 1–3:\n\n\n\n\nEmployee_ID\nShift_ID\nShift_Start\nShift_End\n\n\n\n\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n\n\n9001005\nE051303\n2019-04-02 06:00:00\n2019-04-02 14:00:00\n\n\n\n\nThe table shows two eight-hour morning shifts from 6:00 am to 2:00 pm. We can join the two tables together by ID on a messy many-to-many join, but that tells us nothing about how much they overlap (or fail to overlap) without extensive additional work. For example, we have a unique identifier for employee clip (Clip_ID) and employee shift (Shift_ID), but what we need is a unique identifier that can be used to join the two. Fortunately, for this particular data we can create a unique identifier since both clips and shifts are fundamentally measures of time. While Employee_ID is not in itself unique (i.e., one employee can have multiple clips attached to that ID), Employee_ID combined with time of day is unique. A person can only be in one place at a time, after all!\nTo reshape the data for joining, I created a function that takes any data frame with a start and end column and unfolds it into discrete units of time. Using the code below to create the “Interval_Convert” function, the shift data above for employee 9001005 converts into one entry per hour of the day per shift. As a result, two eight-hour shifts get turned into 16 employee hours (a sample of which is shown below).\n\n\nShow the code\n\n```{r}\nlibrary(sqldf)\nlibrary(lubridate)\n\nInterval_Convert &lt;- function(DF, Start_Col, End_Col, Int_Unit, Int_Length = 1) {\nbrowser()\n  Start_Col2 &lt;- enquo(Start_Col)\n  End_Col2 &lt;- enquo(End_Col)\n  \n  Start_End &lt;- DF %&gt;%\n    ungroup() %&gt;%\n    summarize(Min_Start = min(!!Start_Col2),\n              Max_End = max(!!End_Col2)) %&gt;%\n    mutate(Start = floor_date(Min_Start, Int_Unit),\n           End = ceiling_date(Max_End, Int_Unit))\n  \n  DF &lt;- DF %&gt;%\n    mutate(Single = !!Start_Col2 == !!End_Col2)\n  \n  Interval_Table &lt;- data.frame(Interval_Start = seq.POSIXt(Start_End$Start[1], Start_End$End[1], by = str_c(Int_Length, \" \", Int_Unit))) %&gt;%\n    mutate(Interval_End = lead(Interval_Start)) %&gt;%\n    filter(!is.na(Interval_End))\n  \n  by &lt;- join_by(Interval_Start &lt;= !!End_Col2, Interval_End &gt;= !!Start_Col2)  \n  \n  Interval_Data_Table &lt;- Interval_Table %&gt;% \n    left_join(DF, by) %&gt;% \n    mutate(Seconds_Duration_Within_Interval = if_else(!!End_Col2 &gt; Interval_End, Interval_End, !!End_Col2) -\n             if_else(!!Start_Col2 &lt; Interval_Start, Interval_Start, !!Start_Col2)) %&gt;%\n    filter(!(Single & Interval_End == !!Start_Col2),\n           as.numeric(Seconds_Duration_Within_Interval) &gt; 0)\n  \n  return(Interval_Data_Table)\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterval_Start\nInterval_End\nEmployee_ID\nShift_ID\nShift_Start\nShift_End\nSeconds_Duration_Within_Interval\n\n\n\n\n2019-04-01 06:00:00\n2019-04-01 07:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 07:00:00\n2019-04-01 08:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 08:00:00\n2019-04-01 09:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 09:00:00\n2019-04-01 10:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\nThe footage could be converted in a similar manner, and in this way I could break down both the shift data and the clip data into an hour-by-hour view and compare them to one another. Using this new format, I joined together the full tables of footage and shifts to determine how much footage was recorded with no corresponding shift in the timekeeping system.\n\n\n\n\n\n\n\n\n\nHR_Location\nFootage_Hours_No_Shift\nEmployee_IDs_With_Missing_Shift\n\n\n\n\nAlpha\n1805\n122\n\n\nBeta\n3749\n114\n\n\nDelta\n1208\n133\n\n\nEpsilon\n2899\n157\n\n\nGamma\n4153\n170\n\n\n\n\nTo summarize what the table is telling us: Almost every employee has footage hours that do not match with logged shifts, totaling nearly 14,000 hours when you add up the Footage_Hours_No_Shift column. But what about the opposite case? How many shift hours were logged with no corresponding footage?\n\n\n\n\n\n\n\n\n\nHR_Location\nShift_Hours_No_Footage\nEmployee_IDs_With_Missing_Footage\n\n\n\n\nAlpha\n7338\n127\n\n\nBeta\n6014\n118\n\n\nDelta\n12830\n141\n\n\nEpsilon\n9000\n168\n\n\nGamma\n11960\n183\n\n\n\n\nOh dear. Again, almost every employee has logged shift hours with no footage: 47,000 hours in total. To put it another way, that’s an entire work week per employee not showing up in camera footage.\nAt this point, we could probably rule out deliberate noncompliance. The clip data already implied that most employees were following the policy, and our facility leadership would surely have noticed a mass refusal large enough to show up this clearly in the data.\nOne way to check for deliberate noncompliance would be to first exclude shifts that contain zero footage whatsoever. This would rule out total mismatches, where – for whatever reason – the logged shifts had totally failed to overlap with recorded clips. For the remaining shifts that do contain footage, we could look at the proportion of the shift covered by footage. So, if an eight-hour shift had four hours of recorded footage associated with it, then we could say that 50% of the shift had been recorded. The following histogram is a distribution of the number of employees organized by the percent of their shift-hours they recorded (but only shifts that had a nonzero amount of footage).\n\n\n\n\n\nAs it turned out, most employees recorded the majority of their matching shifts, a finding that roughly aligns with the initial clip analysis. So, what explains the 14,000 hours of footage with no shifts, and the 47,000 hours of shifts with no footage?"
  },
  {
    "objectID": "case-studies/posts/2023/11/16/learning-from-failure.html#causes-of-failure",
    "href": "case-studies/posts/2023/11/16/learning-from-failure.html#causes-of-failure",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Causes of failure",
    "text": "Causes of failure\nHere, I believed, we had reached the end of what I could do with data alone, and so I presented these findings (or lack thereof) to executive leadership. The failure to gather reliable data from linking the clip data to the shift data prompted follow-ups into what exactly was going wrong. As it turned out, many things were going wrong.\nFirst, a number of technical problems plagued the early rollout of the cameras:\n\nAll of our facilities suffered from high turnover, and camera ownership was not consistently updated. Employees who no longer worked at the agency could therefore appear in the clip data – somebody else had taken over their camera but had not put their name and ID on it.\nWe had no way of telling if a camera was not recording due to being docked and recharging or not recording due to being switched off.\nIn the early days of the rollout, footage got assigned to an employee based on the owner of the dock, not the camera. In other words, if Employee A had recorded their shift with their camera but uploaded the footage using a dock assigned to Employee B then the footage would show up in the system as belonging to Employee B.\n\nThe shift data was, unsurprisingly, even worse, and it was here we came across our most important finding. While the evidence showed that there wasn’t any widespread non-compliance with the use of the cameras, there was widespread non-compliance with the use of our shift management software. Details are included in the dropdown box below.\n\n\n\n\n\n\nQuality issues in shift tracking data\n\n\n\n\n\nOur HR system, CAPPS, had a feature that tracked hours worked in order to calculate leave and overtime pay. However, CAPPS was a statewide application designed for 9–5 office workers, and could not capture the irregular working hours of our staff (much less aid in planning future shifts). We had obtained separate shift management software to fill these gaps, but not realized how inconsistently it was being used. All facilities were required to have their employees log their shifts, but some followed through on this better than others. And even for those that did make a good-faith effort at follow-through, quality control was nonexistent.\nIn CAPPS, time entry determined pay, so strong incentives existed to ensure accurate entry. But for our shift management software, no incentives existed at all for making sure that entries were correct. For example, a correctional officer could have a 40-hour work week scheduled in the shift software but miss the entire week due to an injury, and the software would still show them as having worked 40 hours that week. Nobody bothered to go back and correct these types of errors because there was no reason to.\nThe software was intended to be used proactively for planning purposes, not after-the-fact for logging and tracking purposes. Thus, it produced data that was totally inconsistent with actual hours worked, which became apparent when compared to data (like body-worn camera footage) that tracked actual hours on the floor.\nIn the end, we had to rethink a number of aspects of the shift software’s implementation. In the process of these fixes, leadership also came to make explicit that the software’s primary purpose was to help facilities schedule future shifts, not audit hours worked after the fact (which CAPPS already did, just on a day-by-day basis as opposed to an hour-by-hour basis). This analysis was the only time we attempted to use the shift data in this manner."
  },
  {
    "objectID": "case-studies/posts/2023/11/16/learning-from-failure.html#what-we-learned-from-failure",
    "href": "case-studies/posts/2023/11/16/learning-from-failure.html#what-we-learned-from-failure",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "What we learned from failure",
    "text": "What we learned from failure\nWhatever means we used to monitor compliance with the camera policy, we learned that it couldn’t be fully automated. The agency followed up this analysis with a random sampling approach, in which monitors would randomly select times of day they knew a given staff member would have to have their cameras turned on and actually watch the associated clips. This review process confirmed the first impressions from the statistical review above: most employees were making good faith attempts at complying with the policy despite technical glitches, short-staffing, and administrative confusion. It also confirmed that proactive monitoring of correctional officers was a human process which had to come from supervisors and staff.\nThe one piece of the analysis we did use going forward was the clip analysis (converted into a Power BI dashboard and included in the GitHub repository for this article), but only as a supplement for already-launched investigations, not a prompt for one. Body-worn camera footage remained immensely useful for investigations after-the-fact, but inconsistencies in clip data were not, in and of themselves, particularly noteworthy “red flags.” At the end of the day, analytics can contextualize and enhance human judgment, but it cannot replace it.\nIn academia, the bias in favor of positive findings is well-documented. The failure to find something, or a lack of statistical significance, does not lend itself to publication in the same way that a novel discovery does. But, in an applied setting, where results matter more than publication criteria, negative findings can be highly insightful. They can falsify erroneous assumptions, bring unknown problems to light, and prompt the creation of new processes and tools. In this context, a failure is only truly a failure if nothing is learned from it.\n\nFind more case studies\n\n\n\n\n\nAbout the author\n\nNoah Wright is a data scientist with the Texas Juvenile Justice Department. He is interested in the applications of data science to public policy in the context of real-world constraints, and the ethics thereof (ethics being highly relevant in his line of work). He can be reached on LinkedIn.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Noah Wright\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nWright, Noah. 2023. “Learning from failure: ‘Red flags’ in body-worn camera data.” Real World Data Science, November 16, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/11/16/learning-from-failure.html#footnotes",
    "href": "case-studies/posts/2023/11/16/learning-from-failure.html#footnotes",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe underlying data for the analysis as presented in this article was requested through the Texas Public Information Act and went through TJJD’s approval process for ensuring anonymity of records. It is available on GitHub along with the rest of the code used to write this article.↩︎"
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "",
    "text": "Reproducibility, or “the ability of a researcher to duplicate the results of a prior study using the same materials as the original investigator”, is critical for sharing and building upon scientific findings. Reproducibility not only verifies the correctness of processes leading to results but also serves as a prerequisite for assessing generalisability to other datasets or contexts. This we refer to as replicability, or “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”. Reproducibility, which is the focus of our work here, can be challenging – especially in the context of deep learning. This article, and associated material, aims to provide practical advice for overcoming these challenges.\nOur story begins with Davit Svanidze, a master’s degree student in economics at the London School of Economics (LSE). Davit’s efforts to make his bachelor’s thesis reproducible are what inspires this article, and we hope that readers will be able to learn from Davit’s experience and apply those learnings to their own work. Davit will demonstrate the use of Jupyter notebooks, GitHub, and other relevant tools to ensure reproducibility. He will walk us through code documentation, data management, and version control with Git. And, he will share best practices for collaboration, peer review, and dissemination of results.\nDavit’s story starts here, but there is much more for the interested reader to discover. At certain points in this article, we will direct readers to other resources, namely a Jupyter notebook and GitHub repository which contain all the instructions, data and code necessary to reproduce Davit’s research. Together, these components offer a comprehensive overview of the thought process and technical implementation required for reproducibility. While there is no one-size-fits-all approach, the principles remain consistent."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Davit’s journey towards reproducibility",
    "text": "Davit’s journey towards reproducibility\n\nMore power, please\nThe focus of my bachelor’s thesis was to better understand the initial spread of Covid-19 in China using deep learning algorithms. I was keen to make my work reproducible, but not only for my own sake. The “reproducibility crisis” is a well-documented problem in science as a whole,1 2 3 4 with studies suggesting that around one-third of social science studies published between the years 2010 and 2015 in top journals like Nature and Science could not be reproduced.5 Results that cannot be reproduced are not necessarily “wrong”. But, if findings cannot be reproduced, we cannot be sure of their validity.\nFor my own research project, I gathered all data and started working on my computer. After I built the algorithms to train the data, my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long, and I needed a faster, more powerful solution to be able to submit my thesis in time. Fortunately, I could access the university server to train the algorithms. Once the training was complete, I could generate the results on my local computer, since producing maps and tables was not so demanding. However…\n\n\nBloody paths!\nIn switching between machines and computing environments, I soon encountered an issue with my code: the paths, or file directory locations, for the trained algorithms had been hardcoded! As I quickly discovered, hardcoding a path can lead to issues when the code is run in a different environment, as the path might not exist in the new environment.\nAs my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake – which would have been easily corrected if spotted earlier – resulted in incorrect outputs. Such errors could have enormous (negative) implications in a public health context, where evidence-based decisions have real impacts on human lives. It was at this point that I realised that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if they are not able to verify it?\nThe following dummy code demonstrates the hardcoding issue:\n```{python}\n# Hardcoded path\nfile_path = \"/user/notebooks/toydata.csv\"\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nIn the code above, a dummy file (toydata.csv) is used. The dummy file contains data on the prices of three different toys, but only the path of the file is relevant to this example. If the hardcoded file path – \"/user/notebooks/toydata.csv\" – exists on the machine being used, the code will run just fine. But, when run in a different environment without said path, the code will result in a \"File not found error\". Better code that uses relative paths can be written as:\n```{python}\n# Relative path\nimport os\n\nfile_path = os.path.join(os.getcwd(), \"toydata.csv\")\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nYou can see that this code has successfully imported data from the dataset toydata.csv and printed its two columns (toy and price) and three rows.\nThe following example is a simplified version of what happened when I wrote code to train several models, store the results and run a procedure to compare results with the predictive performance of a benchmark model:\n```{python}\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)\n```\n```{python}\n# Load the model result and compare with benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model\nresult = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nEverything looks fine at a glance. But, if you examine the code carefully, you may spot the problem. Initially, when I coded the procedure (training the model, saving and loading the results), I hardcoded the paths and had to change them for each tested model. First, I trained model2, a complex model, and tested it against the benchmark (70 &gt; 50 → accepted). I repeated the procedure for model1 (a simple model). Its result was identical to model2, therefore I kept model1 following the parsimony principle.\nHowever, for the code line loading the result for the current model (line 5, second cell), I forgot to amend the path and so mistakenly loaded the result of model2. As a consequence, I accepted a model which should have been rejected. These wrong results were then spread further in the code, including all charts and maps and the conclusions of my analysis.\nA small coding error like this can therefore be fatal to an analysis. Below is the corrected code:\n```{python}\nimport os\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details (INCLUDING PATHS) in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": os.path.join(os.getcwd(), \"results-of-model1.csv\")}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": os.path.join(os.getcwd(), \"results-of-model2.csv\")}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv(current_model[\"path\"], index=False)\n```\n```{python}\n# Get the model result and compare with the benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model WITH a VARIABLE PATH\nresult = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nHere, the paths are stored with other model details (line 7–8, first cell). Therefore, we can use them as variables when we need them (e.g., line 16, first cell, and line 5, second cell). Now, when the current model is set to model1 (line 11, first cell), everything is automatically adjusted. Also, if the path details need to be changed, we only need to change them once and everything else is automatically adjusted and updated. The code now correctly states that model1 performs worse than the benchmark and is therefore rejected and we should keep model2, which performs best.\nI managed to catch this error in time, but it often can be difficult to spot our own mistakes. That is why making code available to others is crucial. A code review by a second (or third) pair of eyes can save everyone a lot of time and avoid spreading incorrect results and conclusions.\n\n\nSolving compatibility chaos with Docker\nOne might think that it would be easy to copy code from one computer to another and run it without difficulties, but it turns out to be a real headache. Different operating systems on my local computer and the university server caused multiple compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, the server did not support the Python programming language – and all the deep learning algorithm packages that I needed – in the same way as my macOS computer did.\nAs a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed. This way, I could integrate them with different hardware and use the processing power of that hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. In fact, I found the Docker website very helpful, with lots of resources and tutorials available. Once Docker was installed, it was easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they could also make it easier for others to reproduce my work.\nBelow is an example of a Dockerfile which recreates an environment with Python 3.7 on Linux. It describes what, how, when and in which order operations should be carried out to generate the environment with all Python packages required to run the main Python script, main.py.\n\n\nAn example of a Dockerfile.\n\nIn this example, by downloading the project, including the Dockerfile, anyone can run main.py without installing packages or worrying about what OS was used for development or which Python version should be installed. You can view Docker as a great robot chef: show it a recipe (Dockerfile), provide the ingredients (project files), push the start button (to build the container) and wait to sample the results.\n\n\nWhy does nobody check your code?\nEven after implementing Docker, I still faced another challenge to reproducibility: making the verification process for my code easy enough that it could be done by anyone, without them needing a degree in computer science! Increasingly, there is an expectation for researchers to share their code so that results can be reproduced, but there are as yet no widely accepted or enforced standards on how to make code readable and reusable. However, if we are to embrace the concept of reproducibility, we must write and publish code under the assumption that someone, somewhere – boss, team member, journal reviewer, reader – will want to rerun our code. And, if we expect that someone will want to rerun our code (and hopefully check it), we should ensure that the code is readable and does not take too long to run.\nIf your code does take too long to run, some operations can often be accelerated – for example, by reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using PyTorch). Aim for a running time of a few minutes – or about as long as it takes to make a cup of tea or coffee. Of course, if data needs to be reduced to save computational time, the person rerunning your code won’t generate the same results as in your original analysis. This therefore will not lead to reproducibility, sensu stricto. However, as long as you state clearly what are the expected results from the reduced dataset, your peers can at least inspect your code and offer feedback, and this marks a step towards reproducibility.\nWe should also make sure our code is free from bugs – both the kind that might lead to errors in analysis and also those that stop the code running to completion. Bugs can occur for various reasons. For example, some code chunks written on a Windows machine may not properly execute on a macOS machine because the former uses \\ for file paths, while the latter uses /:\n```{python}\n# Path works on macOS/Linux\nwith open(\"../../all/notebooks/toydata.csv\", \"r\") as f:\n    print(f.read())\n\n# Path works only on Windows    \nwith open(r\"..\\..\\all\\notebooks\\toydata.csv\", \"r\") as f:\n   print(f.read())\n```\n\nHere, only the macOS/Linux version works, since the code this capture was taken from was implemented on a Linux server. There are alternatives, however. The code below works on macOS, Linux, and also Windows machines:\n```{python}\nfrom pathlib import Path\n\n# Path works on every OS: macOS/Linux/Windows\n# It will automatically replace the path to \"..\\..\\all\\notebooks\\toydata.csv\" when it runs on Windows\nwith open(Path(\"../../all/notebooks/toydata.csv\"), \"r\") as f:\n    print(f.read())\n```\n\nThe extra Python package, pathlib, is of course unnecessary if you build a Docker container for your project, as discussed in the previous section.\n\n\nJupyter, King of the Notebooks\nBy this stage in my project, I was feeling that I’d made good progress towards ensuring that my work would be reproducible. I’d expended a lot of effort to make my code readable, efficient, and also absent of bugs (or, at least, this is what I was hoping for). I’d also built a Docker container to allow others to replicate my computing environment and rerun the analysis. Still, I wanted to make sure there were no barriers that would prevent people – my supervisors, in particular – from being able to review the work I had done for my undergraduate thesis. What I wanted was a way to present a complete narrative of my project that was easy to understand and follow. For this, I turned to Jupyter Notebook.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nJupyter notebooks combine Markdown text, code, and visualisations. The notebook itself can sit within an online directory of folders and files that contain all the data and code related to a project, allowing readers to understand the processes behind the work and also access the raw resources. From the notebook I produced, readers can see exactly what I did, how I did it, and what my results were.\nWhile creating my notebook, I was able to experiment with my code and iterate quickly. Code cells within a document can be run interactively, which allowed me to try out different approaches to solving a problem and see the results almost in real time. I could also get feedback from others and try out new ideas without having to spend a lot of time writing and debugging code.\n\n\nVersion control with Git and GitHub\nMy Jupyter notebook and associated folders and files are all available via GitHub. Git is a version control system that allows you to keep track of changes to your code over time, while GitHub is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code and collaborate with others without the risk of losing any work. I really couldn’t afford to redo the entire year I spent on my dissertation!\nGit and GitHub are great for reproducibility. By sharing code via these platforms, others can access your work, verify it and reproduce your results without risking changing or, worse, destroying your work – whether partially or completely. These tools also make it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easy to keep track of the different versions of your code and to see how your work has evolved.\nThe following illustration shows the tracking of very simple changes in a Python file. The previous version of the code is shown on the left; the new version is shown on the right. Additions and deletions are highlighted in green and red, and with + and - symbols, respectively.\n\n\nA simple example of GitHub version tracking."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "The deep learning challenge",
    "text": "The deep learning challenge\nSo far, this article has dealt with barriers to reproducibility – and ways around them – that will apply to most, if not all, modern research projects. While I’d encourage any scientist to adopt these practices in their own work, it is important to stress that these alone cannot guarantee reproducibility. In cases where standard statistical procedures are used within statistical software packages, reproducibility is often achievable. However, in reality, even when following the same procedures, differences in outputs can occur, and identifying the reasons for this may be challenging. Cooking offers a simple analogy: subtle changes in room temperature or ingredient quality from one day to the next can impact the final product.\nOne of the challenges for research projects employing machine learning and deep learning algorithms is that outputs can be influenced by the randomness that is inherent in these approaches. Consider the four portraits below, generated by the Midjourney bot.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nEach portrait looks broadly similar at first glance. However, upon closer inspection, critical differences emerge. These differences arise because deep learning models rely on numerous interconnected layers to learn intricate patterns and representations. Slight random perturbations, such as initial parameter values or changes in data samples, can propagate through the network, leading to different decisions during the learning process. As a result, even seemingly negligible randomness can amplify and manifest as considerable differences in the final output, as with the distinct features of the portraits.\nRandomness is not necessarily a bad thing – it mitigates overfitting and helps predictions to be generalised. However, it does present an additional barrier to reproducibility. If you cannot get the same results using the same raw materials – data, code, packages and computing environment – then you might have good reasons to doubt the validity of the findings.\nThere are many elements of an analysis in which randomness may be present and lead to different results. For example, in a classification (where your dependent variable is binary, e.g., success/failure with 1 and 0) or a regression (where your dependent variable is continuous, e.g., temperature measurements of 10.1°C, 2.8°C, etc.), you might need to split your data into training and testing sets. The training set is used to estimate the model (hyper)parameters and the testing set is used to compute the performance of the model. The way the split is usually operationalised is as a random selection of rows of your data. So, in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may therefore lead to different values of the model (hyper)parameters and affect the predictive performance that is measured from the testing set. Also, differences in the testing set may lead to variations in the predictive performance scores, which in turn lead to potentially different interpretations and, ultimately, decisions if the results are used for that purpose.\nThis aspect of randomness in the training of models is relatively well known. But randomness may hide in other parts of code. One such example is illustrated below. Here, using Python, we set the seed number to 0 using np.random.seed(seed value). The random.seed() function from the package numpy (abbreviated np) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. A seed value is an initial input or starting point used by a pseudorandom number generator to generate a sequence of random numbers. It is often an integer or a timestamp. The number generator takes this seed value and uses it to produce a deterministic series of random numbers that appear to be random but can be recreated by using the same seed value. Without providing this seed value, the first execution of the function typically uses the current system time. The animation below generates two random arrays arr1 and arr2 using np.random.rand(3,2). Note that the values 3,2 indicate that we want random values for an array that has 3 rows and 2 columns.\n```{python}\nimport numpy as np\n\n#Set the seed number e.g. to 0\nnp.random.seed(0)\n# Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Set the seed number as before to get the same results\nnp.random.seed(0)\n# Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nIf you run the code yourself multiple times, the values of arr1 and arr2 should remain identical. If this is not the case, check that the seed value is set to 0 in lines 4 and 11. These identical results are possible because we set the seed value to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let’s look at what happens if we remove the line np.random.seed(0):\n```{python}\n#Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nHere, the values of arr1 and arr2 will be different each time we run the code since the seed value was not set and is therefore changing over time.\nThis short code demonstrates how randomness that can be controlled by the seed value may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the seed value will contribute to making your work reproducible. I also find it helpful to document the seed number I use in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see code chunk 9 in the Jupyter notebook) we set the seed value in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Conclusion",
    "text": "Conclusion\nWe hope you have enjoyed learning more about our quest for reproducibility. We have explained why reproducibility matters and provided tips for how to achieve it – or, at least, work towards it. We have introduced a few important issues that you are likely to encounter on your own path to reproducibility. In sum, we have mentioned:\n\nThe importance of having relative instead of hard-coded paths in code.\nOperating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment.\nThe convenience of Jupyter notebooks for code editing – particularly useful for data science projects and work using deep learning because of the ability to include text and code in the same document and make the work accessible to everyone (so long as they have an internet connection).\nThe need for version control using, for example, Git and GitHub, which allows you to keep track of changes in your code and collaborate with others efficiently.\nThe importance of setting the seed values in random number generators.\n\nThe graphic below provides a visual overview of the different components of our study and shows how each component works with the others to support reproducibility.\n\n\n\n\n\nWe use (A) the version control system, Git, and its hosting service, GitHub, which enables a team to share code with peers, efficiently track and synchronise code changes between local and server machines, and reset the project to a working state in case something breaks. Docker containers (B) include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows).\nNow, it is over to you. Our Jupyter notebook provides a walkthrough of our research. Our GitHub repository has all the data, code and other files you need to reproduce our work, and this README file will help you get started.\nAnd with that, we wish you all the best on the road to reproducibility!\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nDavit Svanidze is a master’s degree student in economics at the London School of Economics (LSE). Andre Python is a young professor of statistics at Zhejiang University’s Center for Data Science. Christoph Weisser is a senior data scientist at BASF. Benjamin Säfken is professor of statistics at TU Clausthal. Thomas Kneib is professor of statistics and dean of research at the Faculty of Business and Economic Sciences at Goettingen University. Junfen Fu is professor of pediatrics, chief physician and director of the Endocrinology Department of Children’s Hospital, Zhejiang University, School of Medicine.\n\n\n\n\n\nAcknowledgement\n\nAndre Python has been funded by the National Natural Science Foundation of China (82273731), the National Key Research and Development Program of China (2021YFC2701905) and Zhejiang University global partnership fund (188170-11103).\n\n\n\n\n\nCopyright and licence\n\n© 2023 Davit Svanidze, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu.\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nSvanidze, Davit, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu. 2023. “The road to reproducible research: hazards to avoid and tools to get you there safely.” Real World Data Science, June 15, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "References",
    "text": "References\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–1227.↩︎\nIoannidis, John P. A., Sander Greenland, Mark A. Hlatky, Muin J. Khoury, Malcolm R. Macleod, David Moher, Kenneth F. Schulz, and Robert Tibshirani. 2014. “Increasing Value and Reducing Waste in Research Design, Conduct, and Analysis.” The Lancet 383 (9912): 166–175.↩︎\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716.↩︎\nBaker, Monya. 2016. “Reproducibility Crisis?” Nature 533 (26): 353–366.↩︎\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Gideon Nave, Brian A. Nosek, Thomas Pfeiffer, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science between 2010 and 2015.” Nature Human Behaviour 2: 637–644.↩︎"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "",
    "text": "Undergraduate student Yifan (Rosetta) Hu was responsible for writing the Python script that pre-processes the 2015–2016 UPC, EC, and PPC data for training neural network models. Her script randomly sampled five negative EC descriptions for every positive match between a UPC and EC code. Professor Mandy Korpusik performed the remaining work, including setting up the environment, training the BERT model, and evaluation. Hu spent roughly 10 hours on the competition, and Korpusik spent roughly 40 hours of work (and many additional hours running and monitoring the training and testing scripts)."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nThe goal of this challenge is to use machine learning and natural language processing (NLP) to link language-based entries in the IRI and FNDDS databases. Our proposed approach is based on our prior work using deep learning models to map users’ natural language meal descriptions to the FNDDS database (Korpusik, Collins, and Glass 2017b) to retrieve nutrition information in a spoken diet tracking system. In the past, we found a trade-off between accuracy and cost, leading us to select convolutional neural networks over recurrent long short-term memory (LSTM) networks – with nearly 10x as many parameters and 2x the training time required, LSTMs achieved slightly lower performance on semantic tagging and food database mapping on meals in the breakfast category. Here, we propose to investigate state-of-the-art transformers, specifically the contextual embedding model (i.e., the entire sentence is used as context to generate the embedding) known as BERT (Bidirectional Encoder Representations from Transformers, Devlin et al. 2018).\n\nRelated work\nWithin the past few years, several papers have come out that learn contextual representations of sentences, where the entire sentence is used to generate embeddings.\nELMo (Peters et al. 2018) uses a linear combination of vectors extracted from intermediate layer representations of a bidirectional LSTM trained on a large text corpus as a language model; in this feature-based approach, the ELMo vector of the full input sentence is concatenated with the standard context-independent token representations and passed through a task-dependent model for final prediction. This showed performance improvement over state-of-the-art on six NLP tasks, including question answering, textual entailment, and sentiment analysis.\nOpenAI GPT (Radford et al. 2018) is a fine-tuning approach, where they first pre-train a multi-layer transformer (Vaswani et al. 2017) as a language model on a large text corpus, and then conduct supervised fine-tuning on the specific task of interest, with a linear softmax layer on top of the pre-trained transformer.\nGoogle’s BERT (2018) is a fine-tuning approach similar to GPT, but with the key difference that instead of combining separately trained forward and backward transformers, they instead use a masked language model for pre-training, where they randomly masked out input tokens and predicted only those tokens. They demonstrated state-of-the-art performance on 11 NLP tasks, including the CoNLL 2003 named entity recognition task, which is similar to our semantic tagging task.\nFinally, many models have recently been developed that improve upon BERT, including RoBERTa (which improves BERT’s pre-training by using bigger batches and more data, Y. Liu et al. 2019), XLNet (which uses Transformer-XL and avoids BERT’s pretrain-finetune discrepancy through learning a truly bidirectional context via permutations over the factorization order, Yang et al. 2019), and ALBERT (a lightweight BERT, Lan et al. 2019).\nIn our prior work on language understanding for nutrition (Korpusik et al. 2014, 2016; Korpusik and Glass 2017, 2018, 2019; Korpusik, Collins, and Glass 2017a), we used a similar binary classification approach for learning embeddings, which were then used at test time to map from user-described meals to USDA food database matches, but with convolutional neural networks (CNNs) instead of BERT. (BERT was not created until 2018, and due to limited memory available for deployment, we needed a smaller model than even BERT base, which has 100 million parameters.) Further work demonstrated that BERT outperformed CNNs on several language understanding tasks, including nutrition (Korpusik, Liu, and Glass 2019)."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our approach",
    "text": "Our approach\nOur approach is to fine-tune a large pre-trained BERT language model on the food data. BERT was originally trained on a massive amount of text for a language modelling task (i.e., predicting which word should come next in a sentence). It relies on a transformer model, which uses an “attention” mechanism to identify which words the model should pay the most “attention” to. We are specifically using BERT for binary sequence classification, which refers to predicting a label (i.e., classification) for a sequence of words. In our case, during fine-tuning (i.e., training the model further on our own dataset) we will feed the model pairs of sentences (where one sentence is the UPC description of a food item and the other is the EC description of another food item), and the model will perform binary classification, predicting whether the sentences are a match (i.e., 1) or not (i.e., 0). We start with the 2015–2016 ground truth PPC data for positive examples, and five randomly sampled negative examples per positive example.\n\nTraining methods\nSince we used a neural network model, the only features passed into our model were the tokenized words themselves of the EC and UPC food descriptions – we did not conduct any manual feature engineering (Dong and Liu 2018). The model was trained on a 90/10 split into 90% training and 10% validation data, where the validation data was used as a test set to fine-tune the model’s hyperparameters. We started with a randomly sampled set of 16,000 pairs, batch size of 16 (i.e., the model would train on batches of 16 samples at a time), AdamW (Loshchilov and Hutter 2017) as the optimizer (which adaptively updates the learning rate, or how large the update should be to the model’s parameters), a linear schedule with warmup (i.e., starting with a small learning rate in the first few epochs of training due to large variance in early stages of training, L. Liu et al. 2019), and one epoch (i.e., the number of times the model passes through all the training data). We then added the next randomly sampled set of 16,000 pairs to get a model trained on 32,000 data points. Finally, we reached a total of 48,000 data samples used for training. Each pair of sequences was tokenized with the pre-trained BERT tokenizer, with the special CLS and SEP tokens (where CLS is a learned vector that is typically passed to downstream layers for final classification, and SEP is a learned vector that separates two input sequences), and was padded with zeros to the maximum length input sequence of 240 tokens, so that each input sequence would be the same length.\n\n\nModel development approach\nWe faced many challenges due to the secure nature of the ADRF environment. Since our approach relies on BERT, we were blocked by errors due to the local BERT installation. Typically, BERT is downloaded from the web as the program runs. However, for this challenge, BERT must be installed locally for security reasons. To fix the errors, the BERT models needed to be installed with git lfs clone instead of git.\nSecond, we were unable to retrieve the test data from the database due to SQLAlchemy errors. We found a workaround by using DBeaver directly to save database tables as Excel spreadsheets, rather than accessing the database tables through Python.\nFinally, we needed a GPU in order to efficiently train our BERT models. However, we initially only had a CPU, so there was a delay due to setting up the GPU configuration. Once the GPU image was set up, there was still a CUDA error when running the BERT model during training. We determined that the model was too big to fit into GPU memory, so we found a workaround using gradient checkpointing (trading off computation speed for memory) with the transformers library’s Trainer and TrainingArguments. Unfortunately, the version of transformers we were using did not have these tools, and the library was not updated until less than a week before the deadline, so we still had to train the model on the CPU.\nTo deal with the inability to run jobs in the background, our process was checkpointing our models every five batches, and saving the model predictions during evaluation to a csv file every five batches as well.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our results",
    "text": "Our results\nAfter training, the 48K model (so-called because it was trained on 48,000 data samples) was used at test time via ranking all possible 2017–18 EC descriptions given an unseen UPC description. The rankings were obtained through the model’s output value – the higher the output (or confidence), the more highly we ranked that EC description. To speed up the ranking process, we used blocking (i.e., only ranking a subset of all possible matches), specifically with exact word matches (using only the first six words in the UPC description, which appeared to be the most important), and fed all possible matches through the model in one batch per UPC description. Since we still did not have sufficient time to complete evaluation on the full set of test UPC descriptions, we implemented an expedited evaluation that only considered the first 10 matching EC descriptions in the BERT ranking process (which we call BERT-FAST). We also report results for the slower evaluation method that considers all EC descriptions that match at least one of the first six words in a given UPC description, but note that these results are based on just a small subset of the total test set. See Table 1 below for our results, where the (5?) indicates how often the correct match was ranked among the top-5. See Table 2 for an estimate of how long it takes to train and test the model on a CPU.\n\n\n\n\nTable 1: S@5 and NCDG@5 for BERT, both for fast evaluation over the whole test set, and slower evaluation on a smaller subset (711 UPCs out of 37,693 total).\n\n\n\n\nModel\nSuccess@5\nNDCG@5\n\n\n\n\nBERT-FAST\n0.057\n0.047\n\n\nBERT-SLOW\n0.537\n0.412\n\n\n\n\n\n\nTable 2: An estimate of the time required to train and test the model.\n\n\n\n\n\nTime\n\n\n\n\nTraining (on 48K samples)\n16 hours\n\n\nTesting (BERT-FAST)\n52 hours\n\n\nTesting (BERT-SLOW)\n63 days"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nIn the future, with more time available, we would train on all data, not just our limited dataset of 48,000 pairs, as well as perform evaluation on the held-out test set with the full set of possible EC matches that have one or more words in common with the UPC description. We would compare against baseline word embedding methods such as word2vec (Mikolov et al. 2017) and Glove (Pennington, Socher, and Manning 2014), and we would explore hierarchical prediction methods for improving efficiency and accuracy. Specifically, we would first train a classifier to predict the generic food category, and then train finer-grained models to predict specific foods within a general food category. Finally, we are exploring multi-modal transformer-based approaches that allow two input modalities (i.e., food images and text descriptions of a meal) for predicting the best UPC match."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Lessons learned",
    "text": "Lessons learned\nWe recommend that future challenges provide every team with both a CPU and a GPU in their workspace, to avoid transitioning from one to the other midway through the challenge. In addition, if possible, it would be very helpful to provide a mechanism for running jobs in the background. Finally, it may be useful for teams to submit snippets of code along with library package names, in order for the installations to be tested properly beforehand.\n\n\n\n\n← Part 4: Second place winners\n\n\n\n\nPart 6: The value of competitions →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYifan (Rosetta) Hu is an undergraduate student and Mandy Korpusik is an assistant professor of computer science at Loyola Marymount University’s Seaver College of Science and Engineering.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yifan Hu and Mandy Korpusik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Peter Bond on Unsplash.\n\n\n\nHow to cite\n\nHu, Yifan, and Mandy Korpusik. 2023. “Food for Thought: Third place winners – Loyola Marymount.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "href": "case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "title": "Food for Thought: The value of competitions for confidential data",
    "section": "",
    "text": "We are witnessing a sea change in data collection practices by both governments and businesses – from purposeful collection (through surveys and censuses, for example) to opportunistic (drawing on web and social media data, and administrative datasets). This shift has made clear the importance of record linkage – a government might, for example, look to link records held by its various departments to understand how citizens make use of the gamut of public services.\nHowever, creating manual linkages between datasets can be prohibitively expensive, time consuming, and subject to human constraints and bias. Machine learning (ML) techniques offer the potential to combine data better, faster, and more cheaply. But, as the recently released National AI Research Resources Task Force report highlights, it is important to have an open and transparent approach to ensure that unintended biases do not occur.\nIn other words, ML tools are not a substitute for thoughtful analysis. Both private and public producers of a linked dataset have to determine the level of linkage quality – such as what precision/recall tradeoff is best for the intended purpose (that is, the balance between false-positive links and failure to cover links that should be there), how much processing time and cost is acceptable, and how to address coverage issues. The challenge is made more difficult by the idiosyncrasies of heterogeneous datasets, and more difficult yet when datasets to be linked include confidential data (Christensen and Miguel 2018; Christen, Ranbaduge, and Schnell 2020).\nAnd, of course, an ML solution is never the end of the road: many data linkage scenarios are highly dynamic, involving use cases, datasets, and technical ecosystems that change and evolve over time; effective use of ML in practice necessitates an ongoing and continuous investment (Koch et al. 2021). Because techniques are constantly improving, producers need to keep abreast of new approaches. A model that is working well today may no longer work in a year because of changes in the data, or because the organizational needs have changed so that a certain type of error is no longer acceptable. As Sculley et al. point out, “it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning” (Sculley et al. 2014).\nAlso important is that record linkage is not seen as a technical problem relegated to the realm of computer scientists to solve. The full engagement of domain experts in designing the optimization problem, identifying measures of success, and evaluating the quality of the results is absolutely critical, as is building an understanding of the pros and cons of different measures (Schafer et al. 2021; Hand and Christen 2018). There will need to be much learning by doing in “sandbox” environments, and back and forth communication across communities to achieve successful outcomes, as noted in the recommendations of the Advisory Committee on Data for Evidence Building (a screenshot of which is shown in Figure 1).\n\n\n\n\n\n\nFigure 1: A recommendation for building an “innovation sandbox” as part of the creation of a new National Secure Data Service in the United States.\n\nDespite the importance of trial and error and transparency about linkage quality, there is no handbook that guides domain experts in how to design such sandboxes. There is a very real need for agreed-upon, domain-independent guidelines, or better yet, official standards to evaluate sandboxes. Those standards would define “who” could and would conduct the evaluation, and help guarantee independence and repeatability. And while innovation challenges have been embraced by the federal government, the devil can be very much in the details (Williams 2012).\nIt is for this reason that the approach taken in the Food for Thought linkage competition, and described in this compendium, provides an important first step towards a well specified, replicable framework for achieving high quality outcomes. In that respect it joins other recent efforts to bring together community-level research on shared sensitive data (MacAvaney et al. 2021; Tsakalidis et al. 2022). This competition, like those, helped bring to the foreground both the opportunities and challenges of doing research in secure sandboxes with sensitive data. Notably, these exercises highlight a kind of cultural tension between secure, managed environments, on the one hand, and unfettered machine learning research, on the other. The need for flexibility and agility in computational research bumps up against the need for advance planning and careful step-by-step processes in environments with well-defined data governance rules, and one of the key lessons learned is that the tradeoffs here need to be recognized and planned for.\nThis particular competition was important for a number of other reasons. Thanks to its organization as a competition, complete with prizes and bragging rights for strongly performing teams, it attracted new eyes from computer science and data science to think about how to address a critical real-world linkage problem. It offered the potential to produce approaches that were scalable, transparent, and reproducible. The engagement of domain experts and statisticians meant that it will be possible to conduct an informed error analysis, to explicitly relate the performance metrics in the task to the problem being solved in the real world, and to bring in the expertise of survey methodologists to think about the possible adjustments. And because it identified different approaches of addressing the same problem, it created an environment for new innovative ideas.\nMore generally, in addition to the excitement of the new approaches, this exercise laid bare the fragility of linkages in general and highlighted the importance of secure sandboxes for confidential data. While the promise of privacy preserving technologies is alluring as an alternative to bringing confidential data together in one place, such approaches are likely too immature to deploy ad hoc until a better understanding is established of how to translate real-world problems and their associated data into well-defined tasks, how to measure quality, and particularly how to assess the impact of match quality on different subgroups (Domingo-Ferrer, Sánchez, and Blanco-Justicia 2021). The scientific profession has gone through too painful a lesson with the premature application of differential privacy techniques to ignore the lessons that can be learned from a careful and systematic analysis of different approaches (2021; Van Riper et al. 2020; Ruggles et al. 2019; Giles et al. 2022).\nWe hope that the articles in this collection provide not only the first steps towards a handbook of best practices, but also an inspiration to share lessons learned, so that success can be emulated, and failures understood and avoided.\n\n\n\n\n← Part 5: Third place winners\n\n\n\n\nFind more case studies\n\n\n\n\n\n\n\n\nAbout the authors\n\nSteven Bedrick is an associate professor in Oregon Health and Science University’s Department of Medical Informatics and Clinical Epidemiology.\n\n\nOphir Frieder is a professor in Georgetown University’s Department of Computer Science, and in the Department of Biostatistics, Bioinformatics & Biomathematics at Georgetown University Medical Center.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative.\n\n\nPhilip Resnik holds a joint appointment as professor in the University of Maryland Institute for Advanced Computer Studies and the Department of Linguistics, and an affiliate professor appointment in computer science.\n\n\n\nCopyright and licence\n\n© 2023 Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Alexandru Tugui on Unsplash.\n\n\n\nHow to cite\n\nBedrick, Steven, Ophir Frieder, Julia Lane, and Philip Resnik. 2023. “Food for Thought: The value of competitions for confidential data.” Real World Data Science, August 21, 2023. URL\n\n\n\n\n\n\n\n\n\nReferences\n\nChristen, P., T. Ranbaduge, and R. Schnell. 2020. Linking Sensitive Data - Methods and Techniques for Practical Privacy-Preserving Information Sharing. Springer. https://doi.org/10.1007/978-3-030-59706-1.\n\n\nChristensen, G., and E. Miguel. 2018. “Transparency, Reproducibility, and the Credibility of Economics Research.” Journal of Economic Literature 56 (3): 920–80. https://doi.org/10.1257/jel.20171350.\n\n\nDomingo-Ferrer, J., D. Sánchez, and A. Blanco-Justicia. 2021. “The Limits of Differential Privacy (and Its Misuse in Data Release and Machine Learning).” Communications of the ACM 64 (7): 33–35. https://doi.org/10.1145/3433638.\n\n\nGiles, O., K. Hosseini, G. Mingas, O. Strickson, L. Bowler, C. Rangel Smith, H. Wilde, et al. 2022. “Faking Feature Importance: A Cautionary Tale on the Use of Differentially-Private Synthetic Data.” https://arxiv.org/abs/2203.01363.\n\n\nHand, D., and P. Christen. 2018. “A Note on Using the f-Measure for Evaluating Record Linkage Algorithms.” Statistics and Computing 28 (3): 539–47. https://doi.org/10.1007/s11222-017-9746-6.\n\n\nKoch, B., E. Denton, A. Hanna, and J. G. Foster. 2021. “Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research.” CoRR abs/2112.01716. https://arxiv.org/abs/2112.01716.\n\n\nMacAvaney, S., A. Mittu, G. Coppersmith, J. Leintz, and P. Resnik. 2021. “Community-Level Research on Suicidality Prediction in a Secure Environment: Overview of the CLPsych 2021 Shared Task.” In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access, 70–80. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.clpsych-1.7.\n\n\nRuggles, S., C. Fitch, D. Magnuson, and J. Schroeder. 2019. “Differential Privacy and Census Data: Implications for Social and Economic Research.” AEA Papers and Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nSchafer, K. M., G. Kennedy, A. Gallyer, and P. Resnik. 2021. “A Direct Comparison of Theory-Driven and Machine Learning Prediction of Suicide: A Meta-Analysis.” PLOS ONE 16 (4): 1–23. https://doi.org/10.1371/journal.pone.0249833.\n\n\nSculley, D., G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young. 2014. “Machine Learning: The High Interest Credit Card of Technical Debt.” In SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop).\n\n\nTsakalidis, A., J. Chim, I. M. Bilal, A. Zirikly, D. Atzil-Slonim, F. Nanni, P. Resnik, et al. 2022. “Overview of the CLPsych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts.” In Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology, 184–98. Seattle, USA: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.clpsych-1.16.\n\n\nVan Riper, D., T. Kugler, J. Schroeder, and S. Ruggles. 2020. “Differential Privacy and Racial Residential Segregation.” In 2020 APPAM Fall Research Conference.\n\n\nWilliams, H. 2012. “Innovation Inducement Prizes: Connecting Research to Policy.” Journal of Policy Analysis and Management 31 (3): 752–76. http://www.jstor.org/stable/41653827."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "",
    "text": "DeepFFTLink team members: Yang Wu and Kai Zhang are PhD students at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student at Indiana University Bloomington. Xuhong Zhang is an assistant professor at Indiana University Bloomington. Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Perspective on the challenge",
    "text": "Perspective on the challenge\nText matching is an essential task in natural language processing (NLP, Pang et al. 2016), while record linkage across different sources is an essential task in data science. Machine learning techniques allow people to combine data faster and cheaper than using manual linkage. However, in the context of the Food for Thought challenge, existing methods for matching universal product codes (UPCs) to ensemble codes (ECs) require every UPC to be compared with every EC code (Figure 1a). Such approaches can be computationally expensive in the training process when data is noisy. Here, we propose an ensemble model with a category-based adapter to tackle this problem, drawing on the category information included in UPC and EC data. The category-based adapter allows UPCs to be first matched with only a small and reliable set of ECs (Figure 1b). Then, an ensemble model will be deployed to make predictions for UPC-EC matching. Our proposed approach can achieve competitive performance compared with state-of-the-art models.\n\n\n\n\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\nFigure 1: A toy example of our method. Panel (a) shows the traditional matching method, while (b) is our proposed ensemble model with category-based adapter. With the help of the adapter, UPC 1 only needs to be matched with EC 1 and EC 3."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our approach",
    "text": "Our approach\nWe propose a two-step framework to address this problem. To begin with, we use a category-based adapter to get reliable candidate ECs for each UPC. Then, an ensemble model (Dietterich 2000) is deployed to make a prediction for each UPC-EC pair.\n\nCategory-based adapter\nBy using 2015–2016 UPC-EC data, we created a knowledge base, which is a UPC category–EC pair-wised table for generating candidate ECs. Within this setting, each UPC category is, on average, related to only 32 ECs. This knowledge base is then used as context to further filter the candidate ECs. Note that there are some new ECs generated year by year, which can also be part of the potential ECs in the UPC-EC matching task, since the contextual information of new ECs does not exist in our knowledge base.\n\n\nEnsembled model\nWe ensemble the base-string match and BERT models. BERT is a deep learning model for natural language processing (Devlin et al. 2018). In the base-string match model, we used the Term Frequency-Inverse Document Frequency (TFIDF) of each UPC and EC description as features to calculate a pairwise cosine similarity, which is a distance between instances. Meanwhile, we used features extracted from UPC and EC descriptions to fine-tune the BERT base model and calculated the cosine similarity of embeddings between each UPC and EC. Then we rank ECs based on their similarity scores with the UPC.\n\n\n\n\n\n\nFigure 2: The framework of our proposed model. A two-step strategy is used to make the final prediction.\n\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our results",
    "text": "Our results\nWe randomly selected 500 samples from the 2017–2018 UPC-EC data to train the ensembled weight for each model. Two functions were adapted to make a fusion of base-string and BERT models:\n\\[\nC = a * X + b * Y  \n\\tag{1}\\]\n\\[\nC =  a * log(X) + b * log(Y) \\text{. }\n\\tag{2}\\]\n\\(C\\) denotes the final confidence score. \\(X\\) and \\(Y\\) represent base_string_similarity_score and BERT_similarity_score, respectively. \\(a\\) and \\(b\\) are corresponding model weights for base_string and BERT models.\nA better Success@5 is achieved with function (1). The ensembled weights for the base-string model and BERT model are 0.738 and 0.262, respectively. The experiment result indicates that the base_string model contributes more than the BERT model when the ensemble model makes predictions. The prediction result for the 2017–2018 data is:\n\nSuccess@5: 0.727\nNDCG@5: 0.528\n\nComputation time is 6 hours."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Future work",
    "text": "Future work\nOur next step will focus on adding the newly generated EC data into our knowledge base, which allows the model to be more stable to make predictions for UPC-EC matching. Our model is an unsupervised method, which does not need labels for each instance. We use cosine similarity to rank the matches, so no labels are needed in the training process. However, our future work will try to label some instances to handle the UPC-EC matching task in a supervised manner."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Lessons learned",
    "text": "Lessons learned\n\nIf the data is not complex, simple models may outperform complex models. For example, in our experiment, we found that the base-string model outperforms single RoBERTa (Liu et al. 2019) or BERT models. However, our ensemble model can outperform each individual model since model fusion allows information aggregation from multiple models.\nMulti-label models may not work well on UPC-EC data. In our early work, we tried to consider the UPC-EC matching task as a multi-label problem, e.g., we labeled each EC as a binary label which indicated whether the EC was an appropriate match or not. We mapped UPC and EC pairs into a multi-label table. However, we find that the UPC and EC keeps a one-to-one relation for most UPCs. The model performance of a multi-label model, i.e., Label-Specific Attention Network (LSAN, Xiao et al. 2019), is lower than base-string model on both Success@5 and NDCG@5 metrics.\n\n\n\n\n\n← Part 3: First place winners\n\n\n\n\nPart 5: Third place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYang Wu and Kai Zhang are PhD students, and Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student and Xuhong Zhang is an assistant professor at Indiana University Bloomington.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Hanson Lu on Unsplash.\n\n\n\nHow to cite\n\nWu, Yang, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu. 2023. “Food for Thought: Second place winners – DeepFFTLink.” Real World Data Science, August 21, 2023. URL"
  }
]